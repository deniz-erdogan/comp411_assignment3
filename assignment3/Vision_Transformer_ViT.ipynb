{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6b7ee23c",
      "metadata": {
        "tags": [
          "pdf-title"
        ],
        "id": "6b7ee23c"
      },
      "source": [
        "# Attentional Networks in Computer Vision\n",
        "\n",
        "Prepared by comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
        "\n",
        "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
        "\n",
        "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a01a910e",
      "metadata": {
        "id": "a01a910e"
      },
      "source": [
        "# Part I. Preparation\n",
        "\n",
        "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
        "\n",
        "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a3cdc4fd",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore"
        ],
        "id": "a3cdc4fd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6152858f",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "d342e26f331b45cd9c390a7bdd1c1e05",
            "238cf8e173a548c0ab6cf47f75735fe3",
            "d86e82a2c0e349faa81b2d4db15b77b7",
            "4ea1a7c745c14811af373576f212cd8a",
            "885bd009bd5f49f18cd6c3917eafe8d7",
            "ece3db6e8fb843e6b3d8055fcb50b4d1",
            "afa737f1c7a54d36bcf2feed2b6f1b95",
            "bd8104dc914c400989fb1b9b7897b479",
            "e3caba48189a48cca56663445598b014",
            "618c338ee0804002a4e5c2c0dd0158b7",
            "62807e8b2fae424aa17bb71c90d971fb"
          ]
        },
        "id": "6152858f",
        "outputId": "164e759e-ebb2-4585-ce01-05d8b75b5eeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./comp411/datasets/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d342e26f331b45cd9c390a7bdd1c1e05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./comp411/datasets/cifar-10-python.tar.gz to ./comp411/datasets\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "NUM_TRAIN = 49000\n",
        "\n",
        "# The torchvision.transforms package provides tools for preprocessing data\n",
        "# and for performing data augmentation; here we set up a transform to\n",
        "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
        "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
        "transform = T.Compose([\n",
        "                T.ToTensor(),\n",
        "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "            ])\n",
        "\n",
        "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
        "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
        "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
        "# training set into train and val sets by passing a Sampler object to the\n",
        "# DataLoader telling how it should sample from the underlying Dataset.\n",
        "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
        "                             transform=transform)\n",
        "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
        "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
        "\n",
        "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
        "                           transform=transform)\n",
        "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
        "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
        "\n",
        "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=True, \n",
        "                            transform=transform)\n",
        "loader_test = DataLoader(cifar10_test, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "840763c9",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "840763c9"
      },
      "source": [
        "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
        "\n",
        "The global variables `dtype` and `device` will control the data types throughout this assignment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d502cffe",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore-input"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d502cffe",
        "outputId": "93665fe9-96f5-4c6f-b919-4f9f1e0f76db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cpu\n"
          ]
        }
      ],
      "source": [
        "USE_GPU = True\n",
        "\n",
        "dtype = torch.float32 # we will be using float throughout this tutorial\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constant to control how frequently we print train loss\n",
        "print_every = 100\n",
        "\n",
        "print('using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "787434ed",
      "metadata": {
        "id": "787434ed"
      },
      "source": [
        "# Part II. Barebones Transformers: Self-Attentional Layer\n",
        "\n",
        "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
        "\n",
        "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
        "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
        "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
        "\n",
        "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
        "\n",
        "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
        "\n",
        "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
        "\n",
        "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
        "\n",
        "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
        "\n",
        "5. Reassemble heads into one flat vector and return the output.\n",
        "\n",
        "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "217f44ac",
      "metadata": {
        "id": "217f44ac"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "class SelfAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        \n",
        "        ## initialize module's instance variables\n",
        "        self.input_dims = input_dims\n",
        "        self.head_dims = head_dims\n",
        "        self.num_heads = num_heads\n",
        "        self.proj_dims = head_dims * num_heads\n",
        "        \n",
        "        ## Declare module's parameters\n",
        "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
        "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
        "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
        "\n",
        "        self.init_weights()\n",
        "        \n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Input of shape, [B, N, D] where:\n",
        "        ## - B denotes the batch size\n",
        "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
        "        ## - D corresponds to model dimensionality\n",
        "        b,n,d = x.shape\n",
        "        \n",
        "        ## Construct queries,keys,values\n",
        "        q_ = self.W_Q(x)\n",
        "        k_ = self.W_K(x)\n",
        "        v_ = self.W_V(x)\n",
        "        \n",
        "        ## Seperate q,k,v into their corresponding heads,\n",
        "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
        "        ## - B denotes the batch size\n",
        "        ## - H denotes number of heads\n",
        "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
        "        ## - D//H corresponds to per head dimensionality\n",
        "        q, k, v = map(lambda z: torch.reshape(z, (b,n,self.num_heads,self.head_dims)).permute(0,2,1,3), [q_,k_,v_])\n",
        "       \n",
        "        #########################################################################################\n",
        "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
        "        #########################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        \n",
        "        ## Compute attention logits. Note that this operation is conducted as a\n",
        "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
        "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
        "        ## Output Attention logits should have the size: [B,H,N,N]\n",
        "       \n",
        "        # print(k.size())\n",
        "        # print(q.size())\n",
        "\n",
        "        # dot = torch.bmm(q, k)\n",
        "        # print(dot.size())\n",
        "\n",
        "        # k_ = k_.transpose(2, 1)\n",
        "        # print(k_.size())\n",
        "        # print(q_.size())\n",
        "        attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / np.sqrt(self.head_dims)\n",
        "\n",
        "        #k = k.transpose(2, 1)\n",
        "        # print(k.size())\n",
        "        # print(k_.size())\n",
        "        # print(q.size())\n",
        "        # k = torch.reshape(k, (256, 16, 64))\n",
        "        # print(k.size())\n",
        "        # dot =  k @ q\n",
        "        # attention_score = dot / np.sqrt(self.head_dims)\n",
        "\n",
        "        ## Compute attention Weights. Note that this operation is conducted as a\n",
        "        ## Softmax Normalization across the keys dimension. \n",
        "        ## Hint: You can apply the Softmax operation across the final dimension\n",
        "        attention_weigths = F.softmax(attention_scores, dim=2)\n",
        "       \n",
        "        ## Compute output values. Note that this operation is conducted as a \n",
        "        ## batched matrix multiplication between the Attention Weights matrix and \n",
        "        ## the values tensor. After computing output values, the output should be reshaped\n",
        "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H]\n",
        "        ## Output should be of size [B, N, D]\n",
        "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
        "        \n",
        "        #attn_out = attention_weigths @ v\n",
        "        attn_out = torch.matmul(attention_weigths, v)\n",
        "        attn_out = attn_out.permute(0,2,1,3)\n",
        "        attn_out = torch.reshape(attn_out,(b,n,self.proj_dims))\n",
        "        \n",
        "        \n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             \n",
        "        ################################################################################\n",
        "    \n",
        "        return attn_out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d039f68f",
      "metadata": {
        "id": "d039f68f"
      },
      "source": [
        "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
        "\n",
        "When you run this function, output should have shape (64, 16, 64)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "edf1a8a8",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore-input"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edf1a8a8",
        "outputId": "93635e52-de97-4412-e1dd-946311bd413b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 16, 256])\n"
          ]
        }
      ],
      "source": [
        "def test_self_attn_layer():\n",
        "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 32\n",
        "    layer = SelfAttention(32,64,4)\n",
        "    out = layer(x)\n",
        "    print(out.size())  # you should see [64,16,256]\n",
        "test_self_attn_layer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "980c2a5a",
      "metadata": {
        "id": "980c2a5a"
      },
      "source": [
        "# Part III. Barebones Transformers: Transformer Encoder Block\n",
        "\n",
        "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0b9ef2b6",
      "metadata": {
        "id": "0b9ef2b6"
      },
      "outputs": [],
      "source": [
        "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
        "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
        "        \n",
        "        self.init_weights()\n",
        "        \n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0.1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        o = F.elu(self.fc_1(x))\n",
        "        o = self.fc_2(o)\n",
        "        return o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8a7409ce",
      "metadata": {
        "id": "8a7409ce"
      },
      "outputs": [],
      "source": [
        "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
        "## module follows a simple computational pipeline:\n",
        "## input --> layernorm --> SelfAttention --> skip connection \n",
        "##       --> layernorm --> MLP ---> skip connection ---> output\n",
        "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
        "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
        "## to the SelfAttention block.\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        \n",
        "    ###############################################################\n",
        "    # TODO: Complete the consturctor of  TransformerBlock module  #\n",
        "    ###############################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.num_heads = num_heads\n",
        "        head_dims = self.hidden_dims//self.num_heads\n",
        "        num_heads=self.num_heads\n",
        "        self.norm1 = nn.LayerNorm(self.hidden_dims) \n",
        "        self.attention = SelfAttention(self.hidden_dims,head_dims=head_dims,num_heads=num_heads)\n",
        "        self.norm2 = nn.LayerNorm(self.hidden_dims)\n",
        "        self.mlp = MLP(hidden_dims, hidden_dims, hidden_dims)\n",
        "        \n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###################################################################\n",
        "    #                                 END OF YOUR CODE                #             \n",
        "    ###################################################################\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "    ##############################################################\n",
        "    # TODO: Complete the forward of TransformerBlock module      #\n",
        "    ##############################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
        "        b,h,head = x.shape\n",
        "        x_skip = x\n",
        "\n",
        "        norm1_out = self.norm1(x)\n",
        "        attention_out = self.attention(norm1_out) \n",
        "        attention_out_2 = attention_out+ x_skip\n",
        "        x_skip_2 = attention_out_2\n",
        "        norm2_out = self.norm2(attention_out_2)\n",
        "        mlp_out = self.mlp(norm2_out)\n",
        "        mlp_out =mlp_out+x_skip_2\n",
        "\n",
        "\n",
        "        return mlp_out\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###################################################################\n",
        "    #                                 END OF YOUR CODE                #             \n",
        "    ###################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c74208d7",
      "metadata": {
        "id": "c74208d7"
      },
      "source": [
        "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
        "\n",
        "When you run this function, output should have shape (64, 16, 64)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "648ab1d4",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore-input"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "648ab1d4",
        "outputId": "1f9ed0bd-9c09-494e-ff01-0d20b0e9d8bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 16, 128])\n"
          ]
        }
      ],
      "source": [
        "def test_transfomerblock_layer():\n",
        "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
        "    layer = TransformerBlock(128,4) # hidden dims size 128, heads size 4\n",
        "    out = layer(x)\n",
        "    print(out.size())  # you should see [64,16,128]\n",
        "test_transfomerblock_layer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "544c1132",
      "metadata": {
        "id": "544c1132"
      },
      "source": [
        "# Part IV The Vision Transformer (ViT)\n",
        "\n",
        "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "aca38dd0",
      "metadata": {
        "id": "aca38dd0"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
        "        super(ViT, self).__init__()\n",
        "                \n",
        "        ## initialize module's instance variables\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.input_dims = input_dims\n",
        "        self.output_dims = output_dims\n",
        "        self.num_trans_layers = num_trans_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.image_k = image_k\n",
        "        self.patch_k = patch_k\n",
        "        \n",
        "        self.image_height = self.image_width = image_k\n",
        "        self.patch_height = self.patch_width = patch_k\n",
        "        \n",
        "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
        "                'Image size must be divisible by the patch size.'\n",
        "\n",
        "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
        "        self.patch_flat_len = self.patch_height * self.patch_width\n",
        "        \n",
        "        ## Declare module's parameters\n",
        "        \n",
        "        ## ViT's flattened patch embedding projection:\n",
        "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
        "        \n",
        "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
        "        \n",
        "        ## Learnable classt token and its index among attention sequence elements.\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
        "        self.cls_index = torch.LongTensor([0])\n",
        "        \n",
        "        ## Declare cascaded Transformer blocks:\n",
        "        transformer_encoder_list = []\n",
        "        for _ in range(self.num_trans_layers):\n",
        "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
        "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
        "        \n",
        "        ## Declare the output mlp:\n",
        "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
        "         \n",
        "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
        "        ## Create sliding window pathes using nn.Functional.unfold\n",
        "        ## Input dimensions: [B,D,H,W] where\n",
        "        ## --B : input batch size\n",
        "        ## --D : input channels\n",
        "        ## --H, W: input height and width\n",
        "        ## Output dimensions: [B,N,H*W,D]\n",
        "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
        "        ##      sliding window stride and padding.\n",
        "        b,d,h,w = x.shape\n",
        "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)    \n",
        "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
        "        n = x_unf.size(1)\n",
        "        return x_unf,n\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b = x.size(0)\n",
        "        ## create sliding window patches from the input image\n",
        "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
        "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
        "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
        "        ## linearly embed each flattened patch\n",
        "        x_embed = self.linear_embed(x_patch_flat)\n",
        "        \n",
        "        ## retrieve class token \n",
        "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
        "        ## concatanate class token to input patches\n",
        "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
        "        \n",
        "        ## add positional embedding to input patches + class token \n",
        "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
        "        \n",
        "        ## pass through the transformer encoder\n",
        "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
        "        \n",
        "        ## select the class token \n",
        "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
        "        \n",
        "        ## create output\n",
        "        out = self.out_mlp(out_cls_token)\n",
        "        \n",
        "        return out.squeeze(-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cba4cdab",
      "metadata": {
        "id": "cba4cdab"
      },
      "source": [
        "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
        "\n",
        "When you run this function, output should have shape (64, 16, 64)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2ec9176e",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore-input"
        ],
        "id": "2ec9176e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7edc3adc-7247-4900-8acb-3c9d43c88be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10])\n"
          ]
        }
      ],
      "source": [
        "def test_vit():\n",
        "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
        "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
        "    out = model(x)\n",
        "    print(out.size())  # you should see [64,10]\n",
        "test_vit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "833736a1",
      "metadata": {
        "id": "833736a1"
      },
      "source": [
        "# Part V. Train the ViT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b33f7f1",
      "metadata": {
        "id": "6b33f7f1"
      },
      "source": [
        "### Check Accuracy\n",
        "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network. \n",
        "\n",
        "The check_batch_accuracy function is provided for you below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c50f0b22",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "c50f0b22"
      },
      "outputs": [],
      "source": [
        "def check_batch_accuracy(out, target,eps=1e-7):\n",
        "    b, c = out.shape\n",
        "    with torch.no_grad():\n",
        "        _, pred = out.max(-1) \n",
        "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
        "    return correct, np.float(correct) / (b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dad6403f",
      "metadata": {
        "id": "dad6403f"
      },
      "source": [
        "### Training Loop\n",
        "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c758b6b0",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "c758b6b0"
      },
      "outputs": [],
      "source": [
        "def train(network, optimizer, trainloader):\n",
        "    \"\"\"\n",
        "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
        "    \n",
        "    Inputs:\n",
        "    - network: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
        "    \n",
        "    Returns: overall training accuracy for the epoch\n",
        "    \"\"\"\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    network.train()  # put model to training mode\n",
        "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
        "            \n",
        "        outputs = network(inputs)\n",
        "        loss =  F.cross_entropy(outputs, targets)\n",
        "            \n",
        "        # Zero out all of the gradients for the variables which the optimizer\n",
        "        # will update.\n",
        "        optimizer.zero_grad() \n",
        "\n",
        "        # This is the backwards pass: compute the gradient of the loss with\n",
        "        # respect to each  parameter of the model.\n",
        "        loss.backward()\n",
        "            \n",
        "        # Actually update the parameters of the model using the gradients\n",
        "        # computed by the backwards pass.\n",
        "        optimizer.step()\n",
        "            \n",
        "        loss = loss.detach()\n",
        "        train_loss += loss.item()\n",
        "        correct_p, _ = check_batch_accuracy(outputs, targets) \n",
        "        correct += correct_p\n",
        "        total += targets.size(0)\n",
        "\n",
        "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "        \n",
        "    return 100.*correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe7414b",
      "metadata": {
        "id": "dfe7414b"
      },
      "source": [
        "### Evaluation Loop\n",
        "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "23c264e5",
      "metadata": {
        "id": "23c264e5"
      },
      "outputs": [],
      "source": [
        "def evaluate(network, evalloader):\n",
        "    \"\"\"\n",
        "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
        "    \n",
        "    Inputs:\n",
        "    - network: A PyTorch Module giving the model to train.\n",
        "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
        "    \n",
        "    Returns: overall evaluation accuracy for the epoch\n",
        "    \"\"\"\n",
        "    network.eval() # put model to evaluation mode\n",
        "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    eval_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print('\\n---- Evaluation in process ----')\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
        "            outputs = network(inputs)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "            \n",
        "            eval_loss += loss.item()\n",
        "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
        "            correct += correct_p\n",
        "            total += targets.size(0)\n",
        "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    return 100.*correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00ea5a24",
      "metadata": {
        "id": "00ea5a24"
      },
      "source": [
        "### Overfit a ViT\n",
        "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free. \n",
        "\n",
        "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`. \n",
        "\n",
        "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
        "\n",
        "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "eb34b985",
      "metadata": {
        "id": "eb34b985",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cfeb969-4974-49cb-a0c8-b106938f81ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
            "==> Data ready, batchsize = 25\n"
          ]
        }
      ],
      "source": [
        "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
        "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
        "\n",
        "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
        "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
        "\n",
        "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
        "\n",
        "batch_size_sub = 25\n",
        "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
        "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
        "\n",
        "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2c1e75fc",
      "metadata": {
        "scrolled": true,
        "id": "2c1e75fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3e3937-5a0c-499d-8111-53d60a05f5b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-48c091e2d034>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  return correct, np.float(correct) / (b)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 3.933 | Acc: 16.000% (4/25)\n",
            "Loss: 5.532 | Acc: 14.000% (7/50)\n",
            "Loss: 6.768 | Acc: 12.000% (9/75)\n",
            "Loss: 6.849 | Acc: 10.000% (10/100)\n",
            "Epoch 0 of training is completed, Training accuracy for this epoch is 10.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 4.074 | Acc: 8.000% (2/25)\n",
            "Loss: 5.123 | Acc: 14.000% (7/50)\n",
            "Loss: 4.507 | Acc: 18.667% (14/75)\n",
            "Loss: 4.512 | Acc: 16.000% (16/100)\n",
            "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 16.0\n",
            "\n",
            "Epoch: 1\n",
            "Loss: 4.431 | Acc: 12.000% (3/25)\n",
            "Loss: 4.417 | Acc: 12.000% (6/50)\n",
            "Loss: 4.368 | Acc: 16.000% (12/75)\n",
            "Loss: 4.141 | Acc: 17.000% (17/100)\n",
            "Epoch 1 of training is completed, Training accuracy for this epoch is 17.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 3.427 | Acc: 12.000% (3/25)\n",
            "Loss: 3.746 | Acc: 14.000% (7/50)\n",
            "Loss: 3.568 | Acc: 13.333% (10/75)\n",
            "Loss: 3.674 | Acc: 11.000% (11/100)\n",
            "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 11.0\n",
            "\n",
            "Epoch: 2\n",
            "Loss: 2.669 | Acc: 32.000% (8/25)\n",
            "Loss: 2.459 | Acc: 36.000% (18/50)\n",
            "Loss: 2.448 | Acc: 30.667% (23/75)\n",
            "Loss: 2.774 | Acc: 24.000% (24/100)\n",
            "Epoch 2 of training is completed, Training accuracy for this epoch is 24.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.916 | Acc: 8.000% (2/25)\n",
            "Loss: 2.862 | Acc: 8.000% (4/50)\n",
            "Loss: 2.808 | Acc: 8.000% (6/75)\n",
            "Loss: 2.807 | Acc: 11.000% (11/100)\n",
            "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 11.0\n",
            "\n",
            "Epoch: 3\n",
            "Loss: 2.128 | Acc: 32.000% (8/25)\n",
            "Loss: 2.092 | Acc: 28.000% (14/50)\n",
            "Loss: 2.256 | Acc: 25.333% (19/75)\n",
            "Loss: 2.477 | Acc: 25.000% (25/100)\n",
            "Epoch 3 of training is completed, Training accuracy for this epoch is 25.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.501 | Acc: 24.000% (6/25)\n",
            "Loss: 2.697 | Acc: 18.000% (9/50)\n",
            "Loss: 2.844 | Acc: 14.667% (11/75)\n",
            "Loss: 3.002 | Acc: 14.000% (14/100)\n",
            "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 14.0\n",
            "\n",
            "Epoch: 4\n",
            "Loss: 2.027 | Acc: 32.000% (8/25)\n",
            "Loss: 1.983 | Acc: 32.000% (16/50)\n",
            "Loss: 2.148 | Acc: 29.333% (22/75)\n",
            "Loss: 2.314 | Acc: 26.000% (26/100)\n",
            "Epoch 4 of training is completed, Training accuracy for this epoch is 26.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.412 | Acc: 20.000% (5/25)\n",
            "Loss: 2.434 | Acc: 16.000% (8/50)\n",
            "Loss: 2.570 | Acc: 14.667% (11/75)\n",
            "Loss: 2.629 | Acc: 15.000% (15/100)\n",
            "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 15.0\n",
            "\n",
            "Epoch: 5\n",
            "Loss: 2.072 | Acc: 28.000% (7/25)\n",
            "Loss: 1.914 | Acc: 30.000% (15/50)\n",
            "Loss: 1.945 | Acc: 28.000% (21/75)\n",
            "Loss: 1.993 | Acc: 29.000% (29/100)\n",
            "Epoch 5 of training is completed, Training accuracy for this epoch is 29.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.491 | Acc: 24.000% (6/25)\n",
            "Loss: 2.585 | Acc: 20.000% (10/50)\n",
            "Loss: 2.498 | Acc: 16.000% (12/75)\n",
            "Loss: 2.636 | Acc: 12.000% (12/100)\n",
            "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 12.0\n",
            "\n",
            "Epoch: 6\n",
            "Loss: 1.878 | Acc: 20.000% (5/25)\n",
            "Loss: 1.847 | Acc: 26.000% (13/50)\n",
            "Loss: 1.796 | Acc: 30.667% (23/75)\n",
            "Loss: 1.820 | Acc: 30.000% (30/100)\n",
            "Epoch 6 of training is completed, Training accuracy for this epoch is 30.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.602 | Acc: 16.000% (4/25)\n",
            "Loss: 2.422 | Acc: 24.000% (12/50)\n",
            "Loss: 2.428 | Acc: 18.667% (14/75)\n",
            "Loss: 2.629 | Acc: 15.000% (15/100)\n",
            "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 15.0\n",
            "\n",
            "Epoch: 7\n",
            "Loss: 1.725 | Acc: 32.000% (8/25)\n",
            "Loss: 1.703 | Acc: 40.000% (20/50)\n",
            "Loss: 1.707 | Acc: 36.000% (27/75)\n",
            "Loss: 1.730 | Acc: 36.000% (36/100)\n",
            "Epoch 7 of training is completed, Training accuracy for this epoch is 36.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.569 | Acc: 16.000% (4/25)\n",
            "Loss: 2.466 | Acc: 16.000% (8/50)\n",
            "Loss: 2.357 | Acc: 18.667% (14/75)\n",
            "Loss: 2.578 | Acc: 18.000% (18/100)\n",
            "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 18.0\n",
            "\n",
            "Epoch: 8\n",
            "Loss: 1.587 | Acc: 44.000% (11/25)\n",
            "Loss: 1.408 | Acc: 56.000% (28/50)\n",
            "Loss: 1.420 | Acc: 48.000% (36/75)\n",
            "Loss: 1.507 | Acc: 46.000% (46/100)\n",
            "Epoch 8 of training is completed, Training accuracy for this epoch is 46.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.783 | Acc: 16.000% (4/25)\n",
            "Loss: 2.420 | Acc: 18.000% (9/50)\n",
            "Loss: 2.384 | Acc: 18.667% (14/75)\n",
            "Loss: 2.667 | Acc: 16.000% (16/100)\n",
            "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 16.0\n",
            "\n",
            "Epoch: 9\n",
            "Loss: 1.146 | Acc: 68.000% (17/25)\n",
            "Loss: 1.276 | Acc: 56.000% (28/50)\n",
            "Loss: 1.504 | Acc: 50.667% (38/75)\n",
            "Loss: 1.546 | Acc: 49.000% (49/100)\n",
            "Epoch 9 of training is completed, Training accuracy for this epoch is 49.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.845 | Acc: 12.000% (3/25)\n",
            "Loss: 2.765 | Acc: 16.000% (8/50)\n",
            "Loss: 2.653 | Acc: 20.000% (15/75)\n",
            "Loss: 2.919 | Acc: 16.000% (16/100)\n",
            "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 16.0\n",
            "\n",
            "Final train set accuracy is 49.0\n",
            "Final val set accuracy is 16.0\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.002\n",
        "input_dims = 3\n",
        "hidden_dims = 128\n",
        "output_dims=10\n",
        "num_trans_layers = 4\n",
        "num_heads=4\n",
        "image_k=32\n",
        "patch_k=4\n",
        "\n",
        "model = None\n",
        "optimizer = None\n",
        "\n",
        "################################################################################\n",
        "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "network = ViT(hidden_dims, input_dims, output_dims, num_trans_layers,num_heads,image_k,patch_k)\n",
        "\n",
        "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             \n",
        "################################################################################\n",
        "\n",
        "tr_accs=[]\n",
        "eval_accs=[]\n",
        "for epoch in range(10):\n",
        "    tr_acc = train(network, optimizer, trainloader_sub)\n",
        "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
        "              .format(epoch, tr_acc))  \n",
        "    \n",
        "    eval_acc = evaluate(network, valloader_sub)\n",
        "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
        "              .format(epoch, eval_acc))  \n",
        "    tr_accs.append(tr_acc)\n",
        "    eval_accs.append(eval_acc)\n",
        "    \n",
        "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
        "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "588330f1",
      "metadata": {
        "id": "588330f1"
      },
      "source": [
        "## Train the net\n",
        "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2ee3dabf",
      "metadata": {
        "scrolled": true,
        "id": "2ee3dabf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1cbef5c-6ccf-456d-e131-f585669575e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-48c091e2d034>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  return correct, np.float(correct) / (b)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 3.660 | Acc: 14.062% (9/64)\n",
            "Loss: 3.637 | Acc: 13.281% (17/128)\n",
            "Loss: 4.751 | Acc: 11.458% (22/192)\n",
            "Loss: 4.940 | Acc: 12.109% (31/256)\n",
            "Loss: 5.226 | Acc: 12.188% (39/320)\n",
            "Loss: 5.167 | Acc: 11.458% (44/384)\n",
            "Loss: 5.122 | Acc: 11.161% (50/448)\n",
            "Loss: 4.992 | Acc: 11.328% (58/512)\n",
            "Loss: 4.785 | Acc: 11.632% (67/576)\n",
            "Loss: 4.601 | Acc: 11.250% (72/640)\n",
            "Loss: 4.457 | Acc: 11.790% (83/704)\n",
            "Loss: 4.312 | Acc: 12.500% (96/768)\n",
            "Loss: 4.180 | Acc: 12.740% (106/832)\n",
            "Loss: 4.057 | Acc: 13.170% (118/896)\n",
            "Loss: 3.967 | Acc: 13.125% (126/960)\n",
            "Loss: 3.888 | Acc: 13.184% (135/1024)\n",
            "Loss: 3.817 | Acc: 13.511% (147/1088)\n",
            "Loss: 3.737 | Acc: 13.802% (159/1152)\n",
            "Loss: 3.664 | Acc: 14.062% (171/1216)\n",
            "Loss: 3.605 | Acc: 14.141% (181/1280)\n",
            "Loss: 3.559 | Acc: 14.211% (191/1344)\n",
            "Loss: 3.504 | Acc: 14.134% (199/1408)\n",
            "Loss: 3.455 | Acc: 13.791% (203/1472)\n",
            "Loss: 3.418 | Acc: 13.542% (208/1536)\n",
            "Loss: 3.375 | Acc: 13.688% (219/1600)\n",
            "Loss: 3.331 | Acc: 13.822% (230/1664)\n",
            "Loss: 3.292 | Acc: 13.773% (238/1728)\n",
            "Loss: 3.250 | Acc: 13.783% (247/1792)\n",
            "Loss: 3.216 | Acc: 14.062% (261/1856)\n",
            "Loss: 3.180 | Acc: 14.375% (276/1920)\n",
            "Loss: 3.155 | Acc: 14.264% (283/1984)\n",
            "Loss: 3.127 | Acc: 14.355% (294/2048)\n",
            "Loss: 3.100 | Acc: 14.394% (304/2112)\n",
            "Loss: 3.073 | Acc: 14.430% (314/2176)\n",
            "Loss: 3.043 | Acc: 14.732% (330/2240)\n",
            "Loss: 3.022 | Acc: 14.757% (340/2304)\n",
            "Loss: 2.999 | Acc: 15.034% (356/2368)\n",
            "Loss: 2.974 | Acc: 15.173% (369/2432)\n",
            "Loss: 2.953 | Acc: 15.304% (382/2496)\n",
            "Loss: 2.930 | Acc: 15.547% (398/2560)\n",
            "Loss: 2.911 | Acc: 15.739% (413/2624)\n",
            "Loss: 2.895 | Acc: 15.737% (423/2688)\n",
            "Loss: 2.874 | Acc: 15.952% (439/2752)\n",
            "Loss: 2.857 | Acc: 16.158% (455/2816)\n",
            "Loss: 2.844 | Acc: 16.076% (463/2880)\n",
            "Loss: 2.828 | Acc: 16.101% (474/2944)\n",
            "Loss: 2.817 | Acc: 16.190% (487/3008)\n",
            "Loss: 2.800 | Acc: 16.374% (503/3072)\n",
            "Loss: 2.786 | Acc: 16.550% (519/3136)\n",
            "Loss: 2.775 | Acc: 16.625% (532/3200)\n",
            "Loss: 2.764 | Acc: 16.697% (545/3264)\n",
            "Loss: 2.749 | Acc: 16.647% (554/3328)\n",
            "Loss: 2.735 | Acc: 16.716% (567/3392)\n",
            "Loss: 2.727 | Acc: 16.696% (577/3456)\n",
            "Loss: 2.717 | Acc: 16.875% (594/3520)\n",
            "Loss: 2.705 | Acc: 17.132% (614/3584)\n",
            "Loss: 2.692 | Acc: 17.188% (627/3648)\n",
            "Loss: 2.680 | Acc: 17.403% (646/3712)\n",
            "Loss: 2.669 | Acc: 17.532% (662/3776)\n",
            "Loss: 2.661 | Acc: 17.630% (677/3840)\n",
            "Loss: 2.652 | Acc: 17.777% (694/3904)\n",
            "Loss: 2.642 | Acc: 17.893% (710/3968)\n",
            "Loss: 2.632 | Acc: 18.006% (726/4032)\n",
            "Loss: 2.622 | Acc: 18.188% (745/4096)\n",
            "Loss: 2.611 | Acc: 18.365% (764/4160)\n",
            "Loss: 2.601 | Acc: 18.466% (780/4224)\n",
            "Loss: 2.593 | Acc: 18.540% (795/4288)\n",
            "Loss: 2.588 | Acc: 18.635% (811/4352)\n",
            "Loss: 2.578 | Acc: 18.773% (829/4416)\n",
            "Loss: 2.570 | Acc: 18.862% (845/4480)\n",
            "Loss: 2.565 | Acc: 18.860% (857/4544)\n",
            "Loss: 2.560 | Acc: 18.793% (866/4608)\n",
            "Loss: 2.552 | Acc: 18.771% (877/4672)\n",
            "Loss: 2.543 | Acc: 19.067% (903/4736)\n",
            "Loss: 2.535 | Acc: 19.208% (922/4800)\n",
            "Loss: 2.529 | Acc: 19.264% (937/4864)\n",
            "Loss: 2.525 | Acc: 19.196% (946/4928)\n",
            "Loss: 2.519 | Acc: 19.331% (965/4992)\n",
            "Loss: 2.515 | Acc: 19.304% (976/5056)\n",
            "Loss: 2.511 | Acc: 19.219% (984/5120)\n",
            "Loss: 2.502 | Acc: 19.367% (1004/5184)\n",
            "Loss: 2.493 | Acc: 19.607% (1029/5248)\n",
            "Loss: 2.486 | Acc: 19.691% (1046/5312)\n",
            "Loss: 2.482 | Acc: 19.699% (1059/5376)\n",
            "Loss: 2.476 | Acc: 19.798% (1077/5440)\n",
            "Loss: 2.471 | Acc: 19.913% (1096/5504)\n",
            "Loss: 2.465 | Acc: 19.935% (1110/5568)\n",
            "Loss: 2.457 | Acc: 20.099% (1132/5632)\n",
            "Loss: 2.453 | Acc: 20.102% (1145/5696)\n",
            "Loss: 2.448 | Acc: 20.069% (1156/5760)\n",
            "Loss: 2.441 | Acc: 20.141% (1173/5824)\n",
            "Loss: 2.437 | Acc: 20.126% (1185/5888)\n",
            "Loss: 2.431 | Acc: 20.296% (1208/5952)\n",
            "Loss: 2.424 | Acc: 20.396% (1227/6016)\n",
            "Loss: 2.417 | Acc: 20.493% (1246/6080)\n",
            "Loss: 2.413 | Acc: 20.459% (1257/6144)\n",
            "Loss: 2.409 | Acc: 20.619% (1280/6208)\n",
            "Loss: 2.404 | Acc: 20.711% (1299/6272)\n",
            "Loss: 2.400 | Acc: 20.770% (1316/6336)\n",
            "Loss: 2.396 | Acc: 20.797% (1331/6400)\n",
            "Loss: 2.391 | Acc: 20.823% (1346/6464)\n",
            "Loss: 2.388 | Acc: 20.895% (1364/6528)\n",
            "Loss: 2.384 | Acc: 20.919% (1379/6592)\n",
            "Loss: 2.380 | Acc: 21.004% (1398/6656)\n",
            "Loss: 2.375 | Acc: 20.982% (1410/6720)\n",
            "Loss: 2.371 | Acc: 21.064% (1429/6784)\n",
            "Loss: 2.370 | Acc: 20.999% (1438/6848)\n",
            "Loss: 2.367 | Acc: 21.079% (1457/6912)\n",
            "Loss: 2.362 | Acc: 21.173% (1477/6976)\n",
            "Loss: 2.358 | Acc: 21.207% (1493/7040)\n",
            "Loss: 2.353 | Acc: 21.227% (1508/7104)\n",
            "Loss: 2.352 | Acc: 21.289% (1526/7168)\n",
            "Loss: 2.347 | Acc: 21.350% (1544/7232)\n",
            "Loss: 2.345 | Acc: 21.464% (1566/7296)\n",
            "Loss: 2.342 | Acc: 21.413% (1576/7360)\n",
            "Loss: 2.337 | Acc: 21.538% (1599/7424)\n",
            "Loss: 2.334 | Acc: 21.595% (1617/7488)\n",
            "Loss: 2.331 | Acc: 21.663% (1636/7552)\n",
            "Loss: 2.328 | Acc: 21.717% (1654/7616)\n",
            "Loss: 2.326 | Acc: 21.745% (1670/7680)\n",
            "Loss: 2.324 | Acc: 21.772% (1686/7744)\n",
            "Loss: 2.320 | Acc: 21.849% (1706/7808)\n",
            "Loss: 2.317 | Acc: 21.875% (1722/7872)\n",
            "Loss: 2.314 | Acc: 21.888% (1737/7936)\n",
            "Loss: 2.312 | Acc: 21.925% (1754/8000)\n",
            "Loss: 2.308 | Acc: 22.073% (1780/8064)\n",
            "Loss: 2.305 | Acc: 22.084% (1795/8128)\n",
            "Loss: 2.301 | Acc: 22.156% (1815/8192)\n",
            "Loss: 2.298 | Acc: 22.190% (1832/8256)\n",
            "Loss: 2.296 | Acc: 22.224% (1849/8320)\n",
            "Loss: 2.293 | Acc: 22.257% (1866/8384)\n",
            "Loss: 2.291 | Acc: 22.360% (1889/8448)\n",
            "Loss: 2.290 | Acc: 22.404% (1907/8512)\n",
            "Loss: 2.286 | Acc: 22.540% (1933/8576)\n",
            "Loss: 2.283 | Acc: 22.604% (1953/8640)\n",
            "Loss: 2.281 | Acc: 22.622% (1969/8704)\n",
            "Loss: 2.277 | Acc: 22.742% (1994/8768)\n",
            "Loss: 2.273 | Acc: 22.792% (2013/8832)\n",
            "Loss: 2.273 | Acc: 22.853% (2033/8896)\n",
            "Loss: 2.271 | Acc: 22.891% (2051/8960)\n",
            "Loss: 2.268 | Acc: 22.917% (2068/9024)\n",
            "Loss: 2.266 | Acc: 22.975% (2088/9088)\n",
            "Loss: 2.264 | Acc: 22.979% (2103/9152)\n",
            "Loss: 2.263 | Acc: 23.014% (2121/9216)\n",
            "Loss: 2.260 | Acc: 23.103% (2144/9280)\n",
            "Loss: 2.258 | Acc: 23.127% (2161/9344)\n",
            "Loss: 2.256 | Acc: 23.119% (2175/9408)\n",
            "Loss: 2.253 | Acc: 23.226% (2200/9472)\n",
            "Loss: 2.250 | Acc: 23.343% (2226/9536)\n",
            "Loss: 2.249 | Acc: 23.323% (2239/9600)\n",
            "Loss: 2.246 | Acc: 23.344% (2256/9664)\n",
            "Loss: 2.243 | Acc: 23.458% (2282/9728)\n",
            "Loss: 2.241 | Acc: 23.489% (2300/9792)\n",
            "Loss: 2.239 | Acc: 23.509% (2317/9856)\n",
            "Loss: 2.236 | Acc: 23.569% (2338/9920)\n",
            "Loss: 2.234 | Acc: 23.568% (2353/9984)\n",
            "Loss: 2.232 | Acc: 23.547% (2366/10048)\n",
            "Loss: 2.230 | Acc: 23.546% (2381/10112)\n",
            "Loss: 2.228 | Acc: 23.585% (2400/10176)\n",
            "Loss: 2.228 | Acc: 23.623% (2419/10240)\n",
            "Loss: 2.225 | Acc: 23.719% (2444/10304)\n",
            "Loss: 2.223 | Acc: 23.708% (2458/10368)\n",
            "Loss: 2.221 | Acc: 23.754% (2478/10432)\n",
            "Loss: 2.219 | Acc: 23.809% (2499/10496)\n",
            "Loss: 2.217 | Acc: 23.826% (2516/10560)\n",
            "Loss: 2.214 | Acc: 23.814% (2530/10624)\n",
            "Loss: 2.212 | Acc: 23.859% (2550/10688)\n",
            "Loss: 2.211 | Acc: 23.837% (2563/10752)\n",
            "Loss: 2.209 | Acc: 23.817% (2576/10816)\n",
            "Loss: 2.207 | Acc: 23.888% (2599/10880)\n",
            "Loss: 2.203 | Acc: 24.022% (2629/10944)\n",
            "Loss: 2.201 | Acc: 24.101% (2653/11008)\n",
            "Loss: 2.198 | Acc: 24.160% (2675/11072)\n",
            "Loss: 2.196 | Acc: 24.255% (2701/11136)\n",
            "Loss: 2.195 | Acc: 24.295% (2721/11200)\n",
            "Loss: 2.194 | Acc: 24.325% (2740/11264)\n",
            "Loss: 2.192 | Acc: 24.364% (2760/11328)\n",
            "Loss: 2.189 | Acc: 24.447% (2785/11392)\n",
            "Loss: 2.189 | Acc: 24.441% (2800/11456)\n",
            "Loss: 2.188 | Acc: 24.462% (2818/11520)\n",
            "Loss: 2.187 | Acc: 24.542% (2843/11584)\n",
            "Loss: 2.186 | Acc: 24.596% (2865/11648)\n",
            "Loss: 2.184 | Acc: 24.607% (2882/11712)\n",
            "Loss: 2.182 | Acc: 24.669% (2905/11776)\n",
            "Loss: 2.180 | Acc: 24.730% (2928/11840)\n",
            "Loss: 2.179 | Acc: 24.773% (2949/11904)\n",
            "Loss: 2.177 | Acc: 24.808% (2969/11968)\n",
            "Loss: 2.175 | Acc: 24.850% (2990/12032)\n",
            "Loss: 2.172 | Acc: 24.934% (3016/12096)\n",
            "Loss: 2.170 | Acc: 25.049% (3046/12160)\n",
            "Loss: 2.168 | Acc: 25.147% (3074/12224)\n",
            "Loss: 2.166 | Acc: 25.179% (3094/12288)\n",
            "Loss: 2.165 | Acc: 25.227% (3116/12352)\n",
            "Loss: 2.163 | Acc: 25.282% (3139/12416)\n",
            "Loss: 2.161 | Acc: 25.337% (3162/12480)\n",
            "Loss: 2.158 | Acc: 25.438% (3191/12544)\n",
            "Loss: 2.157 | Acc: 25.452% (3209/12608)\n",
            "Loss: 2.156 | Acc: 25.481% (3229/12672)\n",
            "Loss: 2.153 | Acc: 25.550% (3254/12736)\n",
            "Loss: 2.153 | Acc: 25.570% (3273/12800)\n",
            "Loss: 2.152 | Acc: 25.599% (3293/12864)\n",
            "Loss: 2.149 | Acc: 25.681% (3320/12928)\n",
            "Loss: 2.148 | Acc: 25.724% (3342/12992)\n",
            "Loss: 2.145 | Acc: 25.812% (3370/13056)\n",
            "Loss: 2.144 | Acc: 25.793% (3384/13120)\n",
            "Loss: 2.144 | Acc: 25.827% (3405/13184)\n",
            "Loss: 2.143 | Acc: 25.845% (3424/13248)\n",
            "Loss: 2.141 | Acc: 25.886% (3446/13312)\n",
            "Loss: 2.140 | Acc: 25.920% (3467/13376)\n",
            "Loss: 2.139 | Acc: 25.930% (3485/13440)\n",
            "Loss: 2.137 | Acc: 25.985% (3509/13504)\n",
            "Loss: 2.136 | Acc: 26.017% (3530/13568)\n",
            "Loss: 2.134 | Acc: 26.086% (3556/13632)\n",
            "Loss: 2.133 | Acc: 26.154% (3582/13696)\n",
            "Loss: 2.131 | Acc: 26.163% (3600/13760)\n",
            "Loss: 2.129 | Acc: 26.237% (3627/13824)\n",
            "Loss: 2.128 | Acc: 26.260% (3647/13888)\n",
            "Loss: 2.126 | Acc: 26.304% (3670/13952)\n",
            "Loss: 2.125 | Acc: 26.313% (3688/14016)\n",
            "Loss: 2.123 | Acc: 26.357% (3711/14080)\n",
            "Loss: 2.122 | Acc: 26.372% (3730/14144)\n",
            "Loss: 2.122 | Acc: 26.344% (3743/14208)\n",
            "Loss: 2.120 | Acc: 26.401% (3768/14272)\n",
            "Loss: 2.119 | Acc: 26.402% (3785/14336)\n",
            "Loss: 2.117 | Acc: 26.444% (3808/14400)\n",
            "Loss: 2.116 | Acc: 26.431% (3823/14464)\n",
            "Loss: 2.113 | Acc: 26.507% (3851/14528)\n",
            "Loss: 2.112 | Acc: 26.528% (3871/14592)\n",
            "Loss: 2.110 | Acc: 26.597% (3898/14656)\n",
            "Loss: 2.110 | Acc: 26.637% (3921/14720)\n",
            "Loss: 2.108 | Acc: 26.677% (3944/14784)\n",
            "Loss: 2.107 | Acc: 26.717% (3967/14848)\n",
            "Loss: 2.105 | Acc: 26.750% (3989/14912)\n",
            "Loss: 2.103 | Acc: 26.803% (4014/14976)\n",
            "Loss: 2.102 | Acc: 26.822% (4034/15040)\n",
            "Loss: 2.100 | Acc: 26.847% (4055/15104)\n",
            "Loss: 2.099 | Acc: 26.879% (4077/15168)\n",
            "Loss: 2.098 | Acc: 26.891% (4096/15232)\n",
            "Loss: 2.097 | Acc: 26.916% (4117/15296)\n",
            "Loss: 2.095 | Acc: 26.986% (4145/15360)\n",
            "Loss: 2.095 | Acc: 26.964% (4159/15424)\n",
            "Loss: 2.095 | Acc: 26.931% (4171/15488)\n",
            "Loss: 2.095 | Acc: 26.935% (4189/15552)\n",
            "Loss: 2.093 | Acc: 26.992% (4215/15616)\n",
            "Loss: 2.092 | Acc: 26.996% (4233/15680)\n",
            "Loss: 2.091 | Acc: 27.039% (4257/15744)\n",
            "Loss: 2.090 | Acc: 27.043% (4275/15808)\n",
            "Loss: 2.089 | Acc: 27.060% (4295/15872)\n",
            "Loss: 2.088 | Acc: 27.052% (4311/15936)\n",
            "Loss: 2.087 | Acc: 27.056% (4329/16000)\n",
            "Loss: 2.086 | Acc: 27.061% (4347/16064)\n",
            "Loss: 2.085 | Acc: 27.090% (4369/16128)\n",
            "Loss: 2.084 | Acc: 27.087% (4386/16192)\n",
            "Loss: 2.084 | Acc: 27.067% (4400/16256)\n",
            "Loss: 2.083 | Acc: 27.089% (4421/16320)\n",
            "Loss: 2.082 | Acc: 27.148% (4448/16384)\n",
            "Loss: 2.081 | Acc: 27.183% (4471/16448)\n",
            "Loss: 2.080 | Acc: 27.204% (4492/16512)\n",
            "Loss: 2.079 | Acc: 27.238% (4515/16576)\n",
            "Loss: 2.078 | Acc: 27.272% (4538/16640)\n",
            "Loss: 2.077 | Acc: 27.281% (4557/16704)\n",
            "Loss: 2.077 | Acc: 27.266% (4572/16768)\n",
            "Loss: 2.076 | Acc: 27.269% (4590/16832)\n",
            "Loss: 2.075 | Acc: 27.326% (4617/16896)\n",
            "Loss: 2.074 | Acc: 27.341% (4637/16960)\n",
            "Loss: 2.074 | Acc: 27.350% (4656/17024)\n",
            "Loss: 2.072 | Acc: 27.393% (4681/17088)\n",
            "Loss: 2.071 | Acc: 27.402% (4700/17152)\n",
            "Loss: 2.070 | Acc: 27.416% (4720/17216)\n",
            "Loss: 2.069 | Acc: 27.425% (4739/17280)\n",
            "Loss: 2.069 | Acc: 27.450% (4761/17344)\n",
            "Loss: 2.067 | Acc: 27.505% (4788/17408)\n",
            "Loss: 2.066 | Acc: 27.524% (4809/17472)\n",
            "Loss: 2.067 | Acc: 27.521% (4826/17536)\n",
            "Loss: 2.065 | Acc: 27.562% (4851/17600)\n",
            "Loss: 2.064 | Acc: 27.565% (4869/17664)\n",
            "Loss: 2.063 | Acc: 27.567% (4887/17728)\n",
            "Loss: 2.062 | Acc: 27.585% (4908/17792)\n",
            "Loss: 2.062 | Acc: 27.554% (4920/17856)\n",
            "Loss: 2.061 | Acc: 27.578% (4942/17920)\n",
            "Loss: 2.060 | Acc: 27.586% (4961/17984)\n",
            "Loss: 2.060 | Acc: 27.582% (4978/18048)\n",
            "Loss: 2.059 | Acc: 27.612% (5001/18112)\n",
            "Loss: 2.057 | Acc: 27.646% (5025/18176)\n",
            "Loss: 2.056 | Acc: 27.648% (5043/18240)\n",
            "Loss: 2.055 | Acc: 27.710% (5072/18304)\n",
            "Loss: 2.054 | Acc: 27.738% (5095/18368)\n",
            "Loss: 2.053 | Acc: 27.799% (5124/18432)\n",
            "Loss: 2.051 | Acc: 27.871% (5155/18496)\n",
            "Loss: 2.050 | Acc: 27.872% (5173/18560)\n",
            "Loss: 2.049 | Acc: 27.899% (5196/18624)\n",
            "Loss: 2.047 | Acc: 27.943% (5222/18688)\n",
            "Loss: 2.046 | Acc: 27.981% (5247/18752)\n",
            "Loss: 2.045 | Acc: 28.013% (5271/18816)\n",
            "Loss: 2.043 | Acc: 28.046% (5295/18880)\n",
            "Loss: 2.042 | Acc: 28.083% (5320/18944)\n",
            "Loss: 2.041 | Acc: 28.093% (5340/19008)\n",
            "Loss: 2.041 | Acc: 28.109% (5361/19072)\n",
            "Loss: 2.039 | Acc: 28.172% (5391/19136)\n",
            "Loss: 2.037 | Acc: 28.245% (5423/19200)\n",
            "Loss: 2.036 | Acc: 28.291% (5450/19264)\n",
            "Loss: 2.035 | Acc: 28.337% (5477/19328)\n",
            "Loss: 2.034 | Acc: 28.342% (5496/19392)\n",
            "Loss: 2.033 | Acc: 28.382% (5522/19456)\n",
            "Loss: 2.032 | Acc: 28.397% (5543/19520)\n",
            "Loss: 2.032 | Acc: 28.370% (5556/19584)\n",
            "Loss: 2.031 | Acc: 28.400% (5580/19648)\n",
            "Loss: 2.029 | Acc: 28.475% (5613/19712)\n",
            "Loss: 2.028 | Acc: 28.499% (5636/19776)\n",
            "Loss: 2.027 | Acc: 28.528% (5660/19840)\n",
            "Loss: 2.025 | Acc: 28.562% (5685/19904)\n",
            "Loss: 2.024 | Acc: 28.601% (5711/19968)\n",
            "Loss: 2.023 | Acc: 28.614% (5732/20032)\n",
            "Loss: 2.023 | Acc: 28.652% (5758/20096)\n",
            "Loss: 2.022 | Acc: 28.690% (5784/20160)\n",
            "Loss: 2.021 | Acc: 28.704% (5805/20224)\n",
            "Loss: 2.020 | Acc: 28.721% (5827/20288)\n",
            "Loss: 2.019 | Acc: 28.729% (5847/20352)\n",
            "Loss: 2.019 | Acc: 28.723% (5864/20416)\n",
            "Loss: 2.018 | Acc: 28.750% (5888/20480)\n",
            "Loss: 2.017 | Acc: 28.743% (5905/20544)\n",
            "Loss: 2.017 | Acc: 28.766% (5928/20608)\n",
            "Loss: 2.016 | Acc: 28.802% (5954/20672)\n",
            "Loss: 2.015 | Acc: 28.795% (5971/20736)\n",
            "Loss: 2.015 | Acc: 28.779% (5986/20800)\n",
            "Loss: 2.014 | Acc: 28.791% (6007/20864)\n",
            "Loss: 2.013 | Acc: 28.804% (6028/20928)\n",
            "Loss: 2.012 | Acc: 28.816% (6049/20992)\n",
            "Loss: 2.011 | Acc: 28.833% (6071/21056)\n",
            "Loss: 2.010 | Acc: 28.845% (6092/21120)\n",
            "Loss: 2.009 | Acc: 28.866% (6115/21184)\n",
            "Loss: 2.008 | Acc: 28.892% (6139/21248)\n",
            "Loss: 2.008 | Acc: 28.918% (6163/21312)\n",
            "Loss: 2.006 | Acc: 28.958% (6190/21376)\n",
            "Loss: 2.005 | Acc: 28.960% (6209/21440)\n",
            "Loss: 2.004 | Acc: 28.999% (6236/21504)\n",
            "Loss: 2.003 | Acc: 29.043% (6264/21568)\n",
            "Loss: 2.002 | Acc: 29.063% (6287/21632)\n",
            "Loss: 2.001 | Acc: 29.079% (6309/21696)\n",
            "Loss: 2.000 | Acc: 29.099% (6332/21760)\n",
            "Loss: 1.999 | Acc: 29.138% (6359/21824)\n",
            "Loss: 1.998 | Acc: 29.162% (6383/21888)\n",
            "Loss: 1.997 | Acc: 29.177% (6405/21952)\n",
            "Loss: 1.997 | Acc: 29.206% (6430/22016)\n",
            "Loss: 1.997 | Acc: 29.216% (6451/22080)\n",
            "Loss: 1.996 | Acc: 29.268% (6481/22144)\n",
            "Loss: 1.995 | Acc: 29.296% (6506/22208)\n",
            "Loss: 1.995 | Acc: 29.274% (6520/22272)\n",
            "Loss: 1.995 | Acc: 29.262% (6536/22336)\n",
            "Loss: 1.994 | Acc: 29.272% (6557/22400)\n",
            "Loss: 1.993 | Acc: 29.314% (6585/22464)\n",
            "Loss: 1.993 | Acc: 29.328% (6607/22528)\n",
            "Loss: 1.992 | Acc: 29.333% (6627/22592)\n",
            "Loss: 1.991 | Acc: 29.387% (6658/22656)\n",
            "Loss: 1.990 | Acc: 29.415% (6683/22720)\n",
            "Loss: 1.989 | Acc: 29.429% (6705/22784)\n",
            "Loss: 1.988 | Acc: 29.456% (6730/22848)\n",
            "Loss: 1.988 | Acc: 29.474% (6753/22912)\n",
            "Loss: 1.988 | Acc: 29.492% (6776/22976)\n",
            "Loss: 1.987 | Acc: 29.536% (6805/23040)\n",
            "Loss: 1.986 | Acc: 29.558% (6829/23104)\n",
            "Loss: 1.985 | Acc: 29.580% (6853/23168)\n",
            "Loss: 1.984 | Acc: 29.597% (6876/23232)\n",
            "Loss: 1.983 | Acc: 29.636% (6904/23296)\n",
            "Loss: 1.983 | Acc: 29.628% (6921/23360)\n",
            "Loss: 1.981 | Acc: 29.666% (6949/23424)\n",
            "Loss: 1.981 | Acc: 29.679% (6971/23488)\n",
            "Loss: 1.981 | Acc: 29.671% (6988/23552)\n",
            "Loss: 1.981 | Acc: 29.662% (7005/23616)\n",
            "Loss: 1.980 | Acc: 29.709% (7035/23680)\n",
            "Loss: 1.979 | Acc: 29.746% (7063/23744)\n",
            "Loss: 1.978 | Acc: 29.763% (7086/23808)\n",
            "Loss: 1.978 | Acc: 29.780% (7109/23872)\n",
            "Loss: 1.977 | Acc: 29.825% (7139/23936)\n",
            "Loss: 1.976 | Acc: 29.842% (7162/24000)\n",
            "Loss: 1.975 | Acc: 29.879% (7190/24064)\n",
            "Loss: 1.975 | Acc: 29.878% (7209/24128)\n",
            "Loss: 1.974 | Acc: 29.898% (7233/24192)\n",
            "Loss: 1.973 | Acc: 29.931% (7260/24256)\n",
            "Loss: 1.972 | Acc: 29.942% (7282/24320)\n",
            "Loss: 1.972 | Acc: 29.938% (7300/24384)\n",
            "Loss: 1.971 | Acc: 29.949% (7322/24448)\n",
            "Loss: 1.970 | Acc: 29.993% (7352/24512)\n",
            "Loss: 1.969 | Acc: 30.029% (7380/24576)\n",
            "Loss: 1.969 | Acc: 30.024% (7398/24640)\n",
            "Loss: 1.968 | Acc: 30.044% (7422/24704)\n",
            "Loss: 1.967 | Acc: 30.063% (7446/24768)\n",
            "Loss: 1.966 | Acc: 30.118% (7479/24832)\n",
            "Loss: 1.964 | Acc: 30.145% (7505/24896)\n",
            "Loss: 1.964 | Acc: 30.152% (7526/24960)\n",
            "Loss: 1.964 | Acc: 30.175% (7551/25024)\n",
            "Loss: 1.964 | Acc: 30.186% (7573/25088)\n",
            "Loss: 1.962 | Acc: 30.212% (7599/25152)\n",
            "Loss: 1.962 | Acc: 30.263% (7631/25216)\n",
            "Loss: 1.961 | Acc: 30.281% (7655/25280)\n",
            "Loss: 1.960 | Acc: 30.315% (7683/25344)\n",
            "Loss: 1.959 | Acc: 30.357% (7713/25408)\n",
            "Loss: 1.958 | Acc: 30.375% (7737/25472)\n",
            "Loss: 1.957 | Acc: 30.381% (7758/25536)\n",
            "Loss: 1.956 | Acc: 30.398% (7782/25600)\n",
            "Loss: 1.956 | Acc: 30.389% (7799/25664)\n",
            "Loss: 1.956 | Acc: 30.399% (7821/25728)\n",
            "Loss: 1.956 | Acc: 30.413% (7844/25792)\n",
            "Loss: 1.956 | Acc: 30.411% (7863/25856)\n",
            "Loss: 1.955 | Acc: 30.417% (7884/25920)\n",
            "Loss: 1.954 | Acc: 30.430% (7907/25984)\n",
            "Loss: 1.954 | Acc: 30.425% (7925/26048)\n",
            "Loss: 1.953 | Acc: 30.461% (7954/26112)\n",
            "Loss: 1.953 | Acc: 30.452% (7971/26176)\n",
            "Loss: 1.952 | Acc: 30.438% (7987/26240)\n",
            "Loss: 1.951 | Acc: 30.463% (8013/26304)\n",
            "Loss: 1.951 | Acc: 30.484% (8038/26368)\n",
            "Loss: 1.950 | Acc: 30.501% (8062/26432)\n",
            "Loss: 1.949 | Acc: 30.525% (8088/26496)\n",
            "Loss: 1.949 | Acc: 30.516% (8105/26560)\n",
            "Loss: 1.948 | Acc: 30.525% (8127/26624)\n",
            "Loss: 1.947 | Acc: 30.546% (8152/26688)\n",
            "Loss: 1.947 | Acc: 30.573% (8179/26752)\n",
            "Loss: 1.946 | Acc: 30.590% (8203/26816)\n",
            "Loss: 1.946 | Acc: 30.606% (8227/26880)\n",
            "Loss: 1.946 | Acc: 30.619% (8250/26944)\n",
            "Loss: 1.945 | Acc: 30.635% (8274/27008)\n",
            "Loss: 1.944 | Acc: 30.666% (8302/27072)\n",
            "Loss: 1.943 | Acc: 30.679% (8325/27136)\n",
            "Loss: 1.943 | Acc: 30.695% (8349/27200)\n",
            "Loss: 1.943 | Acc: 30.707% (8372/27264)\n",
            "Loss: 1.942 | Acc: 30.716% (8394/27328)\n",
            "Loss: 1.943 | Acc: 30.688% (8406/27392)\n",
            "Loss: 1.942 | Acc: 30.715% (8433/27456)\n",
            "Loss: 1.941 | Acc: 30.730% (8457/27520)\n",
            "Loss: 1.941 | Acc: 30.728% (8476/27584)\n",
            "Loss: 1.940 | Acc: 30.726% (8495/27648)\n",
            "Loss: 1.940 | Acc: 30.748% (8521/27712)\n",
            "Loss: 1.940 | Acc: 30.732% (8536/27776)\n",
            "Loss: 1.939 | Acc: 30.751% (8561/27840)\n",
            "Loss: 1.938 | Acc: 30.809% (8597/27904)\n",
            "Loss: 1.937 | Acc: 30.821% (8620/27968)\n",
            "Loss: 1.936 | Acc: 30.836% (8644/28032)\n",
            "Loss: 1.936 | Acc: 30.855% (8669/28096)\n",
            "Loss: 1.935 | Acc: 30.874% (8694/28160)\n",
            "Loss: 1.934 | Acc: 30.899% (8721/28224)\n",
            "Loss: 1.934 | Acc: 30.921% (8747/28288)\n",
            "Loss: 1.933 | Acc: 30.978% (8783/28352)\n",
            "Loss: 1.932 | Acc: 30.993% (8807/28416)\n",
            "Loss: 1.931 | Acc: 31.022% (8835/28480)\n",
            "Loss: 1.930 | Acc: 31.064% (8867/28544)\n",
            "Loss: 1.929 | Acc: 31.103% (8898/28608)\n",
            "Loss: 1.928 | Acc: 31.128% (8925/28672)\n",
            "Loss: 1.928 | Acc: 31.153% (8952/28736)\n",
            "Loss: 1.927 | Acc: 31.188% (8982/28800)\n",
            "Loss: 1.926 | Acc: 31.188% (9002/28864)\n",
            "Loss: 1.926 | Acc: 31.205% (9027/28928)\n",
            "Loss: 1.926 | Acc: 31.219% (9051/28992)\n",
            "Loss: 1.925 | Acc: 31.226% (9073/29056)\n",
            "Loss: 1.925 | Acc: 31.229% (9094/29120)\n",
            "Loss: 1.924 | Acc: 31.277% (9128/29184)\n",
            "Loss: 1.924 | Acc: 31.291% (9152/29248)\n",
            "Loss: 1.923 | Acc: 31.332% (9184/29312)\n",
            "Loss: 1.922 | Acc: 31.335% (9205/29376)\n",
            "Loss: 1.922 | Acc: 31.359% (9232/29440)\n",
            "Loss: 1.921 | Acc: 31.369% (9255/29504)\n",
            "Loss: 1.920 | Acc: 31.389% (9281/29568)\n",
            "Loss: 1.919 | Acc: 31.419% (9310/29632)\n",
            "Loss: 1.919 | Acc: 31.432% (9334/29696)\n",
            "Loss: 1.918 | Acc: 31.448% (9359/29760)\n",
            "Loss: 1.918 | Acc: 31.461% (9383/29824)\n",
            "Loss: 1.917 | Acc: 31.461% (9403/29888)\n",
            "Loss: 1.917 | Acc: 31.497% (9434/29952)\n",
            "Loss: 1.916 | Acc: 31.513% (9459/30016)\n",
            "Loss: 1.915 | Acc: 31.539% (9487/30080)\n",
            "Loss: 1.915 | Acc: 31.552% (9511/30144)\n",
            "Loss: 1.915 | Acc: 31.568% (9536/30208)\n",
            "Loss: 1.915 | Acc: 31.580% (9560/30272)\n",
            "Loss: 1.915 | Acc: 31.570% (9577/30336)\n",
            "Loss: 1.914 | Acc: 31.609% (9609/30400)\n",
            "Loss: 1.913 | Acc: 31.618% (9632/30464)\n",
            "Loss: 1.912 | Acc: 31.643% (9660/30528)\n",
            "Loss: 1.912 | Acc: 31.652% (9683/30592)\n",
            "Loss: 1.911 | Acc: 31.654% (9704/30656)\n",
            "Loss: 1.911 | Acc: 31.676% (9731/30720)\n",
            "Loss: 1.910 | Acc: 31.692% (9756/30784)\n",
            "Loss: 1.910 | Acc: 31.701% (9779/30848)\n",
            "Loss: 1.909 | Acc: 31.729% (9808/30912)\n",
            "Loss: 1.909 | Acc: 31.737% (9831/30976)\n",
            "Loss: 1.908 | Acc: 31.759% (9858/31040)\n",
            "Loss: 1.908 | Acc: 31.784% (9886/31104)\n",
            "Loss: 1.907 | Acc: 31.824% (9919/31168)\n",
            "Loss: 1.906 | Acc: 31.858% (9950/31232)\n",
            "Loss: 1.905 | Acc: 31.879% (9977/31296)\n",
            "Loss: 1.905 | Acc: 31.888% (10000/31360)\n",
            "Loss: 1.905 | Acc: 31.886% (10020/31424)\n",
            "Loss: 1.905 | Acc: 31.888% (10041/31488)\n",
            "Loss: 1.904 | Acc: 31.906% (10067/31552)\n",
            "Loss: 1.904 | Acc: 31.933% (10096/31616)\n",
            "Loss: 1.902 | Acc: 31.985% (10133/31680)\n",
            "Loss: 1.902 | Acc: 31.993% (10156/31744)\n",
            "Loss: 1.901 | Acc: 32.023% (10186/31808)\n",
            "Loss: 1.900 | Acc: 32.047% (10214/31872)\n",
            "Loss: 1.899 | Acc: 32.055% (10237/31936)\n",
            "Loss: 1.899 | Acc: 32.075% (10264/32000)\n",
            "Loss: 1.898 | Acc: 32.083% (10287/32064)\n",
            "Loss: 1.897 | Acc: 32.100% (10313/32128)\n",
            "Loss: 1.897 | Acc: 32.114% (10338/32192)\n",
            "Loss: 1.896 | Acc: 32.121% (10361/32256)\n",
            "Loss: 1.896 | Acc: 32.150% (10391/32320)\n",
            "Loss: 1.895 | Acc: 32.152% (10412/32384)\n",
            "Loss: 1.894 | Acc: 32.171% (10439/32448)\n",
            "Loss: 1.894 | Acc: 32.179% (10462/32512)\n",
            "Loss: 1.894 | Acc: 32.189% (10486/32576)\n",
            "Loss: 1.893 | Acc: 32.221% (10517/32640)\n",
            "Loss: 1.892 | Acc: 32.238% (10543/32704)\n",
            "Loss: 1.892 | Acc: 32.260% (10571/32768)\n",
            "Loss: 1.891 | Acc: 32.270% (10595/32832)\n",
            "Loss: 1.891 | Acc: 32.281% (10619/32896)\n",
            "Loss: 1.890 | Acc: 32.303% (10647/32960)\n",
            "Loss: 1.890 | Acc: 32.316% (10672/33024)\n",
            "Loss: 1.889 | Acc: 32.317% (10693/33088)\n",
            "Loss: 1.889 | Acc: 32.327% (10717/33152)\n",
            "Loss: 1.888 | Acc: 32.358% (10748/33216)\n",
            "Loss: 1.888 | Acc: 32.380% (10776/33280)\n",
            "Loss: 1.887 | Acc: 32.381% (10797/33344)\n",
            "Loss: 1.886 | Acc: 32.408% (10827/33408)\n",
            "Loss: 1.886 | Acc: 32.418% (10851/33472)\n",
            "Loss: 1.886 | Acc: 32.428% (10875/33536)\n",
            "Loss: 1.885 | Acc: 32.446% (10902/33600)\n",
            "Loss: 1.885 | Acc: 32.459% (10927/33664)\n",
            "Loss: 1.885 | Acc: 32.460% (10948/33728)\n",
            "Loss: 1.884 | Acc: 32.478% (10975/33792)\n",
            "Loss: 1.883 | Acc: 32.488% (10999/33856)\n",
            "Loss: 1.883 | Acc: 32.503% (11025/33920)\n",
            "Loss: 1.882 | Acc: 32.521% (11052/33984)\n",
            "Loss: 1.882 | Acc: 32.531% (11076/34048)\n",
            "Loss: 1.881 | Acc: 32.531% (11097/34112)\n",
            "Loss: 1.881 | Acc: 32.555% (11126/34176)\n",
            "Loss: 1.880 | Acc: 32.591% (11159/34240)\n",
            "Loss: 1.879 | Acc: 32.626% (11192/34304)\n",
            "Loss: 1.879 | Acc: 32.644% (11219/34368)\n",
            "Loss: 1.878 | Acc: 32.670% (11249/34432)\n",
            "Loss: 1.877 | Acc: 32.679% (11273/34496)\n",
            "Loss: 1.877 | Acc: 32.691% (11298/34560)\n",
            "Loss: 1.877 | Acc: 32.714% (11327/34624)\n",
            "Loss: 1.876 | Acc: 32.726% (11352/34688)\n",
            "Loss: 1.876 | Acc: 32.752% (11382/34752)\n",
            "Loss: 1.875 | Acc: 32.775% (11411/34816)\n",
            "Loss: 1.875 | Acc: 32.781% (11434/34880)\n",
            "Loss: 1.874 | Acc: 32.801% (11462/34944)\n",
            "Loss: 1.874 | Acc: 32.818% (11489/35008)\n",
            "Loss: 1.873 | Acc: 32.827% (11513/35072)\n",
            "Loss: 1.873 | Acc: 32.852% (11543/35136)\n",
            "Loss: 1.872 | Acc: 32.858% (11566/35200)\n",
            "Loss: 1.872 | Acc: 32.864% (11589/35264)\n",
            "Loss: 1.871 | Acc: 32.878% (11615/35328)\n",
            "Loss: 1.871 | Acc: 32.900% (11644/35392)\n",
            "Loss: 1.870 | Acc: 32.920% (11672/35456)\n",
            "Loss: 1.870 | Acc: 32.939% (11700/35520)\n",
            "Loss: 1.870 | Acc: 32.947% (11724/35584)\n",
            "Loss: 1.869 | Acc: 32.970% (11753/35648)\n",
            "Loss: 1.869 | Acc: 32.983% (11779/35712)\n",
            "Loss: 1.868 | Acc: 33.008% (11809/35776)\n",
            "Loss: 1.868 | Acc: 33.027% (11837/35840)\n",
            "Loss: 1.868 | Acc: 33.038% (11862/35904)\n",
            "Loss: 1.867 | Acc: 33.046% (11886/35968)\n",
            "Loss: 1.867 | Acc: 33.054% (11910/36032)\n",
            "Loss: 1.866 | Acc: 33.051% (11930/36096)\n",
            "Loss: 1.866 | Acc: 33.061% (11955/36160)\n",
            "Loss: 1.865 | Acc: 33.078% (11982/36224)\n",
            "Loss: 1.865 | Acc: 33.096% (12010/36288)\n",
            "Loss: 1.865 | Acc: 33.101% (12033/36352)\n",
            "Loss: 1.864 | Acc: 33.093% (12051/36416)\n",
            "Loss: 1.864 | Acc: 33.084% (12069/36480)\n",
            "Loss: 1.864 | Acc: 33.097% (12095/36544)\n",
            "Loss: 1.863 | Acc: 33.094% (12115/36608)\n",
            "Loss: 1.863 | Acc: 33.104% (12140/36672)\n",
            "Loss: 1.862 | Acc: 33.126% (12169/36736)\n",
            "Loss: 1.862 | Acc: 33.141% (12196/36800)\n",
            "Loss: 1.861 | Acc: 33.152% (12221/36864)\n",
            "Loss: 1.860 | Acc: 33.175% (12251/36928)\n",
            "Loss: 1.860 | Acc: 33.210% (12285/36992)\n",
            "Loss: 1.859 | Acc: 33.217% (12309/37056)\n",
            "Loss: 1.860 | Acc: 33.222% (12332/37120)\n",
            "Loss: 1.859 | Acc: 33.227% (12355/37184)\n",
            "Loss: 1.859 | Acc: 33.247% (12384/37248)\n",
            "Loss: 1.859 | Acc: 33.239% (12402/37312)\n",
            "Loss: 1.858 | Acc: 33.241% (12424/37376)\n",
            "Loss: 1.858 | Acc: 33.256% (12451/37440)\n",
            "Loss: 1.857 | Acc: 33.300% (12489/37504)\n",
            "Loss: 1.856 | Acc: 33.297% (12509/37568)\n",
            "Loss: 1.856 | Acc: 33.317% (12538/37632)\n",
            "Loss: 1.855 | Acc: 33.332% (12565/37696)\n",
            "Loss: 1.855 | Acc: 33.324% (12583/37760)\n",
            "Loss: 1.855 | Acc: 33.325% (12605/37824)\n",
            "Loss: 1.854 | Acc: 33.356% (12638/37888)\n",
            "Loss: 1.854 | Acc: 33.347% (12656/37952)\n",
            "Loss: 1.854 | Acc: 33.365% (12684/38016)\n",
            "Loss: 1.853 | Acc: 33.390% (12715/38080)\n",
            "Loss: 1.853 | Acc: 33.392% (12737/38144)\n",
            "Loss: 1.852 | Acc: 33.422% (12770/38208)\n",
            "Loss: 1.852 | Acc: 33.434% (12796/38272)\n",
            "Loss: 1.851 | Acc: 33.467% (12830/38336)\n",
            "Loss: 1.851 | Acc: 33.490% (12860/38400)\n",
            "Loss: 1.850 | Acc: 33.501% (12886/38464)\n",
            "Loss: 1.850 | Acc: 33.503% (12908/38528)\n",
            "Loss: 1.849 | Acc: 33.515% (12934/38592)\n",
            "Loss: 1.849 | Acc: 33.519% (12957/38656)\n",
            "Loss: 1.849 | Acc: 33.510% (12975/38720)\n",
            "Loss: 1.849 | Acc: 33.540% (13008/38784)\n",
            "Loss: 1.848 | Acc: 33.574% (13043/38848)\n",
            "Loss: 1.847 | Acc: 33.617% (13081/38912)\n",
            "Loss: 1.846 | Acc: 33.626% (13106/38976)\n",
            "Loss: 1.847 | Acc: 33.622% (13126/39040)\n",
            "Loss: 1.846 | Acc: 33.641% (13155/39104)\n",
            "Loss: 1.845 | Acc: 33.660% (13184/39168)\n",
            "Loss: 1.845 | Acc: 33.697% (13220/39232)\n",
            "Loss: 1.845 | Acc: 33.688% (13238/39296)\n",
            "Loss: 1.844 | Acc: 33.707% (13267/39360)\n",
            "Loss: 1.844 | Acc: 33.700% (13286/39424)\n",
            "Loss: 1.844 | Acc: 33.704% (13309/39488)\n",
            "Loss: 1.844 | Acc: 33.710% (13333/39552)\n",
            "Loss: 1.844 | Acc: 33.709% (13354/39616)\n",
            "Loss: 1.843 | Acc: 33.730% (13384/39680)\n",
            "Loss: 1.842 | Acc: 33.746% (13412/39744)\n",
            "Loss: 1.842 | Acc: 33.757% (13438/39808)\n",
            "Loss: 1.841 | Acc: 33.773% (13466/39872)\n",
            "Loss: 1.841 | Acc: 33.779% (13490/39936)\n",
            "Loss: 1.841 | Acc: 33.788% (13515/40000)\n",
            "Loss: 1.841 | Acc: 33.796% (13540/40064)\n",
            "Loss: 1.840 | Acc: 33.809% (13567/40128)\n",
            "Loss: 1.840 | Acc: 33.815% (13591/40192)\n",
            "Loss: 1.839 | Acc: 33.831% (13619/40256)\n",
            "Loss: 1.839 | Acc: 33.852% (13649/40320)\n",
            "Loss: 1.838 | Acc: 33.865% (13676/40384)\n",
            "Loss: 1.838 | Acc: 33.861% (13696/40448)\n",
            "Loss: 1.838 | Acc: 33.874% (13723/40512)\n",
            "Loss: 1.838 | Acc: 33.870% (13743/40576)\n",
            "Loss: 1.838 | Acc: 33.875% (13767/40640)\n",
            "Loss: 1.837 | Acc: 33.898% (13798/40704)\n",
            "Loss: 1.837 | Acc: 33.919% (13828/40768)\n",
            "Loss: 1.836 | Acc: 33.929% (13854/40832)\n",
            "Loss: 1.836 | Acc: 33.942% (13881/40896)\n",
            "Loss: 1.836 | Acc: 33.950% (13906/40960)\n",
            "Loss: 1.835 | Acc: 33.965% (13934/41024)\n",
            "Loss: 1.835 | Acc: 33.973% (13959/41088)\n",
            "Loss: 1.834 | Acc: 33.984% (13985/41152)\n",
            "Loss: 1.834 | Acc: 34.013% (14019/41216)\n",
            "Loss: 1.833 | Acc: 34.024% (14045/41280)\n",
            "Loss: 1.833 | Acc: 34.046% (14076/41344)\n",
            "Loss: 1.832 | Acc: 34.066% (14106/41408)\n",
            "Loss: 1.832 | Acc: 34.071% (14130/41472)\n",
            "Loss: 1.832 | Acc: 34.067% (14150/41536)\n",
            "Loss: 1.831 | Acc: 34.089% (14181/41600)\n",
            "Loss: 1.830 | Acc: 34.125% (14218/41664)\n",
            "Loss: 1.830 | Acc: 34.143% (14247/41728)\n",
            "Loss: 1.829 | Acc: 34.162% (14277/41792)\n",
            "Loss: 1.829 | Acc: 34.174% (14304/41856)\n",
            "Loss: 1.829 | Acc: 34.182% (14329/41920)\n",
            "Loss: 1.828 | Acc: 34.196% (14357/41984)\n",
            "Loss: 1.828 | Acc: 34.213% (14386/42048)\n",
            "Loss: 1.828 | Acc: 34.225% (14413/42112)\n",
            "Loss: 1.827 | Acc: 34.256% (14448/42176)\n",
            "Loss: 1.827 | Acc: 34.264% (14473/42240)\n",
            "Loss: 1.826 | Acc: 34.276% (14500/42304)\n",
            "Loss: 1.826 | Acc: 34.278% (14523/42368)\n",
            "Loss: 1.825 | Acc: 34.295% (14552/42432)\n",
            "Loss: 1.825 | Acc: 34.319% (14584/42496)\n",
            "Loss: 1.825 | Acc: 34.328% (14610/42560)\n",
            "Loss: 1.825 | Acc: 34.335% (14635/42624)\n",
            "Loss: 1.824 | Acc: 34.359% (14667/42688)\n",
            "Loss: 1.824 | Acc: 34.375% (14696/42752)\n",
            "Loss: 1.823 | Acc: 34.387% (14723/42816)\n",
            "Loss: 1.823 | Acc: 34.382% (14743/42880)\n",
            "Loss: 1.823 | Acc: 34.387% (14767/42944)\n",
            "Loss: 1.823 | Acc: 34.403% (14796/43008)\n",
            "Loss: 1.823 | Acc: 34.414% (14823/43072)\n",
            "Loss: 1.822 | Acc: 34.442% (14857/43136)\n",
            "Loss: 1.821 | Acc: 34.463% (14888/43200)\n",
            "Loss: 1.821 | Acc: 34.467% (14912/43264)\n",
            "Loss: 1.821 | Acc: 34.465% (14933/43328)\n",
            "Loss: 1.821 | Acc: 34.465% (14955/43392)\n",
            "Loss: 1.821 | Acc: 34.476% (14982/43456)\n",
            "Loss: 1.820 | Acc: 34.478% (15005/43520)\n",
            "Loss: 1.820 | Acc: 34.490% (15032/43584)\n",
            "Loss: 1.819 | Acc: 34.503% (15060/43648)\n",
            "Loss: 1.819 | Acc: 34.503% (15082/43712)\n",
            "Loss: 1.819 | Acc: 34.507% (15106/43776)\n",
            "Loss: 1.819 | Acc: 34.512% (15130/43840)\n",
            "Loss: 1.819 | Acc: 34.514% (15153/43904)\n",
            "Loss: 1.818 | Acc: 34.525% (15180/43968)\n",
            "Loss: 1.818 | Acc: 34.539% (15208/44032)\n",
            "Loss: 1.817 | Acc: 34.556% (15238/44096)\n",
            "Loss: 1.817 | Acc: 34.561% (15262/44160)\n",
            "Loss: 1.817 | Acc: 34.560% (15284/44224)\n",
            "Loss: 1.817 | Acc: 34.562% (15307/44288)\n",
            "Loss: 1.816 | Acc: 34.569% (15332/44352)\n",
            "Loss: 1.816 | Acc: 34.580% (15359/44416)\n",
            "Loss: 1.815 | Acc: 34.593% (15387/44480)\n",
            "Loss: 1.815 | Acc: 34.611% (15417/44544)\n",
            "Loss: 1.814 | Acc: 34.617% (15442/44608)\n",
            "Loss: 1.814 | Acc: 34.612% (15462/44672)\n",
            "Loss: 1.814 | Acc: 34.619% (15487/44736)\n",
            "Loss: 1.814 | Acc: 34.634% (15516/44800)\n",
            "Loss: 1.813 | Acc: 34.651% (15546/44864)\n",
            "Loss: 1.813 | Acc: 34.662% (15573/44928)\n",
            "Loss: 1.813 | Acc: 34.682% (15604/44992)\n",
            "Loss: 1.812 | Acc: 34.703% (15636/45056)\n",
            "Loss: 1.811 | Acc: 34.723% (15667/45120)\n",
            "Loss: 1.811 | Acc: 34.745% (15699/45184)\n",
            "Loss: 1.811 | Acc: 34.740% (15719/45248)\n",
            "Loss: 1.811 | Acc: 34.750% (15746/45312)\n",
            "Loss: 1.810 | Acc: 34.763% (15774/45376)\n",
            "Loss: 1.810 | Acc: 34.767% (15798/45440)\n",
            "Loss: 1.810 | Acc: 34.779% (15826/45504)\n",
            "Loss: 1.809 | Acc: 34.785% (15851/45568)\n",
            "Loss: 1.809 | Acc: 34.796% (15878/45632)\n",
            "Loss: 1.809 | Acc: 34.797% (15901/45696)\n",
            "Loss: 1.809 | Acc: 34.808% (15928/45760)\n",
            "Loss: 1.808 | Acc: 34.829% (15960/45824)\n",
            "Loss: 1.808 | Acc: 34.833% (15984/45888)\n",
            "Loss: 1.808 | Acc: 34.843% (16011/45952)\n",
            "Loss: 1.807 | Acc: 34.860% (16041/46016)\n",
            "Loss: 1.807 | Acc: 34.872% (16069/46080)\n",
            "Loss: 1.806 | Acc: 34.873% (16092/46144)\n",
            "Loss: 1.806 | Acc: 34.877% (16116/46208)\n",
            "Loss: 1.806 | Acc: 34.887% (16143/46272)\n",
            "Loss: 1.806 | Acc: 34.886% (16165/46336)\n",
            "Loss: 1.806 | Acc: 34.884% (16186/46400)\n",
            "Loss: 1.805 | Acc: 34.900% (16216/46464)\n",
            "Loss: 1.805 | Acc: 34.912% (16244/46528)\n",
            "Loss: 1.804 | Acc: 34.924% (16272/46592)\n",
            "Loss: 1.804 | Acc: 34.943% (16303/46656)\n",
            "Loss: 1.804 | Acc: 34.944% (16326/46720)\n",
            "Loss: 1.804 | Acc: 34.950% (16351/46784)\n",
            "Loss: 1.803 | Acc: 34.960% (16378/46848)\n",
            "Loss: 1.803 | Acc: 34.976% (16408/46912)\n",
            "Loss: 1.802 | Acc: 34.990% (16437/46976)\n",
            "Loss: 1.802 | Acc: 35.011% (16469/47040)\n",
            "Loss: 1.802 | Acc: 35.014% (16493/47104)\n",
            "Loss: 1.801 | Acc: 35.028% (16522/47168)\n",
            "Loss: 1.801 | Acc: 35.046% (16553/47232)\n",
            "Loss: 1.800 | Acc: 35.060% (16582/47296)\n",
            "Loss: 1.800 | Acc: 35.068% (16608/47360)\n",
            "Loss: 1.800 | Acc: 35.065% (16629/47424)\n",
            "Loss: 1.800 | Acc: 35.074% (16656/47488)\n",
            "Loss: 1.800 | Acc: 35.077% (16680/47552)\n",
            "Loss: 1.799 | Acc: 35.100% (16713/47616)\n",
            "Loss: 1.799 | Acc: 35.120% (16745/47680)\n",
            "Loss: 1.799 | Acc: 35.144% (16779/47744)\n",
            "Loss: 1.799 | Acc: 35.147% (16803/47808)\n",
            "Loss: 1.798 | Acc: 35.152% (16828/47872)\n",
            "Loss: 1.798 | Acc: 35.172% (16860/47936)\n",
            "Loss: 1.797 | Acc: 35.185% (16889/48000)\n",
            "Loss: 1.797 | Acc: 35.188% (16913/48064)\n",
            "Loss: 1.797 | Acc: 35.194% (16938/48128)\n",
            "Loss: 1.797 | Acc: 35.211% (16969/48192)\n",
            "Loss: 1.796 | Acc: 35.220% (16996/48256)\n",
            "Loss: 1.796 | Acc: 35.224% (17020/48320)\n",
            "Loss: 1.795 | Acc: 35.239% (17050/48384)\n",
            "Loss: 1.795 | Acc: 35.244% (17075/48448)\n",
            "Loss: 1.795 | Acc: 35.245% (17098/48512)\n",
            "Loss: 1.795 | Acc: 35.250% (17123/48576)\n",
            "Loss: 1.794 | Acc: 35.273% (17157/48640)\n",
            "Loss: 1.794 | Acc: 35.287% (17186/48704)\n",
            "Loss: 1.793 | Acc: 35.296% (17213/48768)\n",
            "Loss: 1.793 | Acc: 35.301% (17238/48832)\n",
            "Loss: 1.793 | Acc: 35.316% (17268/48896)\n",
            "Loss: 1.792 | Acc: 35.329% (17297/48960)\n",
            "Loss: 1.792 | Acc: 35.335% (17314/49000)\n",
            "Epoch 0 of training is completed, Training accuracy for this epoch is 35.33469387755102\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 1.444 | Acc: 45.312% (29/64)\n",
            "Loss: 1.441 | Acc: 47.656% (61/128)\n",
            "Loss: 1.497 | Acc: 45.312% (87/192)\n",
            "Loss: 1.550 | Acc: 44.531% (114/256)\n",
            "Loss: 1.508 | Acc: 45.938% (147/320)\n",
            "Loss: 1.501 | Acc: 45.833% (176/384)\n",
            "Loss: 1.536 | Acc: 45.759% (205/448)\n",
            "Loss: 1.537 | Acc: 45.312% (232/512)\n",
            "Loss: 1.512 | Acc: 44.792% (258/576)\n",
            "Loss: 1.507 | Acc: 45.469% (291/640)\n",
            "Loss: 1.520 | Acc: 44.318% (312/704)\n",
            "Loss: 1.531 | Acc: 44.141% (339/768)\n",
            "Loss: 1.528 | Acc: 43.870% (365/832)\n",
            "Loss: 1.522 | Acc: 43.862% (393/896)\n",
            "Loss: 1.509 | Acc: 44.583% (428/960)\n",
            "Loss: 1.488 | Acc: 45.410% (465/1024)\n",
            "Loss: 1.488 | Acc: 45.221% (492/1088)\n",
            "Loss: 1.495 | Acc: 45.139% (520/1152)\n",
            "Loss: 1.497 | Acc: 45.477% (553/1216)\n",
            "Loss: 1.503 | Acc: 45.234% (579/1280)\n",
            "Loss: 1.506 | Acc: 45.015% (605/1344)\n",
            "Loss: 1.509 | Acc: 44.602% (628/1408)\n",
            "Loss: 1.501 | Acc: 44.565% (656/1472)\n",
            "Loss: 1.504 | Acc: 44.596% (685/1536)\n",
            "Loss: 1.503 | Acc: 44.750% (716/1600)\n",
            "Loss: 1.508 | Acc: 44.772% (745/1664)\n",
            "Loss: 1.511 | Acc: 44.792% (774/1728)\n",
            "Loss: 1.510 | Acc: 44.810% (803/1792)\n",
            "Loss: 1.511 | Acc: 44.720% (830/1856)\n",
            "Loss: 1.513 | Acc: 44.792% (860/1920)\n",
            "Loss: 1.511 | Acc: 44.808% (889/1984)\n",
            "Loss: 1.516 | Acc: 44.775% (917/2048)\n",
            "Loss: 1.508 | Acc: 44.981% (950/2112)\n",
            "Loss: 1.508 | Acc: 44.899% (977/2176)\n",
            "Loss: 1.507 | Acc: 45.045% (1009/2240)\n",
            "Loss: 1.507 | Acc: 45.139% (1040/2304)\n",
            "Loss: 1.510 | Acc: 45.059% (1067/2368)\n",
            "Loss: 1.510 | Acc: 45.066% (1096/2432)\n",
            "Loss: 1.505 | Acc: 45.433% (1134/2496)\n",
            "Loss: 1.513 | Acc: 45.234% (1158/2560)\n",
            "Loss: 1.514 | Acc: 45.046% (1182/2624)\n",
            "Loss: 1.512 | Acc: 45.312% (1218/2688)\n",
            "Loss: 1.512 | Acc: 45.240% (1245/2752)\n",
            "Loss: 1.514 | Acc: 45.099% (1270/2816)\n",
            "Loss: 1.515 | Acc: 45.104% (1299/2880)\n",
            "Loss: 1.513 | Acc: 45.279% (1333/2944)\n",
            "Loss: 1.510 | Acc: 45.346% (1364/3008)\n",
            "Loss: 1.509 | Acc: 45.410% (1395/3072)\n",
            "Loss: 1.507 | Acc: 45.281% (1420/3136)\n",
            "Loss: 1.505 | Acc: 45.406% (1453/3200)\n",
            "Loss: 1.508 | Acc: 45.343% (1480/3264)\n",
            "Loss: 1.508 | Acc: 45.463% (1513/3328)\n",
            "Loss: 1.506 | Acc: 45.519% (1544/3392)\n",
            "Loss: 1.507 | Acc: 45.341% (1567/3456)\n",
            "Loss: 1.507 | Acc: 45.284% (1594/3520)\n",
            "Loss: 1.501 | Acc: 45.564% (1633/3584)\n",
            "Loss: 1.502 | Acc: 45.504% (1660/3648)\n",
            "Loss: 1.501 | Acc: 45.582% (1692/3712)\n",
            "Loss: 1.503 | Acc: 45.445% (1716/3776)\n",
            "Loss: 1.501 | Acc: 45.573% (1750/3840)\n",
            "Loss: 1.501 | Acc: 45.569% (1779/3904)\n",
            "Loss: 1.498 | Acc: 45.590% (1809/3968)\n",
            "Loss: 1.500 | Acc: 45.635% (1840/4032)\n",
            "Loss: 1.505 | Acc: 45.630% (1869/4096)\n",
            "Loss: 1.507 | Acc: 45.553% (1895/4160)\n",
            "Loss: 1.505 | Acc: 45.691% (1930/4224)\n",
            "Loss: 1.505 | Acc: 45.546% (1953/4288)\n",
            "Loss: 1.505 | Acc: 45.496% (1980/4352)\n",
            "Loss: 1.503 | Acc: 45.516% (2010/4416)\n",
            "Loss: 1.502 | Acc: 45.580% (2042/4480)\n",
            "Loss: 1.501 | Acc: 45.467% (2066/4544)\n",
            "Loss: 1.504 | Acc: 45.291% (2087/4608)\n",
            "Loss: 1.504 | Acc: 45.270% (2115/4672)\n",
            "Loss: 1.502 | Acc: 45.355% (2148/4736)\n",
            "Loss: 1.504 | Acc: 45.438% (2181/4800)\n",
            "Loss: 1.503 | Acc: 45.415% (2209/4864)\n",
            "Loss: 1.502 | Acc: 45.455% (2240/4928)\n",
            "Loss: 1.503 | Acc: 45.373% (2265/4992)\n",
            "Loss: 1.503 | Acc: 45.392% (2295/5056)\n",
            "Loss: 1.506 | Acc: 45.312% (2320/5120)\n",
            "Loss: 1.504 | Acc: 45.467% (2357/5184)\n",
            "Loss: 1.507 | Acc: 45.332% (2379/5248)\n",
            "Loss: 1.505 | Acc: 45.444% (2414/5312)\n",
            "Loss: 1.506 | Acc: 45.461% (2444/5376)\n",
            "Loss: 1.504 | Acc: 45.515% (2476/5440)\n",
            "Loss: 1.505 | Acc: 45.585% (2509/5504)\n",
            "Loss: 1.509 | Acc: 45.546% (2536/5568)\n",
            "Loss: 1.512 | Acc: 45.437% (2559/5632)\n",
            "Loss: 1.512 | Acc: 45.418% (2587/5696)\n",
            "Loss: 1.510 | Acc: 45.417% (2616/5760)\n",
            "Loss: 1.508 | Acc: 45.467% (2648/5824)\n",
            "Loss: 1.509 | Acc: 45.448% (2676/5888)\n",
            "Loss: 1.510 | Acc: 45.481% (2707/5952)\n",
            "Loss: 1.510 | Acc: 45.529% (2739/6016)\n",
            "Loss: 1.511 | Acc: 45.510% (2767/6080)\n",
            "Loss: 1.511 | Acc: 45.540% (2798/6144)\n",
            "Loss: 1.511 | Acc: 45.538% (2827/6208)\n",
            "Loss: 1.512 | Acc: 45.424% (2849/6272)\n",
            "Loss: 1.511 | Acc: 45.486% (2882/6336)\n",
            "Loss: 1.510 | Acc: 45.453% (2909/6400)\n",
            "Loss: 1.512 | Acc: 45.421% (2936/6464)\n",
            "Loss: 1.512 | Acc: 45.420% (2965/6528)\n",
            "Loss: 1.513 | Acc: 45.404% (2993/6592)\n",
            "Loss: 1.513 | Acc: 45.478% (3027/6656)\n",
            "Loss: 1.514 | Acc: 45.476% (3056/6720)\n",
            "Loss: 1.514 | Acc: 45.519% (3088/6784)\n",
            "Loss: 1.512 | Acc: 45.532% (3118/6848)\n",
            "Loss: 1.515 | Acc: 45.428% (3140/6912)\n",
            "Loss: 1.516 | Acc: 45.399% (3167/6976)\n",
            "Loss: 1.517 | Acc: 45.384% (3195/7040)\n",
            "Loss: 1.516 | Acc: 45.481% (3231/7104)\n",
            "Loss: 1.516 | Acc: 45.564% (3266/7168)\n",
            "Loss: 1.518 | Acc: 45.409% (3284/7232)\n",
            "Loss: 1.516 | Acc: 45.408% (3313/7296)\n",
            "Loss: 1.514 | Acc: 45.503% (3349/7360)\n",
            "Loss: 1.514 | Acc: 45.461% (3375/7424)\n",
            "Loss: 1.512 | Acc: 45.540% (3410/7488)\n",
            "Loss: 1.511 | Acc: 45.657% (3448/7552)\n",
            "Loss: 1.511 | Acc: 45.628% (3475/7616)\n",
            "Loss: 1.510 | Acc: 45.651% (3506/7680)\n",
            "Loss: 1.508 | Acc: 45.700% (3539/7744)\n",
            "Loss: 1.507 | Acc: 45.735% (3571/7808)\n",
            "Loss: 1.508 | Acc: 45.706% (3598/7872)\n",
            "Loss: 1.508 | Acc: 45.665% (3624/7936)\n",
            "Loss: 1.507 | Acc: 45.737% (3659/8000)\n",
            "Loss: 1.506 | Acc: 45.734% (3688/8064)\n",
            "Loss: 1.506 | Acc: 45.755% (3719/8128)\n",
            "Loss: 1.506 | Acc: 45.679% (3742/8192)\n",
            "Loss: 1.507 | Acc: 45.603% (3765/8256)\n",
            "Loss: 1.510 | Acc: 45.445% (3781/8320)\n",
            "Loss: 1.511 | Acc: 45.408% (3807/8384)\n",
            "Loss: 1.510 | Acc: 45.360% (3832/8448)\n",
            "Loss: 1.510 | Acc: 45.348% (3860/8512)\n",
            "Loss: 1.510 | Acc: 45.371% (3891/8576)\n",
            "Loss: 1.511 | Acc: 45.417% (3924/8640)\n",
            "Loss: 1.511 | Acc: 45.404% (3952/8704)\n",
            "Loss: 1.511 | Acc: 45.347% (3976/8768)\n",
            "Loss: 1.510 | Acc: 45.358% (4006/8832)\n",
            "Loss: 1.508 | Acc: 45.459% (4044/8896)\n",
            "Loss: 1.508 | Acc: 45.480% (4075/8960)\n",
            "Loss: 1.508 | Acc: 45.490% (4105/9024)\n",
            "Loss: 1.509 | Acc: 45.434% (4129/9088)\n",
            "Loss: 1.508 | Acc: 45.476% (4162/9152)\n",
            "Loss: 1.507 | Acc: 45.530% (4196/9216)\n",
            "Loss: 1.505 | Acc: 45.560% (4228/9280)\n",
            "Loss: 1.507 | Acc: 45.516% (4253/9344)\n",
            "Loss: 1.507 | Acc: 45.504% (4281/9408)\n",
            "Loss: 1.508 | Acc: 45.471% (4307/9472)\n",
            "Loss: 1.508 | Acc: 45.522% (4341/9536)\n",
            "Loss: 1.507 | Acc: 45.542% (4372/9600)\n",
            "Loss: 1.507 | Acc: 45.499% (4397/9664)\n",
            "Loss: 1.507 | Acc: 45.487% (4425/9728)\n",
            "Loss: 1.508 | Acc: 45.496% (4455/9792)\n",
            "Loss: 1.507 | Acc: 45.475% (4482/9856)\n",
            "Loss: 1.507 | Acc: 45.444% (4508/9920)\n",
            "Loss: 1.508 | Acc: 45.473% (4540/9984)\n",
            "Loss: 1.507 | Acc: 45.490% (4549/10000)\n",
            "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 45.49\n",
            "\n",
            "Epoch: 1\n",
            "Loss: 1.325 | Acc: 62.500% (40/64)\n",
            "Loss: 1.399 | Acc: 56.250% (72/128)\n",
            "Loss: 1.463 | Acc: 53.125% (102/192)\n",
            "Loss: 1.494 | Acc: 50.391% (129/256)\n",
            "Loss: 1.518 | Acc: 48.750% (156/320)\n",
            "Loss: 1.515 | Acc: 48.438% (186/384)\n",
            "Loss: 1.486 | Acc: 48.884% (219/448)\n",
            "Loss: 1.482 | Acc: 47.852% (245/512)\n",
            "Loss: 1.486 | Acc: 47.222% (272/576)\n",
            "Loss: 1.478 | Acc: 46.406% (297/640)\n",
            "Loss: 1.478 | Acc: 46.733% (329/704)\n",
            "Loss: 1.479 | Acc: 46.745% (359/768)\n",
            "Loss: 1.466 | Acc: 47.115% (392/832)\n",
            "Loss: 1.479 | Acc: 46.094% (413/896)\n",
            "Loss: 1.459 | Acc: 46.667% (448/960)\n",
            "Loss: 1.483 | Acc: 46.191% (473/1024)\n",
            "Loss: 1.487 | Acc: 46.140% (502/1088)\n",
            "Loss: 1.474 | Acc: 46.615% (537/1152)\n",
            "Loss: 1.469 | Acc: 46.464% (565/1216)\n",
            "Loss: 1.461 | Acc: 46.875% (600/1280)\n",
            "Loss: 1.466 | Acc: 46.726% (628/1344)\n",
            "Loss: 1.466 | Acc: 46.449% (654/1408)\n",
            "Loss: 1.469 | Acc: 46.128% (679/1472)\n",
            "Loss: 1.467 | Acc: 46.029% (707/1536)\n",
            "Loss: 1.478 | Acc: 46.000% (736/1600)\n",
            "Loss: 1.479 | Acc: 46.034% (766/1664)\n",
            "Loss: 1.489 | Acc: 45.833% (792/1728)\n",
            "Loss: 1.489 | Acc: 45.703% (819/1792)\n",
            "Loss: 1.490 | Acc: 45.744% (849/1856)\n",
            "Loss: 1.493 | Acc: 45.573% (875/1920)\n",
            "Loss: 1.486 | Acc: 45.867% (910/1984)\n",
            "Loss: 1.493 | Acc: 45.361% (929/2048)\n",
            "Loss: 1.495 | Acc: 45.265% (956/2112)\n",
            "Loss: 1.495 | Acc: 45.175% (983/2176)\n",
            "Loss: 1.491 | Acc: 45.312% (1015/2240)\n",
            "Loss: 1.495 | Acc: 45.009% (1037/2304)\n",
            "Loss: 1.495 | Acc: 45.059% (1067/2368)\n",
            "Loss: 1.499 | Acc: 44.984% (1094/2432)\n",
            "Loss: 1.499 | Acc: 44.992% (1123/2496)\n",
            "Loss: 1.495 | Acc: 45.078% (1154/2560)\n",
            "Loss: 1.494 | Acc: 44.893% (1178/2624)\n",
            "Loss: 1.495 | Acc: 44.792% (1204/2688)\n",
            "Loss: 1.499 | Acc: 44.913% (1236/2752)\n",
            "Loss: 1.499 | Acc: 44.993% (1267/2816)\n",
            "Loss: 1.499 | Acc: 45.069% (1298/2880)\n",
            "Loss: 1.498 | Acc: 44.939% (1323/2944)\n",
            "Loss: 1.498 | Acc: 44.880% (1350/3008)\n",
            "Loss: 1.497 | Acc: 44.889% (1379/3072)\n",
            "Loss: 1.494 | Acc: 44.898% (1408/3136)\n",
            "Loss: 1.492 | Acc: 44.844% (1435/3200)\n",
            "Loss: 1.489 | Acc: 45.006% (1469/3264)\n",
            "Loss: 1.491 | Acc: 44.952% (1496/3328)\n",
            "Loss: 1.493 | Acc: 44.959% (1525/3392)\n",
            "Loss: 1.490 | Acc: 45.052% (1557/3456)\n",
            "Loss: 1.491 | Acc: 45.142% (1589/3520)\n",
            "Loss: 1.489 | Acc: 45.173% (1619/3584)\n",
            "Loss: 1.487 | Acc: 45.230% (1650/3648)\n",
            "Loss: 1.490 | Acc: 45.178% (1677/3712)\n",
            "Loss: 1.487 | Acc: 45.101% (1703/3776)\n",
            "Loss: 1.491 | Acc: 44.896% (1724/3840)\n",
            "Loss: 1.494 | Acc: 44.723% (1746/3904)\n",
            "Loss: 1.493 | Acc: 44.733% (1775/3968)\n",
            "Loss: 1.495 | Acc: 44.742% (1804/4032)\n",
            "Loss: 1.493 | Acc: 44.824% (1836/4096)\n",
            "Loss: 1.493 | Acc: 44.784% (1863/4160)\n",
            "Loss: 1.489 | Acc: 44.910% (1897/4224)\n",
            "Loss: 1.490 | Acc: 44.799% (1921/4288)\n",
            "Loss: 1.490 | Acc: 44.738% (1947/4352)\n",
            "Loss: 1.492 | Acc: 44.656% (1972/4416)\n",
            "Loss: 1.490 | Acc: 44.844% (2009/4480)\n",
            "Loss: 1.491 | Acc: 44.718% (2032/4544)\n",
            "Loss: 1.489 | Acc: 44.835% (2066/4608)\n",
            "Loss: 1.487 | Acc: 44.970% (2101/4672)\n",
            "Loss: 1.487 | Acc: 44.975% (2130/4736)\n",
            "Loss: 1.486 | Acc: 45.000% (2160/4800)\n",
            "Loss: 1.487 | Acc: 44.963% (2187/4864)\n",
            "Loss: 1.484 | Acc: 45.049% (2220/4928)\n",
            "Loss: 1.488 | Acc: 45.072% (2250/4992)\n",
            "Loss: 1.486 | Acc: 45.095% (2280/5056)\n",
            "Loss: 1.486 | Acc: 45.020% (2305/5120)\n",
            "Loss: 1.484 | Acc: 44.985% (2332/5184)\n",
            "Loss: 1.488 | Acc: 44.855% (2354/5248)\n",
            "Loss: 1.487 | Acc: 44.955% (2388/5312)\n",
            "Loss: 1.485 | Acc: 45.089% (2424/5376)\n",
            "Loss: 1.484 | Acc: 45.184% (2458/5440)\n",
            "Loss: 1.482 | Acc: 45.167% (2486/5504)\n",
            "Loss: 1.480 | Acc: 45.241% (2519/5568)\n",
            "Loss: 1.479 | Acc: 45.312% (2552/5632)\n",
            "Loss: 1.478 | Acc: 45.383% (2585/5696)\n",
            "Loss: 1.476 | Acc: 45.451% (2618/5760)\n",
            "Loss: 1.475 | Acc: 45.450% (2647/5824)\n",
            "Loss: 1.478 | Acc: 45.363% (2671/5888)\n",
            "Loss: 1.475 | Acc: 45.430% (2704/5952)\n",
            "Loss: 1.474 | Acc: 45.429% (2733/6016)\n",
            "Loss: 1.474 | Acc: 45.444% (2763/6080)\n",
            "Loss: 1.472 | Acc: 45.557% (2799/6144)\n",
            "Loss: 1.472 | Acc: 45.506% (2825/6208)\n",
            "Loss: 1.471 | Acc: 45.536% (2856/6272)\n",
            "Loss: 1.470 | Acc: 45.533% (2885/6336)\n",
            "Loss: 1.471 | Acc: 45.453% (2909/6400)\n",
            "Loss: 1.473 | Acc: 45.328% (2930/6464)\n",
            "Loss: 1.472 | Acc: 45.343% (2960/6528)\n",
            "Loss: 1.472 | Acc: 45.434% (2995/6592)\n",
            "Loss: 1.471 | Acc: 45.448% (3025/6656)\n",
            "Loss: 1.472 | Acc: 45.402% (3051/6720)\n",
            "Loss: 1.472 | Acc: 45.445% (3083/6784)\n",
            "Loss: 1.471 | Acc: 45.400% (3109/6848)\n",
            "Loss: 1.470 | Acc: 45.428% (3140/6912)\n",
            "Loss: 1.470 | Acc: 45.370% (3165/6976)\n",
            "Loss: 1.469 | Acc: 45.369% (3194/7040)\n",
            "Loss: 1.468 | Acc: 45.439% (3228/7104)\n",
            "Loss: 1.468 | Acc: 45.452% (3258/7168)\n",
            "Loss: 1.466 | Acc: 45.492% (3290/7232)\n",
            "Loss: 1.468 | Acc: 45.395% (3312/7296)\n",
            "Loss: 1.466 | Acc: 45.408% (3342/7360)\n",
            "Loss: 1.466 | Acc: 45.501% (3378/7424)\n",
            "Loss: 1.466 | Acc: 45.473% (3405/7488)\n",
            "Loss: 1.465 | Acc: 45.511% (3437/7552)\n",
            "Loss: 1.465 | Acc: 45.562% (3470/7616)\n",
            "Loss: 1.465 | Acc: 45.534% (3497/7680)\n",
            "Loss: 1.463 | Acc: 45.648% (3535/7744)\n",
            "Loss: 1.463 | Acc: 45.697% (3568/7808)\n",
            "Loss: 1.462 | Acc: 45.694% (3597/7872)\n",
            "Loss: 1.461 | Acc: 45.817% (3636/7936)\n",
            "Loss: 1.460 | Acc: 45.850% (3668/8000)\n",
            "Loss: 1.461 | Acc: 45.809% (3694/8064)\n",
            "Loss: 1.464 | Acc: 45.768% (3720/8128)\n",
            "Loss: 1.463 | Acc: 45.813% (3753/8192)\n",
            "Loss: 1.462 | Acc: 45.858% (3786/8256)\n",
            "Loss: 1.462 | Acc: 45.865% (3816/8320)\n",
            "Loss: 1.462 | Acc: 45.825% (3842/8384)\n",
            "Loss: 1.462 | Acc: 45.821% (3871/8448)\n",
            "Loss: 1.463 | Acc: 45.794% (3898/8512)\n",
            "Loss: 1.463 | Acc: 45.861% (3933/8576)\n",
            "Loss: 1.462 | Acc: 45.903% (3966/8640)\n",
            "Loss: 1.463 | Acc: 45.875% (3993/8704)\n",
            "Loss: 1.464 | Acc: 45.860% (4021/8768)\n",
            "Loss: 1.462 | Acc: 45.958% (4059/8832)\n",
            "Loss: 1.462 | Acc: 45.942% (4087/8896)\n",
            "Loss: 1.462 | Acc: 45.904% (4113/8960)\n",
            "Loss: 1.463 | Acc: 45.922% (4144/9024)\n",
            "Loss: 1.463 | Acc: 45.929% (4174/9088)\n",
            "Loss: 1.464 | Acc: 45.892% (4200/9152)\n",
            "Loss: 1.465 | Acc: 45.866% (4227/9216)\n",
            "Loss: 1.465 | Acc: 45.938% (4263/9280)\n",
            "Loss: 1.464 | Acc: 45.955% (4294/9344)\n",
            "Loss: 1.464 | Acc: 45.961% (4324/9408)\n",
            "Loss: 1.463 | Acc: 45.999% (4357/9472)\n",
            "Loss: 1.464 | Acc: 45.963% (4383/9536)\n",
            "Loss: 1.464 | Acc: 45.958% (4412/9600)\n",
            "Loss: 1.464 | Acc: 45.954% (4441/9664)\n",
            "Loss: 1.464 | Acc: 45.950% (4470/9728)\n",
            "Loss: 1.463 | Acc: 45.976% (4502/9792)\n",
            "Loss: 1.463 | Acc: 45.962% (4530/9856)\n",
            "Loss: 1.462 | Acc: 45.978% (4561/9920)\n",
            "Loss: 1.463 | Acc: 45.964% (4589/9984)\n",
            "Loss: 1.462 | Acc: 45.939% (4616/10048)\n",
            "Loss: 1.463 | Acc: 45.906% (4642/10112)\n",
            "Loss: 1.463 | Acc: 45.941% (4675/10176)\n",
            "Loss: 1.463 | Acc: 45.947% (4705/10240)\n",
            "Loss: 1.463 | Acc: 45.982% (4738/10304)\n",
            "Loss: 1.462 | Acc: 46.007% (4770/10368)\n",
            "Loss: 1.462 | Acc: 46.022% (4801/10432)\n",
            "Loss: 1.461 | Acc: 46.046% (4833/10496)\n",
            "Loss: 1.462 | Acc: 46.023% (4860/10560)\n",
            "Loss: 1.461 | Acc: 46.028% (4890/10624)\n",
            "Loss: 1.461 | Acc: 46.042% (4921/10688)\n",
            "Loss: 1.460 | Acc: 46.075% (4954/10752)\n",
            "Loss: 1.460 | Acc: 46.052% (4981/10816)\n",
            "Loss: 1.459 | Acc: 46.066% (5012/10880)\n",
            "Loss: 1.461 | Acc: 46.071% (5042/10944)\n",
            "Loss: 1.462 | Acc: 46.076% (5072/11008)\n",
            "Loss: 1.461 | Acc: 46.107% (5105/11072)\n",
            "Loss: 1.460 | Acc: 46.139% (5138/11136)\n",
            "Loss: 1.459 | Acc: 46.152% (5169/11200)\n",
            "Loss: 1.459 | Acc: 46.156% (5199/11264)\n",
            "Loss: 1.460 | Acc: 46.151% (5228/11328)\n",
            "Loss: 1.461 | Acc: 46.146% (5257/11392)\n",
            "Loss: 1.462 | Acc: 46.133% (5285/11456)\n",
            "Loss: 1.461 | Acc: 46.189% (5321/11520)\n",
            "Loss: 1.462 | Acc: 46.202% (5352/11584)\n",
            "Loss: 1.462 | Acc: 46.248% (5387/11648)\n",
            "Loss: 1.462 | Acc: 46.235% (5415/11712)\n",
            "Loss: 1.463 | Acc: 46.187% (5439/11776)\n",
            "Loss: 1.463 | Acc: 46.225% (5473/11840)\n",
            "Loss: 1.464 | Acc: 46.237% (5504/11904)\n",
            "Loss: 1.465 | Acc: 46.198% (5529/11968)\n",
            "Loss: 1.467 | Acc: 46.135% (5551/12032)\n",
            "Loss: 1.466 | Acc: 46.205% (5589/12096)\n",
            "Loss: 1.467 | Acc: 46.176% (5615/12160)\n",
            "Loss: 1.467 | Acc: 46.188% (5646/12224)\n",
            "Loss: 1.468 | Acc: 46.191% (5676/12288)\n",
            "Loss: 1.467 | Acc: 46.171% (5703/12352)\n",
            "Loss: 1.469 | Acc: 46.150% (5730/12416)\n",
            "Loss: 1.470 | Acc: 46.114% (5755/12480)\n",
            "Loss: 1.469 | Acc: 46.158% (5790/12544)\n",
            "Loss: 1.470 | Acc: 46.137% (5817/12608)\n",
            "Loss: 1.469 | Acc: 46.165% (5850/12672)\n",
            "Loss: 1.470 | Acc: 46.137% (5876/12736)\n",
            "Loss: 1.471 | Acc: 46.102% (5901/12800)\n",
            "Loss: 1.472 | Acc: 46.113% (5932/12864)\n",
            "Loss: 1.471 | Acc: 46.140% (5965/12928)\n",
            "Loss: 1.470 | Acc: 46.175% (5999/12992)\n",
            "Loss: 1.471 | Acc: 46.186% (6030/13056)\n",
            "Loss: 1.471 | Acc: 46.204% (6062/13120)\n",
            "Loss: 1.471 | Acc: 46.215% (6093/13184)\n",
            "Loss: 1.470 | Acc: 46.218% (6123/13248)\n",
            "Loss: 1.471 | Acc: 46.214% (6152/13312)\n",
            "Loss: 1.470 | Acc: 46.262% (6188/13376)\n",
            "Loss: 1.470 | Acc: 46.257% (6217/13440)\n",
            "Loss: 1.470 | Acc: 46.238% (6244/13504)\n",
            "Loss: 1.470 | Acc: 46.241% (6274/13568)\n",
            "Loss: 1.471 | Acc: 46.215% (6300/13632)\n",
            "Loss: 1.471 | Acc: 46.203% (6328/13696)\n",
            "Loss: 1.473 | Acc: 46.163% (6352/13760)\n",
            "Loss: 1.472 | Acc: 46.202% (6387/13824)\n",
            "Loss: 1.472 | Acc: 46.227% (6420/13888)\n",
            "Loss: 1.472 | Acc: 46.216% (6448/13952)\n",
            "Loss: 1.472 | Acc: 46.233% (6480/14016)\n",
            "Loss: 1.472 | Acc: 46.243% (6511/14080)\n",
            "Loss: 1.473 | Acc: 46.239% (6540/14144)\n",
            "Loss: 1.474 | Acc: 46.192% (6563/14208)\n",
            "Loss: 1.473 | Acc: 46.195% (6593/14272)\n",
            "Loss: 1.475 | Acc: 46.129% (6613/14336)\n",
            "Loss: 1.475 | Acc: 46.146% (6645/14400)\n",
            "Loss: 1.475 | Acc: 46.135% (6673/14464)\n",
            "Loss: 1.476 | Acc: 46.125% (6701/14528)\n",
            "Loss: 1.476 | Acc: 46.121% (6730/14592)\n",
            "Loss: 1.476 | Acc: 46.145% (6763/14656)\n",
            "Loss: 1.475 | Acc: 46.189% (6799/14720)\n",
            "Loss: 1.475 | Acc: 46.185% (6828/14784)\n",
            "Loss: 1.475 | Acc: 46.168% (6855/14848)\n",
            "Loss: 1.476 | Acc: 46.144% (6881/14912)\n",
            "Loss: 1.476 | Acc: 46.207% (6920/14976)\n",
            "Loss: 1.475 | Acc: 46.243% (6955/15040)\n",
            "Loss: 1.475 | Acc: 46.246% (6985/15104)\n",
            "Loss: 1.475 | Acc: 46.235% (7013/15168)\n",
            "Loss: 1.475 | Acc: 46.232% (7042/15232)\n",
            "Loss: 1.475 | Acc: 46.215% (7069/15296)\n",
            "Loss: 1.475 | Acc: 46.224% (7100/15360)\n",
            "Loss: 1.475 | Acc: 46.201% (7126/15424)\n",
            "Loss: 1.475 | Acc: 46.197% (7155/15488)\n",
            "Loss: 1.474 | Acc: 46.168% (7180/15552)\n",
            "Loss: 1.474 | Acc: 46.190% (7213/15616)\n",
            "Loss: 1.473 | Acc: 46.231% (7249/15680)\n",
            "Loss: 1.473 | Acc: 46.233% (7279/15744)\n",
            "Loss: 1.473 | Acc: 46.230% (7308/15808)\n",
            "Loss: 1.473 | Acc: 46.245% (7340/15872)\n",
            "Loss: 1.473 | Acc: 46.241% (7369/15936)\n",
            "Loss: 1.472 | Acc: 46.306% (7409/16000)\n",
            "Loss: 1.473 | Acc: 46.265% (7432/16064)\n",
            "Loss: 1.472 | Acc: 46.236% (7457/16128)\n",
            "Loss: 1.472 | Acc: 46.270% (7492/16192)\n",
            "Loss: 1.471 | Acc: 46.297% (7526/16256)\n",
            "Loss: 1.471 | Acc: 46.293% (7555/16320)\n",
            "Loss: 1.470 | Acc: 46.350% (7594/16384)\n",
            "Loss: 1.470 | Acc: 46.395% (7631/16448)\n",
            "Loss: 1.469 | Acc: 46.409% (7663/16512)\n",
            "Loss: 1.469 | Acc: 46.398% (7691/16576)\n",
            "Loss: 1.469 | Acc: 46.400% (7721/16640)\n",
            "Loss: 1.469 | Acc: 46.390% (7749/16704)\n",
            "Loss: 1.469 | Acc: 46.398% (7780/16768)\n",
            "Loss: 1.469 | Acc: 46.406% (7811/16832)\n",
            "Loss: 1.470 | Acc: 46.396% (7839/16896)\n",
            "Loss: 1.469 | Acc: 46.403% (7870/16960)\n",
            "Loss: 1.470 | Acc: 46.399% (7899/17024)\n",
            "Loss: 1.470 | Acc: 46.383% (7926/17088)\n",
            "Loss: 1.470 | Acc: 46.391% (7957/17152)\n",
            "Loss: 1.470 | Acc: 46.358% (7981/17216)\n",
            "Loss: 1.470 | Acc: 46.331% (8006/17280)\n",
            "Loss: 1.471 | Acc: 46.310% (8032/17344)\n",
            "Loss: 1.471 | Acc: 46.341% (8067/17408)\n",
            "Loss: 1.470 | Acc: 46.331% (8095/17472)\n",
            "Loss: 1.470 | Acc: 46.322% (8123/17536)\n",
            "Loss: 1.470 | Acc: 46.364% (8160/17600)\n",
            "Loss: 1.469 | Acc: 46.377% (8192/17664)\n",
            "Loss: 1.469 | Acc: 46.390% (8224/17728)\n",
            "Loss: 1.469 | Acc: 46.403% (8256/17792)\n",
            "Loss: 1.469 | Acc: 46.410% (8287/17856)\n",
            "Loss: 1.469 | Acc: 46.412% (8317/17920)\n",
            "Loss: 1.468 | Acc: 46.425% (8349/17984)\n",
            "Loss: 1.468 | Acc: 46.432% (8380/18048)\n",
            "Loss: 1.468 | Acc: 46.422% (8408/18112)\n",
            "Loss: 1.467 | Acc: 46.457% (8444/18176)\n",
            "Loss: 1.467 | Acc: 46.480% (8478/18240)\n",
            "Loss: 1.467 | Acc: 46.498% (8511/18304)\n",
            "Loss: 1.467 | Acc: 46.467% (8535/18368)\n",
            "Loss: 1.468 | Acc: 46.441% (8560/18432)\n",
            "Loss: 1.469 | Acc: 46.399% (8582/18496)\n",
            "Loss: 1.468 | Acc: 46.412% (8614/18560)\n",
            "Loss: 1.468 | Acc: 46.408% (8643/18624)\n",
            "Loss: 1.468 | Acc: 46.393% (8670/18688)\n",
            "Loss: 1.469 | Acc: 46.374% (8696/18752)\n",
            "Loss: 1.469 | Acc: 46.365% (8724/18816)\n",
            "Loss: 1.469 | Acc: 46.340% (8749/18880)\n",
            "Loss: 1.469 | Acc: 46.342% (8779/18944)\n",
            "Loss: 1.469 | Acc: 46.344% (8809/19008)\n",
            "Loss: 1.469 | Acc: 46.335% (8837/19072)\n",
            "Loss: 1.469 | Acc: 46.300% (8860/19136)\n",
            "Loss: 1.470 | Acc: 46.260% (8882/19200)\n",
            "Loss: 1.470 | Acc: 46.278% (8915/19264)\n",
            "Loss: 1.469 | Acc: 46.332% (8955/19328)\n",
            "Loss: 1.468 | Acc: 46.364% (8991/19392)\n",
            "Loss: 1.468 | Acc: 46.382% (9024/19456)\n",
            "Loss: 1.468 | Acc: 46.388% (9055/19520)\n",
            "Loss: 1.467 | Acc: 46.421% (9091/19584)\n",
            "Loss: 1.467 | Acc: 46.407% (9118/19648)\n",
            "Loss: 1.467 | Acc: 46.444% (9155/19712)\n",
            "Loss: 1.466 | Acc: 46.476% (9191/19776)\n",
            "Loss: 1.466 | Acc: 46.482% (9222/19840)\n",
            "Loss: 1.466 | Acc: 46.508% (9257/19904)\n",
            "Loss: 1.466 | Acc: 46.499% (9285/19968)\n",
            "Loss: 1.466 | Acc: 46.471% (9309/20032)\n",
            "Loss: 1.466 | Acc: 46.507% (9346/20096)\n",
            "Loss: 1.467 | Acc: 46.493% (9373/20160)\n",
            "Loss: 1.467 | Acc: 46.504% (9405/20224)\n",
            "Loss: 1.467 | Acc: 46.486% (9431/20288)\n",
            "Loss: 1.467 | Acc: 46.472% (9458/20352)\n",
            "Loss: 1.467 | Acc: 46.434% (9480/20416)\n",
            "Loss: 1.467 | Acc: 46.421% (9507/20480)\n",
            "Loss: 1.467 | Acc: 46.403% (9533/20544)\n",
            "Loss: 1.467 | Acc: 46.404% (9563/20608)\n",
            "Loss: 1.467 | Acc: 46.425% (9597/20672)\n",
            "Loss: 1.467 | Acc: 46.412% (9624/20736)\n",
            "Loss: 1.468 | Acc: 46.385% (9648/20800)\n",
            "Loss: 1.468 | Acc: 46.362% (9673/20864)\n",
            "Loss: 1.468 | Acc: 46.349% (9700/20928)\n",
            "Loss: 1.468 | Acc: 46.375% (9735/20992)\n",
            "Loss: 1.467 | Acc: 46.386% (9767/21056)\n",
            "Loss: 1.467 | Acc: 46.402% (9800/21120)\n",
            "Loss: 1.467 | Acc: 46.436% (9837/21184)\n",
            "Loss: 1.467 | Acc: 46.437% (9867/21248)\n",
            "Loss: 1.467 | Acc: 46.439% (9897/21312)\n",
            "Loss: 1.467 | Acc: 46.435% (9926/21376)\n",
            "Loss: 1.466 | Acc: 46.455% (9960/21440)\n",
            "Loss: 1.466 | Acc: 46.452% (9989/21504)\n",
            "Loss: 1.466 | Acc: 46.472% (10023/21568)\n",
            "Loss: 1.466 | Acc: 46.450% (10048/21632)\n",
            "Loss: 1.465 | Acc: 46.502% (10089/21696)\n",
            "Loss: 1.466 | Acc: 46.484% (10115/21760)\n",
            "Loss: 1.466 | Acc: 46.490% (10146/21824)\n",
            "Loss: 1.467 | Acc: 46.482% (10174/21888)\n",
            "Loss: 1.466 | Acc: 46.511% (10210/21952)\n",
            "Loss: 1.466 | Acc: 46.503% (10238/22016)\n",
            "Loss: 1.466 | Acc: 46.486% (10264/22080)\n",
            "Loss: 1.466 | Acc: 46.496% (10296/22144)\n",
            "Loss: 1.466 | Acc: 46.506% (10328/22208)\n",
            "Loss: 1.466 | Acc: 46.498% (10356/22272)\n",
            "Loss: 1.467 | Acc: 46.485% (10383/22336)\n",
            "Loss: 1.467 | Acc: 46.438% (10402/22400)\n",
            "Loss: 1.467 | Acc: 46.416% (10427/22464)\n",
            "Loss: 1.467 | Acc: 46.413% (10456/22528)\n",
            "Loss: 1.468 | Acc: 46.384% (10479/22592)\n",
            "Loss: 1.467 | Acc: 46.394% (10511/22656)\n",
            "Loss: 1.467 | Acc: 46.391% (10540/22720)\n",
            "Loss: 1.468 | Acc: 46.353% (10561/22784)\n",
            "Loss: 1.468 | Acc: 46.341% (10588/22848)\n",
            "Loss: 1.469 | Acc: 46.316% (10612/22912)\n",
            "Loss: 1.469 | Acc: 46.279% (10633/22976)\n",
            "Loss: 1.469 | Acc: 46.237% (10653/23040)\n",
            "Loss: 1.469 | Acc: 46.230% (10681/23104)\n",
            "Loss: 1.469 | Acc: 46.258% (10717/23168)\n",
            "Loss: 1.469 | Acc: 46.247% (10744/23232)\n",
            "Loss: 1.469 | Acc: 46.248% (10774/23296)\n",
            "Loss: 1.469 | Acc: 46.289% (10813/23360)\n",
            "Loss: 1.469 | Acc: 46.290% (10843/23424)\n",
            "Loss: 1.469 | Acc: 46.262% (10866/23488)\n",
            "Loss: 1.469 | Acc: 46.255% (10894/23552)\n",
            "Loss: 1.469 | Acc: 46.269% (10927/23616)\n",
            "Loss: 1.469 | Acc: 46.258% (10954/23680)\n",
            "Loss: 1.469 | Acc: 46.243% (10980/23744)\n",
            "Loss: 1.468 | Acc: 46.270% (11016/23808)\n",
            "Loss: 1.468 | Acc: 46.255% (11042/23872)\n",
            "Loss: 1.468 | Acc: 46.273% (11076/23936)\n",
            "Loss: 1.468 | Acc: 46.288% (11109/24000)\n",
            "Loss: 1.467 | Acc: 46.293% (11140/24064)\n",
            "Loss: 1.467 | Acc: 46.299% (11171/24128)\n",
            "Loss: 1.468 | Acc: 46.288% (11198/24192)\n",
            "Loss: 1.468 | Acc: 46.281% (11226/24256)\n",
            "Loss: 1.468 | Acc: 46.262% (11251/24320)\n",
            "Loss: 1.469 | Acc: 46.243% (11276/24384)\n",
            "Loss: 1.469 | Acc: 46.245% (11306/24448)\n",
            "Loss: 1.468 | Acc: 46.267% (11341/24512)\n",
            "Loss: 1.468 | Acc: 46.257% (11368/24576)\n",
            "Loss: 1.468 | Acc: 46.278% (11403/24640)\n",
            "Loss: 1.468 | Acc: 46.280% (11433/24704)\n",
            "Loss: 1.467 | Acc: 46.314% (11471/24768)\n",
            "Loss: 1.468 | Acc: 46.283% (11493/24832)\n",
            "Loss: 1.468 | Acc: 46.277% (11521/24896)\n",
            "Loss: 1.468 | Acc: 46.278% (11551/24960)\n",
            "Loss: 1.469 | Acc: 46.240% (11571/25024)\n",
            "Loss: 1.470 | Acc: 46.225% (11597/25088)\n",
            "Loss: 1.470 | Acc: 46.211% (11623/25152)\n",
            "Loss: 1.470 | Acc: 46.221% (11655/25216)\n",
            "Loss: 1.470 | Acc: 46.226% (11686/25280)\n",
            "Loss: 1.470 | Acc: 46.232% (11717/25344)\n",
            "Loss: 1.470 | Acc: 46.214% (11742/25408)\n",
            "Loss: 1.469 | Acc: 46.239% (11778/25472)\n",
            "Loss: 1.469 | Acc: 46.241% (11808/25536)\n",
            "Loss: 1.469 | Acc: 46.250% (11840/25600)\n",
            "Loss: 1.470 | Acc: 46.201% (11857/25664)\n",
            "Loss: 1.470 | Acc: 46.203% (11887/25728)\n",
            "Loss: 1.470 | Acc: 46.231% (11924/25792)\n",
            "Loss: 1.470 | Acc: 46.233% (11954/25856)\n",
            "Loss: 1.470 | Acc: 46.231% (11983/25920)\n",
            "Loss: 1.470 | Acc: 46.217% (12009/25984)\n",
            "Loss: 1.470 | Acc: 46.222% (12040/26048)\n",
            "Loss: 1.470 | Acc: 46.243% (12075/26112)\n",
            "Loss: 1.470 | Acc: 46.264% (12110/26176)\n",
            "Loss: 1.470 | Acc: 46.261% (12139/26240)\n",
            "Loss: 1.469 | Acc: 46.271% (12171/26304)\n",
            "Loss: 1.469 | Acc: 46.272% (12201/26368)\n",
            "Loss: 1.469 | Acc: 46.273% (12231/26432)\n",
            "Loss: 1.469 | Acc: 46.279% (12262/26496)\n",
            "Loss: 1.469 | Acc: 46.280% (12292/26560)\n",
            "Loss: 1.469 | Acc: 46.274% (12320/26624)\n",
            "Loss: 1.469 | Acc: 46.302% (12357/26688)\n",
            "Loss: 1.469 | Acc: 46.314% (12390/26752)\n",
            "Loss: 1.468 | Acc: 46.323% (12422/26816)\n",
            "Loss: 1.468 | Acc: 46.324% (12452/26880)\n",
            "Loss: 1.468 | Acc: 46.303% (12476/26944)\n",
            "Loss: 1.469 | Acc: 46.297% (12504/27008)\n",
            "Loss: 1.468 | Acc: 46.317% (12539/27072)\n",
            "Loss: 1.468 | Acc: 46.341% (12575/27136)\n",
            "Loss: 1.468 | Acc: 46.342% (12605/27200)\n",
            "Loss: 1.468 | Acc: 46.354% (12638/27264)\n",
            "Loss: 1.468 | Acc: 46.366% (12671/27328)\n",
            "Loss: 1.468 | Acc: 46.375% (12703/27392)\n",
            "Loss: 1.467 | Acc: 46.383% (12735/27456)\n",
            "Loss: 1.467 | Acc: 46.384% (12765/27520)\n",
            "Loss: 1.467 | Acc: 46.393% (12797/27584)\n",
            "Loss: 1.468 | Acc: 46.376% (12822/27648)\n",
            "Loss: 1.467 | Acc: 46.417% (12863/27712)\n",
            "Loss: 1.467 | Acc: 46.418% (12893/27776)\n",
            "Loss: 1.467 | Acc: 46.437% (12928/27840)\n",
            "Loss: 1.467 | Acc: 46.449% (12961/27904)\n",
            "Loss: 1.466 | Acc: 46.446% (12990/27968)\n",
            "Loss: 1.467 | Acc: 46.436% (13017/28032)\n",
            "Loss: 1.467 | Acc: 46.459% (13053/28096)\n",
            "Loss: 1.467 | Acc: 46.449% (13080/28160)\n",
            "Loss: 1.467 | Acc: 46.436% (13106/28224)\n",
            "Loss: 1.468 | Acc: 46.433% (13135/28288)\n",
            "Loss: 1.467 | Acc: 46.448% (13169/28352)\n",
            "Loss: 1.466 | Acc: 46.474% (13206/28416)\n",
            "Loss: 1.467 | Acc: 46.478% (13237/28480)\n",
            "Loss: 1.467 | Acc: 46.479% (13267/28544)\n",
            "Loss: 1.467 | Acc: 46.470% (13294/28608)\n",
            "Loss: 1.467 | Acc: 46.460% (13321/28672)\n",
            "Loss: 1.467 | Acc: 46.454% (13349/28736)\n",
            "Loss: 1.468 | Acc: 46.417% (13368/28800)\n",
            "Loss: 1.467 | Acc: 46.418% (13398/28864)\n",
            "Loss: 1.468 | Acc: 46.398% (13422/28928)\n",
            "Loss: 1.468 | Acc: 46.389% (13449/28992)\n",
            "Loss: 1.468 | Acc: 46.369% (13473/29056)\n",
            "Loss: 1.468 | Acc: 46.377% (13505/29120)\n",
            "Loss: 1.468 | Acc: 46.385% (13537/29184)\n",
            "Loss: 1.468 | Acc: 46.376% (13564/29248)\n",
            "Loss: 1.468 | Acc: 46.356% (13588/29312)\n",
            "Loss: 1.468 | Acc: 46.364% (13620/29376)\n",
            "Loss: 1.468 | Acc: 46.369% (13651/29440)\n",
            "Loss: 1.468 | Acc: 46.363% (13679/29504)\n",
            "Loss: 1.468 | Acc: 46.341% (13702/29568)\n",
            "Loss: 1.468 | Acc: 46.335% (13730/29632)\n",
            "Loss: 1.468 | Acc: 46.319% (13755/29696)\n",
            "Loss: 1.468 | Acc: 46.334% (13789/29760)\n",
            "Loss: 1.468 | Acc: 46.342% (13821/29824)\n",
            "Loss: 1.468 | Acc: 46.336% (13849/29888)\n",
            "Loss: 1.468 | Acc: 46.344% (13881/29952)\n",
            "Loss: 1.468 | Acc: 46.339% (13909/30016)\n",
            "Loss: 1.468 | Acc: 46.343% (13940/30080)\n",
            "Loss: 1.468 | Acc: 46.344% (13970/30144)\n",
            "Loss: 1.468 | Acc: 46.355% (14003/30208)\n",
            "Loss: 1.467 | Acc: 46.370% (14037/30272)\n",
            "Loss: 1.467 | Acc: 46.374% (14068/30336)\n",
            "Loss: 1.466 | Acc: 46.405% (14107/30400)\n",
            "Loss: 1.467 | Acc: 46.392% (14133/30464)\n",
            "Loss: 1.467 | Acc: 46.390% (14162/30528)\n",
            "Loss: 1.467 | Acc: 46.398% (14194/30592)\n",
            "Loss: 1.467 | Acc: 46.402% (14225/30656)\n",
            "Loss: 1.467 | Acc: 46.393% (14252/30720)\n",
            "Loss: 1.467 | Acc: 46.378% (14277/30784)\n",
            "Loss: 1.468 | Acc: 46.360% (14301/30848)\n",
            "Loss: 1.468 | Acc: 46.348% (14327/30912)\n",
            "Loss: 1.468 | Acc: 46.352% (14358/30976)\n",
            "Loss: 1.468 | Acc: 46.347% (14386/31040)\n",
            "Loss: 1.468 | Acc: 46.361% (14420/31104)\n",
            "Loss: 1.467 | Acc: 46.378% (14455/31168)\n",
            "Loss: 1.468 | Acc: 46.372% (14483/31232)\n",
            "Loss: 1.467 | Acc: 46.377% (14514/31296)\n",
            "Loss: 1.467 | Acc: 46.368% (14541/31360)\n",
            "Loss: 1.467 | Acc: 46.366% (14570/31424)\n",
            "Loss: 1.467 | Acc: 46.361% (14598/31488)\n",
            "Loss: 1.467 | Acc: 46.346% (14623/31552)\n",
            "Loss: 1.467 | Acc: 46.347% (14653/31616)\n",
            "Loss: 1.468 | Acc: 46.335% (14679/31680)\n",
            "Loss: 1.468 | Acc: 46.349% (14713/31744)\n",
            "Loss: 1.468 | Acc: 46.334% (14738/31808)\n",
            "Loss: 1.468 | Acc: 46.323% (14764/31872)\n",
            "Loss: 1.468 | Acc: 46.321% (14793/31936)\n",
            "Loss: 1.468 | Acc: 46.325% (14824/32000)\n",
            "Loss: 1.468 | Acc: 46.314% (14850/32064)\n",
            "Loss: 1.468 | Acc: 46.321% (14882/32128)\n",
            "Loss: 1.468 | Acc: 46.325% (14913/32192)\n",
            "Loss: 1.467 | Acc: 46.345% (14949/32256)\n",
            "Loss: 1.467 | Acc: 46.361% (14984/32320)\n",
            "Loss: 1.467 | Acc: 46.350% (15010/32384)\n",
            "Loss: 1.467 | Acc: 46.354% (15041/32448)\n",
            "Loss: 1.467 | Acc: 46.377% (15078/32512)\n",
            "Loss: 1.466 | Acc: 46.368% (15105/32576)\n",
            "Loss: 1.467 | Acc: 46.379% (15138/32640)\n",
            "Loss: 1.466 | Acc: 46.380% (15168/32704)\n",
            "Loss: 1.466 | Acc: 46.396% (15203/32768)\n",
            "Loss: 1.467 | Acc: 46.385% (15229/32832)\n",
            "Loss: 1.467 | Acc: 46.398% (15263/32896)\n",
            "Loss: 1.466 | Acc: 46.414% (15298/32960)\n",
            "Loss: 1.466 | Acc: 46.415% (15328/33024)\n",
            "Loss: 1.467 | Acc: 46.407% (15355/33088)\n",
            "Loss: 1.467 | Acc: 46.395% (15381/33152)\n",
            "Loss: 1.467 | Acc: 46.372% (15403/33216)\n",
            "Loss: 1.467 | Acc: 46.367% (15431/33280)\n",
            "Loss: 1.467 | Acc: 46.362% (15459/33344)\n",
            "Loss: 1.467 | Acc: 46.348% (15484/33408)\n",
            "Loss: 1.467 | Acc: 46.361% (15518/33472)\n",
            "Loss: 1.467 | Acc: 46.356% (15546/33536)\n",
            "Loss: 1.467 | Acc: 46.360% (15577/33600)\n",
            "Loss: 1.467 | Acc: 46.346% (15602/33664)\n",
            "Loss: 1.467 | Acc: 46.353% (15634/33728)\n",
            "Loss: 1.467 | Acc: 46.354% (15664/33792)\n",
            "Loss: 1.467 | Acc: 46.364% (15697/33856)\n",
            "Loss: 1.467 | Acc: 46.374% (15730/33920)\n",
            "Loss: 1.467 | Acc: 46.375% (15760/33984)\n",
            "Loss: 1.467 | Acc: 46.373% (15789/34048)\n",
            "Loss: 1.467 | Acc: 46.371% (15818/34112)\n",
            "Loss: 1.467 | Acc: 46.360% (15844/34176)\n",
            "Loss: 1.467 | Acc: 46.384% (15882/34240)\n",
            "Loss: 1.467 | Acc: 46.388% (15913/34304)\n",
            "Loss: 1.467 | Acc: 46.383% (15941/34368)\n",
            "Loss: 1.467 | Acc: 46.393% (15974/34432)\n",
            "Loss: 1.467 | Acc: 46.388% (16002/34496)\n",
            "Loss: 1.467 | Acc: 46.383% (16030/34560)\n",
            "Loss: 1.467 | Acc: 46.370% (16055/34624)\n",
            "Loss: 1.467 | Acc: 46.382% (16089/34688)\n",
            "Loss: 1.467 | Acc: 46.386% (16120/34752)\n",
            "Loss: 1.466 | Acc: 46.395% (16153/34816)\n",
            "Loss: 1.466 | Acc: 46.385% (16179/34880)\n",
            "Loss: 1.466 | Acc: 46.394% (16212/34944)\n",
            "Loss: 1.467 | Acc: 46.395% (16242/35008)\n",
            "Loss: 1.467 | Acc: 46.399% (16273/35072)\n",
            "Loss: 1.467 | Acc: 46.414% (16308/35136)\n",
            "Loss: 1.466 | Acc: 46.429% (16343/35200)\n",
            "Loss: 1.467 | Acc: 46.416% (16368/35264)\n",
            "Loss: 1.468 | Acc: 46.414% (16397/35328)\n",
            "Loss: 1.468 | Acc: 46.417% (16428/35392)\n",
            "Loss: 1.467 | Acc: 46.418% (16458/35456)\n",
            "Loss: 1.468 | Acc: 46.413% (16486/35520)\n",
            "Loss: 1.468 | Acc: 46.403% (16512/35584)\n",
            "Loss: 1.468 | Acc: 46.398% (16540/35648)\n",
            "Loss: 1.468 | Acc: 46.385% (16565/35712)\n",
            "Loss: 1.468 | Acc: 46.397% (16599/35776)\n",
            "Loss: 1.468 | Acc: 46.395% (16628/35840)\n",
            "Loss: 1.468 | Acc: 46.407% (16662/35904)\n",
            "Loss: 1.469 | Acc: 46.405% (16691/35968)\n",
            "Loss: 1.469 | Acc: 46.412% (16723/36032)\n",
            "Loss: 1.468 | Acc: 46.418% (16755/36096)\n",
            "Loss: 1.468 | Acc: 46.421% (16786/36160)\n",
            "Loss: 1.468 | Acc: 46.408% (16811/36224)\n",
            "Loss: 1.468 | Acc: 46.426% (16847/36288)\n",
            "Loss: 1.468 | Acc: 46.421% (16875/36352)\n",
            "Loss: 1.468 | Acc: 46.405% (16899/36416)\n",
            "Loss: 1.468 | Acc: 46.393% (16924/36480)\n",
            "Loss: 1.469 | Acc: 46.388% (16952/36544)\n",
            "Loss: 1.468 | Acc: 46.416% (16992/36608)\n",
            "Loss: 1.468 | Acc: 46.406% (17018/36672)\n",
            "Loss: 1.468 | Acc: 46.401% (17046/36736)\n",
            "Loss: 1.468 | Acc: 46.413% (17080/36800)\n",
            "Loss: 1.468 | Acc: 46.414% (17110/36864)\n",
            "Loss: 1.468 | Acc: 46.415% (17140/36928)\n",
            "Loss: 1.468 | Acc: 46.410% (17168/36992)\n",
            "Loss: 1.468 | Acc: 46.416% (17200/37056)\n",
            "Loss: 1.468 | Acc: 46.433% (17236/37120)\n",
            "Loss: 1.468 | Acc: 46.455% (17274/37184)\n",
            "Loss: 1.467 | Acc: 46.486% (17315/37248)\n",
            "Loss: 1.467 | Acc: 46.489% (17346/37312)\n",
            "Loss: 1.467 | Acc: 46.487% (17375/37376)\n",
            "Loss: 1.467 | Acc: 46.496% (17408/37440)\n",
            "Loss: 1.467 | Acc: 46.496% (17438/37504)\n",
            "Loss: 1.466 | Acc: 46.489% (17465/37568)\n",
            "Loss: 1.466 | Acc: 46.479% (17491/37632)\n",
            "Loss: 1.466 | Acc: 46.485% (17523/37696)\n",
            "Loss: 1.466 | Acc: 46.475% (17549/37760)\n",
            "Loss: 1.466 | Acc: 46.473% (17578/37824)\n",
            "Loss: 1.466 | Acc: 46.490% (17614/37888)\n",
            "Loss: 1.466 | Acc: 46.498% (17647/37952)\n",
            "Loss: 1.466 | Acc: 46.499% (17677/38016)\n",
            "Loss: 1.465 | Acc: 46.502% (17708/38080)\n",
            "Loss: 1.466 | Acc: 46.497% (17736/38144)\n",
            "Loss: 1.465 | Acc: 46.506% (17769/38208)\n",
            "Loss: 1.465 | Acc: 46.509% (17800/38272)\n",
            "Loss: 1.466 | Acc: 46.507% (17829/38336)\n",
            "Loss: 1.466 | Acc: 46.500% (17856/38400)\n",
            "Loss: 1.466 | Acc: 46.508% (17889/38464)\n",
            "Loss: 1.466 | Acc: 46.491% (17912/38528)\n",
            "Loss: 1.466 | Acc: 46.466% (17932/38592)\n",
            "Loss: 1.466 | Acc: 46.477% (17966/38656)\n",
            "Loss: 1.466 | Acc: 46.464% (17991/38720)\n",
            "Loss: 1.467 | Acc: 46.455% (18017/38784)\n",
            "Loss: 1.467 | Acc: 46.437% (18040/38848)\n",
            "Loss: 1.467 | Acc: 46.446% (18073/38912)\n",
            "Loss: 1.467 | Acc: 46.441% (18101/38976)\n",
            "Loss: 1.467 | Acc: 46.432% (18127/39040)\n",
            "Loss: 1.467 | Acc: 46.443% (18161/39104)\n",
            "Loss: 1.467 | Acc: 46.444% (18191/39168)\n",
            "Loss: 1.467 | Acc: 46.449% (18223/39232)\n",
            "Loss: 1.467 | Acc: 46.453% (18254/39296)\n",
            "Loss: 1.467 | Acc: 46.456% (18285/39360)\n",
            "Loss: 1.467 | Acc: 46.464% (18318/39424)\n",
            "Loss: 1.467 | Acc: 46.477% (18353/39488)\n",
            "Loss: 1.467 | Acc: 46.473% (18381/39552)\n",
            "Loss: 1.466 | Acc: 46.464% (18407/39616)\n",
            "Loss: 1.466 | Acc: 46.464% (18437/39680)\n",
            "Loss: 1.466 | Acc: 46.470% (18469/39744)\n",
            "Loss: 1.466 | Acc: 46.476% (18501/39808)\n",
            "Loss: 1.466 | Acc: 46.461% (18525/39872)\n",
            "Loss: 1.466 | Acc: 46.462% (18555/39936)\n",
            "Loss: 1.466 | Acc: 46.450% (18580/40000)\n",
            "Loss: 1.466 | Acc: 46.443% (18607/40064)\n",
            "Loss: 1.465 | Acc: 46.456% (18642/40128)\n",
            "Loss: 1.466 | Acc: 46.450% (18669/40192)\n",
            "Loss: 1.466 | Acc: 46.450% (18699/40256)\n",
            "Loss: 1.466 | Acc: 46.466% (18735/40320)\n",
            "Loss: 1.466 | Acc: 46.461% (18763/40384)\n",
            "Loss: 1.466 | Acc: 46.470% (18796/40448)\n",
            "Loss: 1.465 | Acc: 46.473% (18827/40512)\n",
            "Loss: 1.465 | Acc: 46.493% (18865/40576)\n",
            "Loss: 1.465 | Acc: 46.503% (18899/40640)\n",
            "Loss: 1.464 | Acc: 46.514% (18933/40704)\n",
            "Loss: 1.464 | Acc: 46.507% (18960/40768)\n",
            "Loss: 1.464 | Acc: 46.515% (18993/40832)\n",
            "Loss: 1.464 | Acc: 46.513% (19022/40896)\n",
            "Loss: 1.464 | Acc: 46.526% (19057/40960)\n",
            "Loss: 1.464 | Acc: 46.529% (19088/41024)\n",
            "Loss: 1.464 | Acc: 46.544% (19124/41088)\n",
            "Loss: 1.463 | Acc: 46.545% (19154/41152)\n",
            "Loss: 1.464 | Acc: 46.535% (19180/41216)\n",
            "Loss: 1.464 | Acc: 46.529% (19207/41280)\n",
            "Loss: 1.464 | Acc: 46.539% (19241/41344)\n",
            "Loss: 1.464 | Acc: 46.525% (19265/41408)\n",
            "Loss: 1.463 | Acc: 46.537% (19300/41472)\n",
            "Loss: 1.463 | Acc: 46.543% (19332/41536)\n",
            "Loss: 1.464 | Acc: 46.524% (19354/41600)\n",
            "Loss: 1.464 | Acc: 46.529% (19386/41664)\n",
            "Loss: 1.464 | Acc: 46.530% (19416/41728)\n",
            "Loss: 1.464 | Acc: 46.518% (19441/41792)\n",
            "Loss: 1.464 | Acc: 46.514% (19469/41856)\n",
            "Loss: 1.464 | Acc: 46.510% (19497/41920)\n",
            "Loss: 1.464 | Acc: 46.501% (19523/41984)\n",
            "Loss: 1.464 | Acc: 46.509% (19556/42048)\n",
            "Loss: 1.464 | Acc: 46.514% (19588/42112)\n",
            "Loss: 1.464 | Acc: 46.512% (19617/42176)\n",
            "Loss: 1.464 | Acc: 46.503% (19643/42240)\n",
            "Loss: 1.464 | Acc: 46.506% (19674/42304)\n",
            "Loss: 1.464 | Acc: 46.516% (19708/42368)\n",
            "Loss: 1.464 | Acc: 46.507% (19734/42432)\n",
            "Loss: 1.464 | Acc: 46.501% (19761/42496)\n",
            "Loss: 1.464 | Acc: 46.513% (19796/42560)\n",
            "Loss: 1.464 | Acc: 46.516% (19827/42624)\n",
            "Loss: 1.464 | Acc: 46.517% (19857/42688)\n",
            "Loss: 1.463 | Acc: 46.512% (19885/42752)\n",
            "Loss: 1.463 | Acc: 46.525% (19920/42816)\n",
            "Loss: 1.463 | Acc: 46.528% (19951/42880)\n",
            "Loss: 1.463 | Acc: 46.528% (19981/42944)\n",
            "Loss: 1.463 | Acc: 46.533% (20013/43008)\n",
            "Loss: 1.463 | Acc: 46.529% (20041/43072)\n",
            "Loss: 1.463 | Acc: 46.516% (20065/43136)\n",
            "Loss: 1.463 | Acc: 46.530% (20101/43200)\n",
            "Loss: 1.463 | Acc: 46.521% (20127/43264)\n",
            "Loss: 1.463 | Acc: 46.529% (20160/43328)\n",
            "Loss: 1.463 | Acc: 46.527% (20189/43392)\n",
            "Loss: 1.463 | Acc: 46.530% (20220/43456)\n",
            "Loss: 1.464 | Acc: 46.514% (20243/43520)\n",
            "Loss: 1.464 | Acc: 46.524% (20277/43584)\n",
            "Loss: 1.464 | Acc: 46.531% (20310/43648)\n",
            "Loss: 1.463 | Acc: 46.539% (20343/43712)\n",
            "Loss: 1.463 | Acc: 46.551% (20378/43776)\n",
            "Loss: 1.463 | Acc: 46.549% (20407/43840)\n",
            "Loss: 1.463 | Acc: 46.542% (20434/43904)\n",
            "Loss: 1.463 | Acc: 46.532% (20459/43968)\n",
            "Loss: 1.463 | Acc: 46.537% (20491/44032)\n",
            "Loss: 1.464 | Acc: 46.528% (20517/44096)\n",
            "Loss: 1.464 | Acc: 46.526% (20546/44160)\n",
            "Loss: 1.464 | Acc: 46.538% (20581/44224)\n",
            "Loss: 1.463 | Acc: 46.561% (20621/44288)\n",
            "Loss: 1.463 | Acc: 46.575% (20657/44352)\n",
            "Loss: 1.463 | Acc: 46.585% (20691/44416)\n",
            "Loss: 1.463 | Acc: 46.585% (20721/44480)\n",
            "Loss: 1.463 | Acc: 46.583% (20750/44544)\n",
            "Loss: 1.463 | Acc: 46.584% (20780/44608)\n",
            "Loss: 1.464 | Acc: 46.568% (20803/44672)\n",
            "Loss: 1.464 | Acc: 46.580% (20838/44736)\n",
            "Loss: 1.464 | Acc: 46.571% (20864/44800)\n",
            "Loss: 1.464 | Acc: 46.570% (20893/44864)\n",
            "Loss: 1.464 | Acc: 46.568% (20922/44928)\n",
            "Loss: 1.464 | Acc: 46.559% (20948/44992)\n",
            "Loss: 1.464 | Acc: 46.558% (20977/45056)\n",
            "Loss: 1.464 | Acc: 46.562% (21009/45120)\n",
            "Loss: 1.464 | Acc: 46.570% (21042/45184)\n",
            "Loss: 1.464 | Acc: 46.570% (21072/45248)\n",
            "Loss: 1.465 | Acc: 46.553% (21094/45312)\n",
            "Loss: 1.465 | Acc: 46.547% (21121/45376)\n",
            "Loss: 1.465 | Acc: 46.536% (21146/45440)\n",
            "Loss: 1.465 | Acc: 46.530% (21173/45504)\n",
            "Loss: 1.465 | Acc: 46.537% (21206/45568)\n",
            "Loss: 1.465 | Acc: 46.548% (21241/45632)\n",
            "Loss: 1.465 | Acc: 46.545% (21269/45696)\n",
            "Loss: 1.465 | Acc: 46.538% (21296/45760)\n",
            "Loss: 1.465 | Acc: 46.550% (21331/45824)\n",
            "Loss: 1.464 | Acc: 46.548% (21360/45888)\n",
            "Loss: 1.464 | Acc: 46.553% (21392/45952)\n",
            "Loss: 1.464 | Acc: 46.560% (21425/46016)\n",
            "Loss: 1.464 | Acc: 46.571% (21460/46080)\n",
            "Loss: 1.464 | Acc: 46.569% (21489/46144)\n",
            "Loss: 1.463 | Acc: 46.576% (21522/46208)\n",
            "Loss: 1.463 | Acc: 46.570% (21549/46272)\n",
            "Loss: 1.464 | Acc: 46.558% (21573/46336)\n",
            "Loss: 1.464 | Acc: 46.543% (21596/46400)\n",
            "Loss: 1.464 | Acc: 46.539% (21624/46464)\n",
            "Loss: 1.464 | Acc: 46.548% (21658/46528)\n",
            "Loss: 1.463 | Acc: 46.559% (21693/46592)\n",
            "Loss: 1.463 | Acc: 46.571% (21728/46656)\n",
            "Loss: 1.463 | Acc: 46.573% (21759/46720)\n",
            "Loss: 1.463 | Acc: 46.584% (21794/46784)\n",
            "Loss: 1.463 | Acc: 46.587% (21825/46848)\n",
            "Loss: 1.463 | Acc: 46.587% (21855/46912)\n",
            "Loss: 1.463 | Acc: 46.594% (21888/46976)\n",
            "Loss: 1.463 | Acc: 46.597% (21919/47040)\n",
            "Loss: 1.462 | Acc: 46.605% (21953/47104)\n",
            "Loss: 1.462 | Acc: 46.604% (21982/47168)\n",
            "Loss: 1.462 | Acc: 46.604% (22012/47232)\n",
            "Loss: 1.462 | Acc: 46.619% (22049/47296)\n",
            "Loss: 1.462 | Acc: 46.634% (22086/47360)\n",
            "Loss: 1.462 | Acc: 46.633% (22115/47424)\n",
            "Loss: 1.462 | Acc: 46.641% (22149/47488)\n",
            "Loss: 1.462 | Acc: 46.646% (22181/47552)\n",
            "Loss: 1.462 | Acc: 46.650% (22213/47616)\n",
            "Loss: 1.462 | Acc: 46.651% (22243/47680)\n",
            "Loss: 1.461 | Acc: 46.651% (22273/47744)\n",
            "Loss: 1.461 | Acc: 46.660% (22307/47808)\n",
            "Loss: 1.461 | Acc: 46.670% (22342/47872)\n",
            "Loss: 1.461 | Acc: 46.677% (22375/47936)\n",
            "Loss: 1.461 | Acc: 46.696% (22414/48000)\n",
            "Loss: 1.461 | Acc: 46.698% (22445/48064)\n",
            "Loss: 1.461 | Acc: 46.690% (22471/48128)\n",
            "Loss: 1.461 | Acc: 46.694% (22503/48192)\n",
            "Loss: 1.461 | Acc: 46.693% (22532/48256)\n",
            "Loss: 1.461 | Acc: 46.691% (22561/48320)\n",
            "Loss: 1.461 | Acc: 46.693% (22592/48384)\n",
            "Loss: 1.461 | Acc: 46.695% (22623/48448)\n",
            "Loss: 1.461 | Acc: 46.700% (22655/48512)\n",
            "Loss: 1.460 | Acc: 46.719% (22694/48576)\n",
            "Loss: 1.460 | Acc: 46.727% (22728/48640)\n",
            "Loss: 1.460 | Acc: 46.713% (22751/48704)\n",
            "Loss: 1.460 | Acc: 46.711% (22780/48768)\n",
            "Loss: 1.461 | Acc: 46.699% (22804/48832)\n",
            "Loss: 1.460 | Acc: 46.697% (22833/48896)\n",
            "Loss: 1.460 | Acc: 46.699% (22864/48960)\n",
            "Loss: 1.460 | Acc: 46.694% (22880/49000)\n",
            "Epoch 1 of training is completed, Training accuracy for this epoch is 46.69387755102041\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 1.455 | Acc: 42.188% (27/64)\n",
            "Loss: 1.374 | Acc: 46.094% (59/128)\n",
            "Loss: 1.430 | Acc: 44.792% (86/192)\n",
            "Loss: 1.457 | Acc: 43.359% (111/256)\n",
            "Loss: 1.438 | Acc: 43.125% (138/320)\n",
            "Loss: 1.452 | Acc: 42.708% (164/384)\n",
            "Loss: 1.476 | Acc: 43.973% (197/448)\n",
            "Loss: 1.466 | Acc: 44.727% (229/512)\n",
            "Loss: 1.463 | Acc: 45.312% (261/576)\n",
            "Loss: 1.453 | Acc: 45.000% (288/640)\n",
            "Loss: 1.461 | Acc: 44.602% (314/704)\n",
            "Loss: 1.466 | Acc: 44.792% (344/768)\n",
            "Loss: 1.466 | Acc: 45.192% (376/832)\n",
            "Loss: 1.451 | Acc: 45.424% (407/896)\n",
            "Loss: 1.438 | Acc: 46.042% (442/960)\n",
            "Loss: 1.419 | Acc: 46.875% (480/1024)\n",
            "Loss: 1.423 | Acc: 47.059% (512/1088)\n",
            "Loss: 1.426 | Acc: 47.222% (544/1152)\n",
            "Loss: 1.424 | Acc: 47.368% (576/1216)\n",
            "Loss: 1.435 | Acc: 46.953% (601/1280)\n",
            "Loss: 1.434 | Acc: 46.652% (627/1344)\n",
            "Loss: 1.432 | Acc: 46.662% (657/1408)\n",
            "Loss: 1.421 | Acc: 46.603% (686/1472)\n",
            "Loss: 1.427 | Acc: 46.419% (713/1536)\n",
            "Loss: 1.432 | Acc: 46.312% (741/1600)\n",
            "Loss: 1.439 | Acc: 46.394% (772/1664)\n",
            "Loss: 1.445 | Acc: 46.238% (799/1728)\n",
            "Loss: 1.440 | Acc: 46.652% (836/1792)\n",
            "Loss: 1.437 | Acc: 46.875% (870/1856)\n",
            "Loss: 1.438 | Acc: 46.979% (902/1920)\n",
            "Loss: 1.437 | Acc: 47.127% (935/1984)\n",
            "Loss: 1.438 | Acc: 47.314% (969/2048)\n",
            "Loss: 1.432 | Acc: 47.491% (1003/2112)\n",
            "Loss: 1.434 | Acc: 47.381% (1031/2176)\n",
            "Loss: 1.431 | Acc: 47.500% (1064/2240)\n",
            "Loss: 1.431 | Acc: 47.569% (1096/2304)\n",
            "Loss: 1.431 | Acc: 47.677% (1129/2368)\n",
            "Loss: 1.430 | Acc: 47.697% (1160/2432)\n",
            "Loss: 1.429 | Acc: 47.997% (1198/2496)\n",
            "Loss: 1.439 | Acc: 47.969% (1228/2560)\n",
            "Loss: 1.439 | Acc: 47.828% (1255/2624)\n",
            "Loss: 1.439 | Acc: 47.842% (1286/2688)\n",
            "Loss: 1.438 | Acc: 47.892% (1318/2752)\n",
            "Loss: 1.441 | Acc: 47.798% (1346/2816)\n",
            "Loss: 1.443 | Acc: 47.708% (1374/2880)\n",
            "Loss: 1.439 | Acc: 47.962% (1412/2944)\n",
            "Loss: 1.438 | Acc: 47.939% (1442/3008)\n",
            "Loss: 1.437 | Acc: 47.852% (1470/3072)\n",
            "Loss: 1.436 | Acc: 47.864% (1501/3136)\n",
            "Loss: 1.435 | Acc: 47.906% (1533/3200)\n",
            "Loss: 1.435 | Acc: 47.886% (1563/3264)\n",
            "Loss: 1.437 | Acc: 47.806% (1591/3328)\n",
            "Loss: 1.435 | Acc: 47.818% (1622/3392)\n",
            "Loss: 1.435 | Acc: 47.743% (1650/3456)\n",
            "Loss: 1.435 | Acc: 47.756% (1681/3520)\n",
            "Loss: 1.429 | Acc: 48.131% (1725/3584)\n",
            "Loss: 1.430 | Acc: 47.999% (1751/3648)\n",
            "Loss: 1.429 | Acc: 48.168% (1788/3712)\n",
            "Loss: 1.429 | Acc: 48.199% (1820/3776)\n",
            "Loss: 1.427 | Acc: 48.333% (1856/3840)\n",
            "Loss: 1.425 | Acc: 48.463% (1892/3904)\n",
            "Loss: 1.422 | Acc: 48.740% (1934/3968)\n",
            "Loss: 1.423 | Acc: 48.735% (1965/4032)\n",
            "Loss: 1.428 | Acc: 48.706% (1995/4096)\n",
            "Loss: 1.429 | Acc: 48.678% (2025/4160)\n",
            "Loss: 1.426 | Acc: 48.816% (2062/4224)\n",
            "Loss: 1.425 | Acc: 48.904% (2097/4288)\n",
            "Loss: 1.423 | Acc: 49.012% (2133/4352)\n",
            "Loss: 1.421 | Acc: 49.117% (2169/4416)\n",
            "Loss: 1.420 | Acc: 49.107% (2200/4480)\n",
            "Loss: 1.418 | Acc: 49.142% (2233/4544)\n",
            "Loss: 1.420 | Acc: 49.110% (2263/4608)\n",
            "Loss: 1.420 | Acc: 49.058% (2292/4672)\n",
            "Loss: 1.418 | Acc: 49.177% (2329/4736)\n",
            "Loss: 1.419 | Acc: 49.208% (2362/4800)\n",
            "Loss: 1.418 | Acc: 49.260% (2396/4864)\n",
            "Loss: 1.422 | Acc: 49.249% (2427/4928)\n",
            "Loss: 1.422 | Acc: 49.139% (2453/4992)\n",
            "Loss: 1.422 | Acc: 49.110% (2483/5056)\n",
            "Loss: 1.424 | Acc: 49.023% (2510/5120)\n",
            "Loss: 1.421 | Acc: 49.132% (2547/5184)\n",
            "Loss: 1.424 | Acc: 48.990% (2571/5248)\n",
            "Loss: 1.424 | Acc: 48.908% (2598/5312)\n",
            "Loss: 1.426 | Acc: 48.828% (2625/5376)\n",
            "Loss: 1.426 | Acc: 48.897% (2660/5440)\n",
            "Loss: 1.428 | Acc: 48.892% (2691/5504)\n",
            "Loss: 1.433 | Acc: 48.671% (2710/5568)\n",
            "Loss: 1.433 | Acc: 48.739% (2745/5632)\n",
            "Loss: 1.433 | Acc: 48.666% (2772/5696)\n",
            "Loss: 1.432 | Acc: 48.663% (2803/5760)\n",
            "Loss: 1.432 | Acc: 48.695% (2836/5824)\n",
            "Loss: 1.436 | Acc: 48.539% (2858/5888)\n",
            "Loss: 1.436 | Acc: 48.522% (2888/5952)\n",
            "Loss: 1.435 | Acc: 48.537% (2920/6016)\n",
            "Loss: 1.437 | Acc: 48.454% (2946/6080)\n",
            "Loss: 1.436 | Acc: 48.470% (2978/6144)\n",
            "Loss: 1.435 | Acc: 48.566% (3015/6208)\n",
            "Loss: 1.437 | Acc: 48.485% (3041/6272)\n",
            "Loss: 1.436 | Acc: 48.532% (3075/6336)\n",
            "Loss: 1.437 | Acc: 48.484% (3103/6400)\n",
            "Loss: 1.438 | Acc: 48.468% (3133/6464)\n",
            "Loss: 1.439 | Acc: 48.483% (3165/6528)\n",
            "Loss: 1.441 | Acc: 48.453% (3194/6592)\n",
            "Loss: 1.441 | Acc: 48.407% (3222/6656)\n",
            "Loss: 1.441 | Acc: 48.348% (3249/6720)\n",
            "Loss: 1.440 | Acc: 48.379% (3282/6784)\n",
            "Loss: 1.439 | Acc: 48.423% (3316/6848)\n",
            "Loss: 1.441 | Acc: 48.293% (3338/6912)\n",
            "Loss: 1.442 | Acc: 48.308% (3370/6976)\n",
            "Loss: 1.442 | Acc: 48.281% (3399/7040)\n",
            "Loss: 1.442 | Acc: 48.283% (3430/7104)\n",
            "Loss: 1.441 | Acc: 48.298% (3462/7168)\n",
            "Loss: 1.443 | Acc: 48.216% (3487/7232)\n",
            "Loss: 1.441 | Acc: 48.218% (3518/7296)\n",
            "Loss: 1.439 | Acc: 48.302% (3555/7360)\n",
            "Loss: 1.441 | Acc: 48.222% (3580/7424)\n",
            "Loss: 1.438 | Acc: 48.371% (3622/7488)\n",
            "Loss: 1.437 | Acc: 48.385% (3654/7552)\n",
            "Loss: 1.437 | Acc: 48.346% (3682/7616)\n",
            "Loss: 1.437 | Acc: 48.333% (3712/7680)\n",
            "Loss: 1.435 | Acc: 48.399% (3748/7744)\n",
            "Loss: 1.434 | Acc: 48.450% (3783/7808)\n",
            "Loss: 1.435 | Acc: 48.310% (3803/7872)\n",
            "Loss: 1.434 | Acc: 48.337% (3836/7936)\n",
            "Loss: 1.435 | Acc: 48.288% (3863/8000)\n",
            "Loss: 1.433 | Acc: 48.301% (3895/8064)\n",
            "Loss: 1.434 | Acc: 48.253% (3922/8128)\n",
            "Loss: 1.434 | Acc: 48.279% (3955/8192)\n",
            "Loss: 1.435 | Acc: 48.110% (3972/8256)\n",
            "Loss: 1.438 | Acc: 47.945% (3989/8320)\n",
            "Loss: 1.438 | Acc: 47.901% (4016/8384)\n",
            "Loss: 1.438 | Acc: 47.917% (4048/8448)\n",
            "Loss: 1.438 | Acc: 47.897% (4077/8512)\n",
            "Loss: 1.439 | Acc: 47.866% (4105/8576)\n",
            "Loss: 1.439 | Acc: 47.824% (4132/8640)\n",
            "Loss: 1.441 | Acc: 47.771% (4158/8704)\n",
            "Loss: 1.441 | Acc: 47.742% (4186/8768)\n",
            "Loss: 1.440 | Acc: 47.769% (4219/8832)\n",
            "Loss: 1.440 | Acc: 47.819% (4254/8896)\n",
            "Loss: 1.440 | Acc: 47.824% (4285/8960)\n",
            "Loss: 1.440 | Acc: 47.806% (4314/9024)\n",
            "Loss: 1.440 | Acc: 47.788% (4343/9088)\n",
            "Loss: 1.440 | Acc: 47.804% (4375/9152)\n",
            "Loss: 1.438 | Acc: 47.841% (4409/9216)\n",
            "Loss: 1.437 | Acc: 47.920% (4447/9280)\n",
            "Loss: 1.437 | Acc: 47.849% (4471/9344)\n",
            "Loss: 1.437 | Acc: 47.906% (4507/9408)\n",
            "Loss: 1.438 | Acc: 47.878% (4535/9472)\n",
            "Loss: 1.437 | Acc: 47.903% (4568/9536)\n",
            "Loss: 1.436 | Acc: 48.010% (4609/9600)\n",
            "Loss: 1.437 | Acc: 47.972% (4636/9664)\n",
            "Loss: 1.437 | Acc: 47.985% (4668/9728)\n",
            "Loss: 1.437 | Acc: 48.049% (4705/9792)\n",
            "Loss: 1.437 | Acc: 48.052% (4736/9856)\n",
            "Loss: 1.438 | Acc: 48.085% (4770/9920)\n",
            "Loss: 1.437 | Acc: 48.127% (4805/9984)\n",
            "Loss: 1.437 | Acc: 48.100% (4810/10000)\n",
            "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 48.1\n",
            "\n",
            "Epoch: 2\n",
            "Loss: 1.461 | Acc: 53.125% (34/64)\n",
            "Loss: 1.499 | Acc: 46.875% (60/128)\n",
            "Loss: 1.412 | Acc: 51.562% (99/192)\n",
            "Loss: 1.392 | Acc: 51.953% (133/256)\n",
            "Loss: 1.364 | Acc: 52.812% (169/320)\n",
            "Loss: 1.355 | Acc: 53.385% (205/384)\n",
            "Loss: 1.352 | Acc: 53.795% (241/448)\n",
            "Loss: 1.366 | Acc: 52.539% (269/512)\n",
            "Loss: 1.346 | Acc: 53.299% (307/576)\n",
            "Loss: 1.328 | Acc: 53.281% (341/640)\n",
            "Loss: 1.325 | Acc: 53.267% (375/704)\n",
            "Loss: 1.354 | Acc: 51.953% (399/768)\n",
            "Loss: 1.348 | Acc: 52.284% (435/832)\n",
            "Loss: 1.360 | Acc: 51.786% (464/896)\n",
            "Loss: 1.371 | Acc: 51.146% (491/960)\n",
            "Loss: 1.378 | Acc: 51.074% (523/1024)\n",
            "Loss: 1.399 | Acc: 49.632% (540/1088)\n",
            "Loss: 1.401 | Acc: 49.653% (572/1152)\n",
            "Loss: 1.413 | Acc: 49.424% (601/1216)\n",
            "Loss: 1.410 | Acc: 49.375% (632/1280)\n",
            "Loss: 1.421 | Acc: 49.033% (659/1344)\n",
            "Loss: 1.427 | Acc: 48.793% (687/1408)\n",
            "Loss: 1.430 | Acc: 48.302% (711/1472)\n",
            "Loss: 1.429 | Acc: 48.372% (743/1536)\n",
            "Loss: 1.424 | Acc: 48.375% (774/1600)\n",
            "Loss: 1.425 | Acc: 48.257% (803/1664)\n",
            "Loss: 1.420 | Acc: 48.495% (838/1728)\n",
            "Loss: 1.416 | Acc: 48.772% (874/1792)\n",
            "Loss: 1.418 | Acc: 48.707% (904/1856)\n",
            "Loss: 1.415 | Acc: 48.854% (938/1920)\n",
            "Loss: 1.407 | Acc: 48.992% (972/1984)\n",
            "Loss: 1.410 | Acc: 48.730% (998/2048)\n",
            "Loss: 1.407 | Acc: 48.769% (1030/2112)\n",
            "Loss: 1.402 | Acc: 48.759% (1061/2176)\n",
            "Loss: 1.395 | Acc: 49.018% (1098/2240)\n",
            "Loss: 1.398 | Acc: 48.655% (1121/2304)\n",
            "Loss: 1.400 | Acc: 48.733% (1154/2368)\n",
            "Loss: 1.398 | Acc: 48.684% (1184/2432)\n",
            "Loss: 1.397 | Acc: 48.878% (1220/2496)\n",
            "Loss: 1.403 | Acc: 48.516% (1242/2560)\n",
            "Loss: 1.403 | Acc: 48.628% (1276/2624)\n",
            "Loss: 1.408 | Acc: 48.400% (1301/2688)\n",
            "Loss: 1.403 | Acc: 48.656% (1339/2752)\n",
            "Loss: 1.405 | Acc: 48.615% (1369/2816)\n",
            "Loss: 1.408 | Acc: 48.542% (1398/2880)\n",
            "Loss: 1.404 | Acc: 48.607% (1431/2944)\n",
            "Loss: 1.397 | Acc: 48.936% (1472/3008)\n",
            "Loss: 1.392 | Acc: 49.056% (1507/3072)\n",
            "Loss: 1.396 | Acc: 48.948% (1535/3136)\n",
            "Loss: 1.396 | Acc: 49.000% (1568/3200)\n",
            "Loss: 1.395 | Acc: 49.112% (1603/3264)\n",
            "Loss: 1.395 | Acc: 49.038% (1632/3328)\n",
            "Loss: 1.391 | Acc: 49.057% (1664/3392)\n",
            "Loss: 1.390 | Acc: 48.987% (1693/3456)\n",
            "Loss: 1.390 | Acc: 49.034% (1726/3520)\n",
            "Loss: 1.388 | Acc: 49.247% (1765/3584)\n",
            "Loss: 1.392 | Acc: 48.958% (1786/3648)\n",
            "Loss: 1.393 | Acc: 48.976% (1818/3712)\n",
            "Loss: 1.396 | Acc: 48.941% (1848/3776)\n",
            "Loss: 1.397 | Acc: 48.906% (1878/3840)\n",
            "Loss: 1.397 | Acc: 48.822% (1906/3904)\n",
            "Loss: 1.397 | Acc: 48.715% (1933/3968)\n",
            "Loss: 1.400 | Acc: 48.562% (1958/4032)\n",
            "Loss: 1.399 | Acc: 48.633% (1992/4096)\n",
            "Loss: 1.399 | Acc: 48.558% (2020/4160)\n",
            "Loss: 1.398 | Acc: 48.509% (2049/4224)\n",
            "Loss: 1.397 | Acc: 48.531% (2081/4288)\n",
            "Loss: 1.398 | Acc: 48.529% (2112/4352)\n",
            "Loss: 1.399 | Acc: 48.528% (2143/4416)\n",
            "Loss: 1.395 | Acc: 48.661% (2180/4480)\n",
            "Loss: 1.396 | Acc: 48.592% (2208/4544)\n",
            "Loss: 1.393 | Acc: 48.741% (2246/4608)\n",
            "Loss: 1.393 | Acc: 48.801% (2280/4672)\n",
            "Loss: 1.393 | Acc: 48.796% (2311/4736)\n",
            "Loss: 1.393 | Acc: 48.833% (2344/4800)\n",
            "Loss: 1.396 | Acc: 48.808% (2374/4864)\n",
            "Loss: 1.396 | Acc: 48.904% (2410/4928)\n",
            "Loss: 1.394 | Acc: 48.958% (2444/4992)\n",
            "Loss: 1.394 | Acc: 49.031% (2479/5056)\n",
            "Loss: 1.394 | Acc: 49.121% (2515/5120)\n",
            "Loss: 1.392 | Acc: 49.228% (2552/5184)\n",
            "Loss: 1.392 | Acc: 49.238% (2584/5248)\n",
            "Loss: 1.390 | Acc: 49.247% (2616/5312)\n",
            "Loss: 1.394 | Acc: 49.163% (2643/5376)\n",
            "Loss: 1.393 | Acc: 49.191% (2676/5440)\n",
            "Loss: 1.394 | Acc: 49.164% (2706/5504)\n",
            "Loss: 1.394 | Acc: 49.156% (2737/5568)\n",
            "Loss: 1.394 | Acc: 49.183% (2770/5632)\n",
            "Loss: 1.393 | Acc: 49.228% (2804/5696)\n",
            "Loss: 1.391 | Acc: 49.236% (2836/5760)\n",
            "Loss: 1.391 | Acc: 49.245% (2868/5824)\n",
            "Loss: 1.392 | Acc: 49.287% (2902/5888)\n",
            "Loss: 1.390 | Acc: 49.345% (2937/5952)\n",
            "Loss: 1.393 | Acc: 49.235% (2962/6016)\n",
            "Loss: 1.392 | Acc: 49.359% (3001/6080)\n",
            "Loss: 1.390 | Acc: 49.479% (3040/6144)\n",
            "Loss: 1.390 | Acc: 49.452% (3070/6208)\n",
            "Loss: 1.390 | Acc: 49.474% (3103/6272)\n",
            "Loss: 1.390 | Acc: 49.479% (3135/6336)\n",
            "Loss: 1.388 | Acc: 49.547% (3171/6400)\n",
            "Loss: 1.385 | Acc: 49.629% (3208/6464)\n",
            "Loss: 1.386 | Acc: 49.632% (3240/6528)\n",
            "Loss: 1.385 | Acc: 49.651% (3273/6592)\n",
            "Loss: 1.387 | Acc: 49.654% (3305/6656)\n",
            "Loss: 1.387 | Acc: 49.688% (3339/6720)\n",
            "Loss: 1.385 | Acc: 49.764% (3376/6784)\n",
            "Loss: 1.385 | Acc: 49.737% (3406/6848)\n",
            "Loss: 1.383 | Acc: 49.812% (3443/6912)\n",
            "Loss: 1.386 | Acc: 49.685% (3466/6976)\n",
            "Loss: 1.388 | Acc: 49.631% (3494/7040)\n",
            "Loss: 1.385 | Acc: 49.733% (3533/7104)\n",
            "Loss: 1.384 | Acc: 49.721% (3564/7168)\n",
            "Loss: 1.385 | Acc: 49.723% (3596/7232)\n",
            "Loss: 1.385 | Acc: 49.740% (3629/7296)\n",
            "Loss: 1.384 | Acc: 49.742% (3661/7360)\n",
            "Loss: 1.387 | Acc: 49.609% (3683/7424)\n",
            "Loss: 1.386 | Acc: 49.599% (3714/7488)\n",
            "Loss: 1.388 | Acc: 49.576% (3744/7552)\n",
            "Loss: 1.387 | Acc: 49.593% (3777/7616)\n",
            "Loss: 1.385 | Acc: 49.701% (3817/7680)\n",
            "Loss: 1.385 | Acc: 49.638% (3844/7744)\n",
            "Loss: 1.385 | Acc: 49.654% (3877/7808)\n",
            "Loss: 1.384 | Acc: 49.708% (3913/7872)\n",
            "Loss: 1.384 | Acc: 49.698% (3944/7936)\n",
            "Loss: 1.384 | Acc: 49.737% (3979/8000)\n",
            "Loss: 1.382 | Acc: 49.727% (4010/8064)\n",
            "Loss: 1.383 | Acc: 49.717% (4041/8128)\n",
            "Loss: 1.383 | Acc: 49.756% (4076/8192)\n",
            "Loss: 1.384 | Acc: 49.746% (4107/8256)\n",
            "Loss: 1.383 | Acc: 49.772% (4141/8320)\n",
            "Loss: 1.382 | Acc: 49.761% (4172/8384)\n",
            "Loss: 1.379 | Acc: 49.893% (4215/8448)\n",
            "Loss: 1.378 | Acc: 49.965% (4253/8512)\n",
            "Loss: 1.377 | Acc: 49.930% (4282/8576)\n",
            "Loss: 1.377 | Acc: 49.919% (4313/8640)\n",
            "Loss: 1.377 | Acc: 49.897% (4343/8704)\n",
            "Loss: 1.376 | Acc: 49.943% (4379/8768)\n",
            "Loss: 1.375 | Acc: 49.943% (4411/8832)\n",
            "Loss: 1.374 | Acc: 50.022% (4450/8896)\n",
            "Loss: 1.374 | Acc: 49.989% (4479/8960)\n",
            "Loss: 1.375 | Acc: 49.978% (4510/9024)\n",
            "Loss: 1.376 | Acc: 49.956% (4540/9088)\n",
            "Loss: 1.376 | Acc: 49.924% (4569/9152)\n",
            "Loss: 1.377 | Acc: 49.913% (4600/9216)\n",
            "Loss: 1.378 | Acc: 49.860% (4627/9280)\n",
            "Loss: 1.379 | Acc: 49.839% (4657/9344)\n",
            "Loss: 1.379 | Acc: 49.862% (4691/9408)\n",
            "Loss: 1.379 | Acc: 49.852% (4722/9472)\n",
            "Loss: 1.380 | Acc: 49.832% (4752/9536)\n",
            "Loss: 1.379 | Acc: 49.875% (4788/9600)\n",
            "Loss: 1.379 | Acc: 49.886% (4821/9664)\n",
            "Loss: 1.378 | Acc: 49.897% (4854/9728)\n",
            "Loss: 1.378 | Acc: 49.877% (4884/9792)\n",
            "Loss: 1.379 | Acc: 49.899% (4918/9856)\n",
            "Loss: 1.380 | Acc: 49.849% (4945/9920)\n",
            "Loss: 1.381 | Acc: 49.810% (4973/9984)\n",
            "Loss: 1.379 | Acc: 49.881% (5012/10048)\n",
            "Loss: 1.381 | Acc: 49.832% (5039/10112)\n",
            "Loss: 1.380 | Acc: 49.843% (5072/10176)\n",
            "Loss: 1.380 | Acc: 49.854% (5105/10240)\n",
            "Loss: 1.379 | Acc: 49.874% (5139/10304)\n",
            "Loss: 1.380 | Acc: 49.884% (5172/10368)\n",
            "Loss: 1.380 | Acc: 49.837% (5199/10432)\n",
            "Loss: 1.382 | Acc: 49.781% (5225/10496)\n",
            "Loss: 1.382 | Acc: 49.763% (5255/10560)\n",
            "Loss: 1.383 | Acc: 49.727% (5283/10624)\n",
            "Loss: 1.384 | Acc: 49.654% (5307/10688)\n",
            "Loss: 1.386 | Acc: 49.591% (5332/10752)\n",
            "Loss: 1.386 | Acc: 49.612% (5366/10816)\n",
            "Loss: 1.386 | Acc: 49.614% (5398/10880)\n",
            "Loss: 1.384 | Acc: 49.698% (5439/10944)\n",
            "Loss: 1.384 | Acc: 49.746% (5476/11008)\n",
            "Loss: 1.382 | Acc: 49.783% (5512/11072)\n",
            "Loss: 1.382 | Acc: 49.784% (5544/11136)\n",
            "Loss: 1.382 | Acc: 49.768% (5574/11200)\n",
            "Loss: 1.382 | Acc: 49.796% (5609/11264)\n",
            "Loss: 1.383 | Acc: 49.806% (5642/11328)\n",
            "Loss: 1.381 | Acc: 49.903% (5685/11392)\n",
            "Loss: 1.380 | Acc: 49.930% (5720/11456)\n",
            "Loss: 1.382 | Acc: 49.896% (5748/11520)\n",
            "Loss: 1.381 | Acc: 49.871% (5777/11584)\n",
            "Loss: 1.381 | Acc: 49.871% (5809/11648)\n",
            "Loss: 1.381 | Acc: 49.846% (5838/11712)\n",
            "Loss: 1.380 | Acc: 49.898% (5876/11776)\n",
            "Loss: 1.380 | Acc: 49.890% (5907/11840)\n",
            "Loss: 1.380 | Acc: 49.882% (5938/11904)\n",
            "Loss: 1.380 | Acc: 49.925% (5975/11968)\n",
            "Loss: 1.381 | Acc: 49.875% (6001/12032)\n",
            "Loss: 1.381 | Acc: 49.876% (6033/12096)\n",
            "Loss: 1.381 | Acc: 49.885% (6066/12160)\n",
            "Loss: 1.380 | Acc: 49.869% (6096/12224)\n",
            "Loss: 1.381 | Acc: 49.862% (6127/12288)\n",
            "Loss: 1.381 | Acc: 49.887% (6162/12352)\n",
            "Loss: 1.379 | Acc: 49.911% (6197/12416)\n",
            "Loss: 1.379 | Acc: 49.952% (6234/12480)\n",
            "Loss: 1.379 | Acc: 49.928% (6263/12544)\n",
            "Loss: 1.379 | Acc: 49.929% (6295/12608)\n",
            "Loss: 1.381 | Acc: 49.882% (6321/12672)\n",
            "Loss: 1.381 | Acc: 49.874% (6352/12736)\n",
            "Loss: 1.381 | Acc: 49.875% (6384/12800)\n",
            "Loss: 1.382 | Acc: 49.883% (6417/12864)\n",
            "Loss: 1.382 | Acc: 49.899% (6451/12928)\n",
            "Loss: 1.382 | Acc: 49.885% (6481/12992)\n",
            "Loss: 1.382 | Acc: 49.885% (6513/13056)\n",
            "Loss: 1.381 | Acc: 49.893% (6546/13120)\n",
            "Loss: 1.380 | Acc: 49.924% (6582/13184)\n",
            "Loss: 1.381 | Acc: 49.932% (6615/13248)\n",
            "Loss: 1.381 | Acc: 49.925% (6646/13312)\n",
            "Loss: 1.382 | Acc: 49.888% (6673/13376)\n",
            "Loss: 1.383 | Acc: 49.874% (6703/13440)\n",
            "Loss: 1.382 | Acc: 49.889% (6737/13504)\n",
            "Loss: 1.382 | Acc: 49.897% (6770/13568)\n",
            "Loss: 1.382 | Acc: 49.919% (6805/13632)\n",
            "Loss: 1.383 | Acc: 49.876% (6831/13696)\n",
            "Loss: 1.383 | Acc: 49.862% (6861/13760)\n",
            "Loss: 1.384 | Acc: 49.805% (6885/13824)\n",
            "Loss: 1.384 | Acc: 49.770% (6912/13888)\n",
            "Loss: 1.383 | Acc: 49.835% (6953/13952)\n",
            "Loss: 1.383 | Acc: 49.864% (6989/14016)\n",
            "Loss: 1.382 | Acc: 49.879% (7023/14080)\n",
            "Loss: 1.382 | Acc: 49.873% (7054/14144)\n",
            "Loss: 1.382 | Acc: 49.916% (7092/14208)\n",
            "Loss: 1.382 | Acc: 49.923% (7125/14272)\n",
            "Loss: 1.382 | Acc: 49.951% (7161/14336)\n",
            "Loss: 1.383 | Acc: 49.917% (7188/14400)\n",
            "Loss: 1.383 | Acc: 49.903% (7218/14464)\n",
            "Loss: 1.382 | Acc: 49.966% (7259/14528)\n",
            "Loss: 1.383 | Acc: 49.952% (7289/14592)\n",
            "Loss: 1.382 | Acc: 49.959% (7322/14656)\n",
            "Loss: 1.383 | Acc: 49.946% (7352/14720)\n",
            "Loss: 1.383 | Acc: 49.946% (7384/14784)\n",
            "Loss: 1.383 | Acc: 49.939% (7415/14848)\n",
            "Loss: 1.382 | Acc: 49.973% (7452/14912)\n",
            "Loss: 1.382 | Acc: 50.000% (7488/14976)\n",
            "Loss: 1.381 | Acc: 49.987% (7518/15040)\n",
            "Loss: 1.381 | Acc: 50.007% (7553/15104)\n",
            "Loss: 1.383 | Acc: 49.941% (7575/15168)\n",
            "Loss: 1.383 | Acc: 49.928% (7605/15232)\n",
            "Loss: 1.383 | Acc: 49.895% (7632/15296)\n",
            "Loss: 1.383 | Acc: 49.876% (7661/15360)\n",
            "Loss: 1.382 | Acc: 49.903% (7697/15424)\n",
            "Loss: 1.383 | Acc: 49.910% (7730/15488)\n",
            "Loss: 1.384 | Acc: 49.891% (7759/15552)\n",
            "Loss: 1.385 | Acc: 49.846% (7784/15616)\n",
            "Loss: 1.384 | Acc: 49.853% (7817/15680)\n",
            "Loss: 1.384 | Acc: 49.848% (7848/15744)\n",
            "Loss: 1.384 | Acc: 49.873% (7884/15808)\n",
            "Loss: 1.383 | Acc: 49.880% (7917/15872)\n",
            "Loss: 1.384 | Acc: 49.868% (7947/15936)\n",
            "Loss: 1.384 | Acc: 49.881% (7981/16000)\n",
            "Loss: 1.383 | Acc: 49.882% (8013/16064)\n",
            "Loss: 1.384 | Acc: 49.876% (8044/16128)\n",
            "Loss: 1.384 | Acc: 49.852% (8072/16192)\n",
            "Loss: 1.384 | Acc: 49.883% (8109/16256)\n",
            "Loss: 1.384 | Acc: 49.871% (8139/16320)\n",
            "Loss: 1.384 | Acc: 49.866% (8170/16384)\n",
            "Loss: 1.386 | Acc: 49.812% (8193/16448)\n",
            "Loss: 1.387 | Acc: 49.764% (8217/16512)\n",
            "Loss: 1.387 | Acc: 49.735% (8244/16576)\n",
            "Loss: 1.387 | Acc: 49.784% (8284/16640)\n",
            "Loss: 1.387 | Acc: 49.773% (8314/16704)\n",
            "Loss: 1.389 | Acc: 49.744% (8341/16768)\n",
            "Loss: 1.389 | Acc: 49.745% (8373/16832)\n",
            "Loss: 1.389 | Acc: 49.751% (8406/16896)\n",
            "Loss: 1.390 | Acc: 49.735% (8435/16960)\n",
            "Loss: 1.389 | Acc: 49.730% (8466/17024)\n",
            "Loss: 1.389 | Acc: 49.707% (8494/17088)\n",
            "Loss: 1.390 | Acc: 49.674% (8520/17152)\n",
            "Loss: 1.391 | Acc: 49.657% (8549/17216)\n",
            "Loss: 1.391 | Acc: 49.630% (8576/17280)\n",
            "Loss: 1.392 | Acc: 49.596% (8602/17344)\n",
            "Loss: 1.392 | Acc: 49.575% (8630/17408)\n",
            "Loss: 1.393 | Acc: 49.576% (8662/17472)\n",
            "Loss: 1.393 | Acc: 49.544% (8688/17536)\n",
            "Loss: 1.393 | Acc: 49.551% (8721/17600)\n",
            "Loss: 1.393 | Acc: 49.519% (8747/17664)\n",
            "Loss: 1.393 | Acc: 49.521% (8779/17728)\n",
            "Loss: 1.393 | Acc: 49.533% (8813/17792)\n",
            "Loss: 1.394 | Acc: 49.513% (8841/17856)\n",
            "Loss: 1.394 | Acc: 49.492% (8869/17920)\n",
            "Loss: 1.393 | Acc: 49.511% (8904/17984)\n",
            "Loss: 1.393 | Acc: 49.507% (8935/18048)\n",
            "Loss: 1.393 | Acc: 49.520% (8969/18112)\n",
            "Loss: 1.392 | Acc: 49.532% (9003/18176)\n",
            "Loss: 1.392 | Acc: 49.534% (9035/18240)\n",
            "Loss: 1.392 | Acc: 49.536% (9067/18304)\n",
            "Loss: 1.391 | Acc: 49.532% (9098/18368)\n",
            "Loss: 1.391 | Acc: 49.539% (9131/18432)\n",
            "Loss: 1.390 | Acc: 49.546% (9164/18496)\n",
            "Loss: 1.390 | Acc: 49.558% (9198/18560)\n",
            "Loss: 1.389 | Acc: 49.597% (9237/18624)\n",
            "Loss: 1.389 | Acc: 49.593% (9268/18688)\n",
            "Loss: 1.389 | Acc: 49.600% (9301/18752)\n",
            "Loss: 1.388 | Acc: 49.601% (9333/18816)\n",
            "Loss: 1.388 | Acc: 49.619% (9368/18880)\n",
            "Loss: 1.388 | Acc: 49.609% (9398/18944)\n",
            "Loss: 1.388 | Acc: 49.648% (9437/19008)\n",
            "Loss: 1.388 | Acc: 49.633% (9466/19072)\n",
            "Loss: 1.388 | Acc: 49.666% (9504/19136)\n",
            "Loss: 1.388 | Acc: 49.682% (9539/19200)\n",
            "Loss: 1.388 | Acc: 49.642% (9563/19264)\n",
            "Loss: 1.389 | Acc: 49.596% (9586/19328)\n",
            "Loss: 1.388 | Acc: 49.608% (9620/19392)\n",
            "Loss: 1.389 | Acc: 49.584% (9647/19456)\n",
            "Loss: 1.389 | Acc: 49.580% (9678/19520)\n",
            "Loss: 1.389 | Acc: 49.581% (9710/19584)\n",
            "Loss: 1.390 | Acc: 49.547% (9735/19648)\n",
            "Loss: 1.390 | Acc: 49.543% (9766/19712)\n",
            "Loss: 1.390 | Acc: 49.520% (9793/19776)\n",
            "Loss: 1.390 | Acc: 49.526% (9826/19840)\n",
            "Loss: 1.390 | Acc: 49.518% (9856/19904)\n",
            "Loss: 1.391 | Acc: 49.494% (9883/19968)\n",
            "Loss: 1.391 | Acc: 49.471% (9910/20032)\n",
            "Loss: 1.391 | Acc: 49.468% (9941/20096)\n",
            "Loss: 1.391 | Acc: 49.459% (9971/20160)\n",
            "Loss: 1.392 | Acc: 49.441% (9999/20224)\n",
            "Loss: 1.392 | Acc: 49.423% (10027/20288)\n",
            "Loss: 1.392 | Acc: 49.396% (10053/20352)\n",
            "Loss: 1.392 | Acc: 49.407% (10087/20416)\n",
            "Loss: 1.392 | Acc: 49.414% (10120/20480)\n",
            "Loss: 1.392 | Acc: 49.426% (10154/20544)\n",
            "Loss: 1.391 | Acc: 49.447% (10190/20608)\n",
            "Loss: 1.392 | Acc: 49.420% (10216/20672)\n",
            "Loss: 1.392 | Acc: 49.426% (10249/20736)\n",
            "Loss: 1.392 | Acc: 49.423% (10280/20800)\n",
            "Loss: 1.392 | Acc: 49.434% (10314/20864)\n",
            "Loss: 1.392 | Acc: 49.431% (10345/20928)\n",
            "Loss: 1.393 | Acc: 49.414% (10373/20992)\n",
            "Loss: 1.393 | Acc: 49.397% (10401/21056)\n",
            "Loss: 1.393 | Acc: 49.399% (10433/21120)\n",
            "Loss: 1.394 | Acc: 49.391% (10463/21184)\n",
            "Loss: 1.394 | Acc: 49.379% (10492/21248)\n",
            "Loss: 1.393 | Acc: 49.381% (10524/21312)\n",
            "Loss: 1.393 | Acc: 49.406% (10561/21376)\n",
            "Loss: 1.392 | Acc: 49.426% (10597/21440)\n",
            "Loss: 1.393 | Acc: 49.409% (10625/21504)\n",
            "Loss: 1.392 | Acc: 49.434% (10662/21568)\n",
            "Loss: 1.393 | Acc: 49.399% (10686/21632)\n",
            "Loss: 1.393 | Acc: 49.378% (10713/21696)\n",
            "Loss: 1.393 | Acc: 49.370% (10743/21760)\n",
            "Loss: 1.393 | Acc: 49.386% (10778/21824)\n",
            "Loss: 1.393 | Acc: 49.397% (10812/21888)\n",
            "Loss: 1.393 | Acc: 49.408% (10846/21952)\n",
            "Loss: 1.392 | Acc: 49.410% (10878/22016)\n",
            "Loss: 1.392 | Acc: 49.389% (10905/22080)\n",
            "Loss: 1.392 | Acc: 49.413% (10942/22144)\n",
            "Loss: 1.392 | Acc: 49.410% (10973/22208)\n",
            "Loss: 1.393 | Acc: 49.403% (11003/22272)\n",
            "Loss: 1.393 | Acc: 49.409% (11036/22336)\n",
            "Loss: 1.392 | Acc: 49.429% (11072/22400)\n",
            "Loss: 1.391 | Acc: 49.448% (11108/22464)\n",
            "Loss: 1.391 | Acc: 49.490% (11149/22528)\n",
            "Loss: 1.391 | Acc: 49.495% (11182/22592)\n",
            "Loss: 1.391 | Acc: 49.484% (11211/22656)\n",
            "Loss: 1.390 | Acc: 49.503% (11247/22720)\n",
            "Loss: 1.390 | Acc: 49.522% (11283/22784)\n",
            "Loss: 1.390 | Acc: 49.527% (11316/22848)\n",
            "Loss: 1.390 | Acc: 49.524% (11347/22912)\n",
            "Loss: 1.389 | Acc: 49.530% (11380/22976)\n",
            "Loss: 1.390 | Acc: 49.523% (11410/23040)\n",
            "Loss: 1.389 | Acc: 49.533% (11444/23104)\n",
            "Loss: 1.390 | Acc: 49.499% (11468/23168)\n",
            "Loss: 1.390 | Acc: 49.514% (11503/23232)\n",
            "Loss: 1.390 | Acc: 49.506% (11533/23296)\n",
            "Loss: 1.390 | Acc: 49.516% (11567/23360)\n",
            "Loss: 1.391 | Acc: 49.492% (11593/23424)\n",
            "Loss: 1.391 | Acc: 49.498% (11626/23488)\n",
            "Loss: 1.391 | Acc: 49.486% (11655/23552)\n",
            "Loss: 1.391 | Acc: 49.475% (11684/23616)\n",
            "Loss: 1.391 | Acc: 49.476% (11716/23680)\n",
            "Loss: 1.391 | Acc: 49.478% (11748/23744)\n",
            "Loss: 1.391 | Acc: 49.500% (11785/23808)\n",
            "Loss: 1.391 | Acc: 49.481% (11812/23872)\n",
            "Loss: 1.391 | Acc: 49.494% (11847/23936)\n",
            "Loss: 1.391 | Acc: 49.517% (11884/24000)\n",
            "Loss: 1.391 | Acc: 49.518% (11916/24064)\n",
            "Loss: 1.391 | Acc: 49.523% (11949/24128)\n",
            "Loss: 1.391 | Acc: 49.521% (11980/24192)\n",
            "Loss: 1.391 | Acc: 49.530% (12014/24256)\n",
            "Loss: 1.391 | Acc: 49.507% (12040/24320)\n",
            "Loss: 1.390 | Acc: 49.528% (12077/24384)\n",
            "Loss: 1.390 | Acc: 49.534% (12110/24448)\n",
            "Loss: 1.390 | Acc: 49.555% (12147/24512)\n",
            "Loss: 1.390 | Acc: 49.593% (12188/24576)\n",
            "Loss: 1.390 | Acc: 49.606% (12223/24640)\n",
            "Loss: 1.389 | Acc: 49.591% (12251/24704)\n",
            "Loss: 1.390 | Acc: 49.568% (12277/24768)\n",
            "Loss: 1.389 | Acc: 49.589% (12314/24832)\n",
            "Loss: 1.390 | Acc: 49.578% (12343/24896)\n",
            "Loss: 1.390 | Acc: 49.559% (12370/24960)\n",
            "Loss: 1.390 | Acc: 49.556% (12401/25024)\n",
            "Loss: 1.390 | Acc: 49.558% (12433/25088)\n",
            "Loss: 1.390 | Acc: 49.563% (12466/25152)\n",
            "Loss: 1.390 | Acc: 49.552% (12495/25216)\n",
            "Loss: 1.390 | Acc: 49.537% (12523/25280)\n",
            "Loss: 1.389 | Acc: 49.582% (12566/25344)\n",
            "Loss: 1.389 | Acc: 49.610% (12605/25408)\n",
            "Loss: 1.388 | Acc: 49.635% (12643/25472)\n",
            "Loss: 1.389 | Acc: 49.632% (12674/25536)\n",
            "Loss: 1.388 | Acc: 49.652% (12711/25600)\n",
            "Loss: 1.388 | Acc: 49.630% (12737/25664)\n",
            "Loss: 1.388 | Acc: 49.650% (12774/25728)\n",
            "Loss: 1.387 | Acc: 49.663% (12809/25792)\n",
            "Loss: 1.388 | Acc: 49.664% (12841/25856)\n",
            "Loss: 1.388 | Acc: 49.660% (12872/25920)\n",
            "Loss: 1.387 | Acc: 49.700% (12914/25984)\n",
            "Loss: 1.387 | Acc: 49.678% (12940/26048)\n",
            "Loss: 1.387 | Acc: 49.705% (12979/26112)\n",
            "Loss: 1.387 | Acc: 49.710% (13012/26176)\n",
            "Loss: 1.386 | Acc: 49.707% (13043/26240)\n",
            "Loss: 1.386 | Acc: 49.692% (13071/26304)\n",
            "Loss: 1.386 | Acc: 49.712% (13108/26368)\n",
            "Loss: 1.386 | Acc: 49.716% (13141/26432)\n",
            "Loss: 1.386 | Acc: 49.732% (13177/26496)\n",
            "Loss: 1.386 | Acc: 49.744% (13212/26560)\n",
            "Loss: 1.386 | Acc: 49.726% (13239/26624)\n",
            "Loss: 1.387 | Acc: 49.708% (13266/26688)\n",
            "Loss: 1.387 | Acc: 49.697% (13295/26752)\n",
            "Loss: 1.386 | Acc: 49.705% (13329/26816)\n",
            "Loss: 1.386 | Acc: 49.710% (13362/26880)\n",
            "Loss: 1.386 | Acc: 49.703% (13392/26944)\n",
            "Loss: 1.386 | Acc: 49.707% (13425/27008)\n",
            "Loss: 1.386 | Acc: 49.701% (13455/27072)\n",
            "Loss: 1.386 | Acc: 49.679% (13481/27136)\n",
            "Loss: 1.386 | Acc: 49.662% (13508/27200)\n",
            "Loss: 1.386 | Acc: 49.666% (13541/27264)\n",
            "Loss: 1.386 | Acc: 49.678% (13576/27328)\n",
            "Loss: 1.386 | Acc: 49.682% (13609/27392)\n",
            "Loss: 1.385 | Acc: 49.687% (13642/27456)\n",
            "Loss: 1.385 | Acc: 49.669% (13669/27520)\n",
            "Loss: 1.385 | Acc: 49.670% (13701/27584)\n",
            "Loss: 1.385 | Acc: 49.689% (13738/27648)\n",
            "Loss: 1.385 | Acc: 49.697% (13772/27712)\n",
            "Loss: 1.385 | Acc: 49.687% (13801/27776)\n",
            "Loss: 1.385 | Acc: 49.691% (13834/27840)\n",
            "Loss: 1.385 | Acc: 49.688% (13865/27904)\n",
            "Loss: 1.385 | Acc: 49.703% (13901/27968)\n",
            "Loss: 1.384 | Acc: 49.722% (13938/28032)\n",
            "Loss: 1.384 | Acc: 49.729% (13972/28096)\n",
            "Loss: 1.384 | Acc: 49.751% (14010/28160)\n",
            "Loss: 1.384 | Acc: 49.741% (14039/28224)\n",
            "Loss: 1.384 | Acc: 49.760% (14076/28288)\n",
            "Loss: 1.383 | Acc: 49.767% (14110/28352)\n",
            "Loss: 1.383 | Acc: 49.743% (14135/28416)\n",
            "Loss: 1.383 | Acc: 49.758% (14171/28480)\n",
            "Loss: 1.383 | Acc: 49.783% (14210/28544)\n",
            "Loss: 1.382 | Acc: 49.783% (14242/28608)\n",
            "Loss: 1.382 | Acc: 49.777% (14272/28672)\n",
            "Loss: 1.382 | Acc: 49.795% (14309/28736)\n",
            "Loss: 1.382 | Acc: 49.788% (14339/28800)\n",
            "Loss: 1.382 | Acc: 49.789% (14371/28864)\n",
            "Loss: 1.382 | Acc: 49.810% (14409/28928)\n",
            "Loss: 1.382 | Acc: 49.810% (14441/28992)\n",
            "Loss: 1.382 | Acc: 49.794% (14468/29056)\n",
            "Loss: 1.382 | Acc: 49.825% (14509/29120)\n",
            "Loss: 1.382 | Acc: 49.815% (14538/29184)\n",
            "Loss: 1.382 | Acc: 49.809% (14568/29248)\n",
            "Loss: 1.382 | Acc: 49.812% (14601/29312)\n",
            "Loss: 1.382 | Acc: 49.820% (14635/29376)\n",
            "Loss: 1.382 | Acc: 49.806% (14663/29440)\n",
            "Loss: 1.382 | Acc: 49.834% (14703/29504)\n",
            "Loss: 1.382 | Acc: 49.838% (14736/29568)\n",
            "Loss: 1.382 | Acc: 49.845% (14770/29632)\n",
            "Loss: 1.382 | Acc: 49.855% (14805/29696)\n",
            "Loss: 1.382 | Acc: 49.862% (14839/29760)\n",
            "Loss: 1.382 | Acc: 49.863% (14871/29824)\n",
            "Loss: 1.382 | Acc: 49.866% (14904/29888)\n",
            "Loss: 1.381 | Acc: 49.880% (14940/29952)\n",
            "Loss: 1.381 | Acc: 49.870% (14969/30016)\n",
            "Loss: 1.381 | Acc: 49.880% (15004/30080)\n",
            "Loss: 1.381 | Acc: 49.874% (15034/30144)\n",
            "Loss: 1.382 | Acc: 49.858% (15061/30208)\n",
            "Loss: 1.382 | Acc: 49.845% (15089/30272)\n",
            "Loss: 1.382 | Acc: 49.848% (15122/30336)\n",
            "Loss: 1.382 | Acc: 49.836% (15150/30400)\n",
            "Loss: 1.383 | Acc: 49.823% (15178/30464)\n",
            "Loss: 1.383 | Acc: 49.830% (15212/30528)\n",
            "Loss: 1.382 | Acc: 49.850% (15250/30592)\n",
            "Loss: 1.382 | Acc: 49.876% (15290/30656)\n",
            "Loss: 1.382 | Acc: 49.886% (15325/30720)\n",
            "Loss: 1.382 | Acc: 49.899% (15361/30784)\n",
            "Loss: 1.382 | Acc: 49.890% (15390/30848)\n",
            "Loss: 1.382 | Acc: 49.890% (15422/30912)\n",
            "Loss: 1.381 | Acc: 49.890% (15454/30976)\n",
            "Loss: 1.382 | Acc: 49.881% (15483/31040)\n",
            "Loss: 1.381 | Acc: 49.884% (15516/31104)\n",
            "Loss: 1.382 | Acc: 49.868% (15543/31168)\n",
            "Loss: 1.382 | Acc: 49.888% (15581/31232)\n",
            "Loss: 1.382 | Acc: 49.872% (15608/31296)\n",
            "Loss: 1.382 | Acc: 49.879% (15642/31360)\n",
            "Loss: 1.382 | Acc: 49.847% (15664/31424)\n",
            "Loss: 1.382 | Acc: 49.832% (15691/31488)\n",
            "Loss: 1.383 | Acc: 49.838% (15725/31552)\n",
            "Loss: 1.383 | Acc: 49.836% (15756/31616)\n",
            "Loss: 1.382 | Acc: 49.842% (15790/31680)\n",
            "Loss: 1.382 | Acc: 49.833% (15819/31744)\n",
            "Loss: 1.382 | Acc: 49.811% (15844/31808)\n",
            "Loss: 1.382 | Acc: 49.796% (15871/31872)\n",
            "Loss: 1.382 | Acc: 49.787% (15900/31936)\n",
            "Loss: 1.382 | Acc: 49.781% (15930/32000)\n",
            "Loss: 1.382 | Acc: 49.791% (15965/32064)\n",
            "Loss: 1.382 | Acc: 49.795% (15998/32128)\n",
            "Loss: 1.382 | Acc: 49.795% (16030/32192)\n",
            "Loss: 1.382 | Acc: 49.798% (16063/32256)\n",
            "Loss: 1.382 | Acc: 49.808% (16098/32320)\n",
            "Loss: 1.382 | Acc: 49.827% (16136/32384)\n",
            "Loss: 1.382 | Acc: 49.843% (16173/32448)\n",
            "Loss: 1.382 | Acc: 49.840% (16204/32512)\n",
            "Loss: 1.382 | Acc: 49.843% (16237/32576)\n",
            "Loss: 1.381 | Acc: 49.856% (16273/32640)\n",
            "Loss: 1.382 | Acc: 49.841% (16300/32704)\n",
            "Loss: 1.382 | Acc: 49.844% (16333/32768)\n",
            "Loss: 1.382 | Acc: 49.845% (16365/32832)\n",
            "Loss: 1.382 | Acc: 49.842% (16396/32896)\n",
            "Loss: 1.382 | Acc: 49.863% (16435/32960)\n",
            "Loss: 1.382 | Acc: 49.843% (16460/33024)\n",
            "Loss: 1.382 | Acc: 49.849% (16494/33088)\n",
            "Loss: 1.381 | Acc: 49.873% (16534/33152)\n",
            "Loss: 1.381 | Acc: 49.865% (16563/33216)\n",
            "Loss: 1.381 | Acc: 49.889% (16603/33280)\n",
            "Loss: 1.381 | Acc: 49.868% (16628/33344)\n",
            "Loss: 1.381 | Acc: 49.880% (16664/33408)\n",
            "Loss: 1.381 | Acc: 49.898% (16702/33472)\n",
            "Loss: 1.381 | Acc: 49.905% (16736/33536)\n",
            "Loss: 1.381 | Acc: 49.896% (16765/33600)\n",
            "Loss: 1.380 | Acc: 49.890% (16795/33664)\n",
            "Loss: 1.381 | Acc: 49.867% (16819/33728)\n",
            "Loss: 1.381 | Acc: 49.870% (16852/33792)\n",
            "Loss: 1.381 | Acc: 49.867% (16883/33856)\n",
            "Loss: 1.381 | Acc: 49.856% (16911/33920)\n",
            "Loss: 1.380 | Acc: 49.876% (16950/33984)\n",
            "Loss: 1.381 | Acc: 49.844% (16971/34048)\n",
            "Loss: 1.382 | Acc: 49.824% (16996/34112)\n",
            "Loss: 1.382 | Acc: 49.824% (17028/34176)\n",
            "Loss: 1.382 | Acc: 49.828% (17061/34240)\n",
            "Loss: 1.381 | Acc: 49.825% (17092/34304)\n",
            "Loss: 1.382 | Acc: 49.817% (17121/34368)\n",
            "Loss: 1.382 | Acc: 49.832% (17158/34432)\n",
            "Loss: 1.382 | Acc: 49.829% (17189/34496)\n",
            "Loss: 1.381 | Acc: 49.835% (17223/34560)\n",
            "Loss: 1.381 | Acc: 49.832% (17254/34624)\n",
            "Loss: 1.382 | Acc: 49.821% (17282/34688)\n",
            "Loss: 1.382 | Acc: 49.830% (17317/34752)\n",
            "Loss: 1.382 | Acc: 49.822% (17346/34816)\n",
            "Loss: 1.382 | Acc: 49.842% (17385/34880)\n",
            "Loss: 1.382 | Acc: 49.843% (17417/34944)\n",
            "Loss: 1.382 | Acc: 49.837% (17447/35008)\n",
            "Loss: 1.382 | Acc: 49.869% (17490/35072)\n",
            "Loss: 1.382 | Acc: 49.875% (17524/35136)\n",
            "Loss: 1.382 | Acc: 49.875% (17556/35200)\n",
            "Loss: 1.382 | Acc: 49.895% (17595/35264)\n",
            "Loss: 1.382 | Acc: 49.890% (17625/35328)\n",
            "Loss: 1.382 | Acc: 49.884% (17655/35392)\n",
            "Loss: 1.382 | Acc: 49.876% (17684/35456)\n",
            "Loss: 1.382 | Acc: 49.876% (17716/35520)\n",
            "Loss: 1.382 | Acc: 49.859% (17742/35584)\n",
            "Loss: 1.382 | Acc: 49.863% (17775/35648)\n",
            "Loss: 1.382 | Acc: 49.857% (17805/35712)\n",
            "Loss: 1.381 | Acc: 49.874% (17843/35776)\n",
            "Loss: 1.382 | Acc: 49.863% (17871/35840)\n",
            "Loss: 1.381 | Acc: 49.883% (17910/35904)\n",
            "Loss: 1.381 | Acc: 49.883% (17942/35968)\n",
            "Loss: 1.381 | Acc: 49.878% (17972/36032)\n",
            "Loss: 1.381 | Acc: 49.864% (17999/36096)\n",
            "Loss: 1.382 | Acc: 49.870% (18033/36160)\n",
            "Loss: 1.381 | Acc: 49.898% (18075/36224)\n",
            "Loss: 1.381 | Acc: 49.876% (18099/36288)\n",
            "Loss: 1.381 | Acc: 49.860% (18125/36352)\n",
            "Loss: 1.381 | Acc: 49.868% (18160/36416)\n",
            "Loss: 1.381 | Acc: 49.860% (18189/36480)\n",
            "Loss: 1.381 | Acc: 49.863% (18222/36544)\n",
            "Loss: 1.381 | Acc: 49.863% (18254/36608)\n",
            "Loss: 1.381 | Acc: 49.866% (18287/36672)\n",
            "Loss: 1.381 | Acc: 49.861% (18317/36736)\n",
            "Loss: 1.381 | Acc: 49.856% (18347/36800)\n",
            "Loss: 1.381 | Acc: 49.848% (18376/36864)\n",
            "Loss: 1.381 | Acc: 49.854% (18410/36928)\n",
            "Loss: 1.381 | Acc: 49.859% (18444/36992)\n",
            "Loss: 1.380 | Acc: 49.862% (18477/37056)\n",
            "Loss: 1.380 | Acc: 49.873% (18513/37120)\n",
            "Loss: 1.380 | Acc: 49.884% (18549/37184)\n",
            "Loss: 1.380 | Acc: 49.890% (18583/37248)\n",
            "Loss: 1.380 | Acc: 49.893% (18616/37312)\n",
            "Loss: 1.380 | Acc: 49.888% (18646/37376)\n",
            "Loss: 1.380 | Acc: 49.880% (18675/37440)\n",
            "Loss: 1.380 | Acc: 49.880% (18707/37504)\n",
            "Loss: 1.379 | Acc: 49.904% (18748/37568)\n",
            "Loss: 1.379 | Acc: 49.918% (18785/37632)\n",
            "Loss: 1.378 | Acc: 49.926% (18820/37696)\n",
            "Loss: 1.378 | Acc: 49.915% (18848/37760)\n",
            "Loss: 1.378 | Acc: 49.913% (18879/37824)\n",
            "Loss: 1.378 | Acc: 49.900% (18906/37888)\n",
            "Loss: 1.378 | Acc: 49.889% (18934/37952)\n",
            "Loss: 1.378 | Acc: 49.895% (18968/38016)\n",
            "Loss: 1.378 | Acc: 49.900% (19002/38080)\n",
            "Loss: 1.378 | Acc: 49.906% (19036/38144)\n",
            "Loss: 1.378 | Acc: 49.916% (19072/38208)\n",
            "Loss: 1.378 | Acc: 49.922% (19106/38272)\n",
            "Loss: 1.378 | Acc: 49.922% (19138/38336)\n",
            "Loss: 1.378 | Acc: 49.911% (19166/38400)\n",
            "Loss: 1.378 | Acc: 49.925% (19203/38464)\n",
            "Loss: 1.378 | Acc: 49.933% (19238/38528)\n",
            "Loss: 1.378 | Acc: 49.933% (19270/38592)\n",
            "Loss: 1.378 | Acc: 49.935% (19303/38656)\n",
            "Loss: 1.378 | Acc: 49.928% (19332/38720)\n",
            "Loss: 1.378 | Acc: 49.933% (19366/38784)\n",
            "Loss: 1.378 | Acc: 49.936% (19399/38848)\n",
            "Loss: 1.377 | Acc: 49.938% (19432/38912)\n",
            "Loss: 1.378 | Acc: 49.936% (19463/38976)\n",
            "Loss: 1.378 | Acc: 49.931% (19493/39040)\n",
            "Loss: 1.377 | Acc: 49.939% (19528/39104)\n",
            "Loss: 1.378 | Acc: 49.936% (19559/39168)\n",
            "Loss: 1.377 | Acc: 49.936% (19591/39232)\n",
            "Loss: 1.377 | Acc: 49.959% (19632/39296)\n",
            "Loss: 1.377 | Acc: 49.954% (19662/39360)\n",
            "Loss: 1.377 | Acc: 49.949% (19692/39424)\n",
            "Loss: 1.377 | Acc: 49.937% (19719/39488)\n",
            "Loss: 1.377 | Acc: 49.949% (19756/39552)\n",
            "Loss: 1.377 | Acc: 49.955% (19790/39616)\n",
            "Loss: 1.377 | Acc: 49.955% (19822/39680)\n",
            "Loss: 1.377 | Acc: 49.957% (19855/39744)\n",
            "Loss: 1.376 | Acc: 49.965% (19890/39808)\n",
            "Loss: 1.376 | Acc: 49.977% (19927/39872)\n",
            "Loss: 1.375 | Acc: 49.997% (19967/39936)\n",
            "Loss: 1.375 | Acc: 50.002% (20001/40000)\n",
            "Loss: 1.375 | Acc: 50.010% (20036/40064)\n",
            "Loss: 1.376 | Acc: 50.007% (20067/40128)\n",
            "Loss: 1.376 | Acc: 50.017% (20103/40192)\n",
            "Loss: 1.376 | Acc: 50.010% (20132/40256)\n",
            "Loss: 1.376 | Acc: 50.012% (20165/40320)\n",
            "Loss: 1.375 | Acc: 50.032% (20205/40384)\n",
            "Loss: 1.375 | Acc: 50.040% (20240/40448)\n",
            "Loss: 1.375 | Acc: 50.039% (20272/40512)\n",
            "Loss: 1.375 | Acc: 50.042% (20305/40576)\n",
            "Loss: 1.375 | Acc: 50.039% (20336/40640)\n",
            "Loss: 1.374 | Acc: 50.057% (20375/40704)\n",
            "Loss: 1.375 | Acc: 50.044% (20402/40768)\n",
            "Loss: 1.375 | Acc: 50.042% (20433/40832)\n",
            "Loss: 1.375 | Acc: 50.037% (20463/40896)\n",
            "Loss: 1.375 | Acc: 50.024% (20490/40960)\n",
            "Loss: 1.375 | Acc: 50.029% (20524/41024)\n",
            "Loss: 1.375 | Acc: 50.022% (20553/41088)\n",
            "Loss: 1.376 | Acc: 50.024% (20586/41152)\n",
            "Loss: 1.376 | Acc: 50.027% (20619/41216)\n",
            "Loss: 1.376 | Acc: 50.010% (20644/41280)\n",
            "Loss: 1.376 | Acc: 49.995% (20670/41344)\n",
            "Loss: 1.376 | Acc: 49.993% (20701/41408)\n",
            "Loss: 1.376 | Acc: 49.983% (20729/41472)\n",
            "Loss: 1.377 | Acc: 49.983% (20761/41536)\n",
            "Loss: 1.377 | Acc: 49.976% (20790/41600)\n",
            "Loss: 1.377 | Acc: 49.954% (20813/41664)\n",
            "Loss: 1.377 | Acc: 49.952% (20844/41728)\n",
            "Loss: 1.377 | Acc: 49.957% (20878/41792)\n",
            "Loss: 1.378 | Acc: 49.947% (20906/41856)\n",
            "Loss: 1.378 | Acc: 49.955% (20941/41920)\n",
            "Loss: 1.378 | Acc: 49.964% (20977/41984)\n",
            "Loss: 1.378 | Acc: 49.952% (21004/42048)\n",
            "Loss: 1.378 | Acc: 49.948% (21034/42112)\n",
            "Loss: 1.378 | Acc: 49.948% (21066/42176)\n",
            "Loss: 1.378 | Acc: 49.962% (21104/42240)\n",
            "Loss: 1.377 | Acc: 49.965% (21137/42304)\n",
            "Loss: 1.377 | Acc: 49.958% (21166/42368)\n",
            "Loss: 1.377 | Acc: 49.962% (21200/42432)\n",
            "Loss: 1.377 | Acc: 49.960% (21231/42496)\n",
            "Loss: 1.377 | Acc: 49.965% (21265/42560)\n",
            "Loss: 1.377 | Acc: 49.970% (21299/42624)\n",
            "Loss: 1.377 | Acc: 49.970% (21331/42688)\n",
            "Loss: 1.377 | Acc: 49.984% (21369/42752)\n",
            "Loss: 1.377 | Acc: 49.977% (21398/42816)\n",
            "Loss: 1.377 | Acc: 49.972% (21428/42880)\n",
            "Loss: 1.377 | Acc: 49.972% (21460/42944)\n",
            "Loss: 1.377 | Acc: 49.972% (21492/43008)\n",
            "Loss: 1.377 | Acc: 49.981% (21528/43072)\n",
            "Loss: 1.376 | Acc: 50.002% (21569/43136)\n",
            "Loss: 1.376 | Acc: 50.021% (21609/43200)\n",
            "Loss: 1.376 | Acc: 50.025% (21643/43264)\n",
            "Loss: 1.376 | Acc: 50.023% (21674/43328)\n",
            "Loss: 1.376 | Acc: 50.021% (21705/43392)\n",
            "Loss: 1.376 | Acc: 50.018% (21736/43456)\n",
            "Loss: 1.376 | Acc: 50.048% (21781/43520)\n",
            "Loss: 1.375 | Acc: 50.064% (21820/43584)\n",
            "Loss: 1.375 | Acc: 50.066% (21853/43648)\n",
            "Loss: 1.375 | Acc: 50.066% (21885/43712)\n",
            "Loss: 1.376 | Acc: 50.057% (21913/43776)\n",
            "Loss: 1.376 | Acc: 50.052% (21943/43840)\n",
            "Loss: 1.376 | Acc: 50.034% (21967/43904)\n",
            "Loss: 1.376 | Acc: 50.050% (22006/43968)\n",
            "Loss: 1.375 | Acc: 50.064% (22044/44032)\n",
            "Loss: 1.375 | Acc: 50.073% (22080/44096)\n",
            "Loss: 1.375 | Acc: 50.061% (22107/44160)\n",
            "Loss: 1.376 | Acc: 50.047% (22133/44224)\n",
            "Loss: 1.376 | Acc: 50.038% (22161/44288)\n",
            "Loss: 1.376 | Acc: 50.029% (22189/44352)\n",
            "Loss: 1.376 | Acc: 50.025% (22219/44416)\n",
            "Loss: 1.376 | Acc: 50.027% (22252/44480)\n",
            "Loss: 1.376 | Acc: 50.029% (22285/44544)\n",
            "Loss: 1.376 | Acc: 50.043% (22323/44608)\n",
            "Loss: 1.376 | Acc: 50.051% (22359/44672)\n",
            "Loss: 1.376 | Acc: 50.045% (22388/44736)\n",
            "Loss: 1.376 | Acc: 50.047% (22421/44800)\n",
            "Loss: 1.376 | Acc: 50.036% (22448/44864)\n",
            "Loss: 1.376 | Acc: 50.058% (22490/44928)\n",
            "Loss: 1.376 | Acc: 50.044% (22516/44992)\n",
            "Loss: 1.376 | Acc: 50.044% (22548/45056)\n",
            "Loss: 1.375 | Acc: 50.049% (22582/45120)\n",
            "Loss: 1.375 | Acc: 50.051% (22615/45184)\n",
            "Loss: 1.375 | Acc: 50.051% (22647/45248)\n",
            "Loss: 1.375 | Acc: 50.040% (22674/45312)\n",
            "Loss: 1.376 | Acc: 50.026% (22700/45376)\n",
            "Loss: 1.376 | Acc: 50.018% (22728/45440)\n",
            "Loss: 1.376 | Acc: 50.031% (22766/45504)\n",
            "Loss: 1.375 | Acc: 50.042% (22803/45568)\n",
            "Loss: 1.375 | Acc: 50.044% (22836/45632)\n",
            "Loss: 1.375 | Acc: 50.048% (22870/45696)\n",
            "Loss: 1.375 | Acc: 50.052% (22904/45760)\n",
            "Loss: 1.375 | Acc: 50.055% (22937/45824)\n",
            "Loss: 1.375 | Acc: 50.078% (22980/45888)\n",
            "Loss: 1.375 | Acc: 50.076% (23011/45952)\n",
            "Loss: 1.375 | Acc: 50.065% (23038/46016)\n",
            "Loss: 1.375 | Acc: 50.065% (23070/46080)\n",
            "Loss: 1.375 | Acc: 50.074% (23106/46144)\n",
            "Loss: 1.375 | Acc: 50.069% (23136/46208)\n",
            "Loss: 1.375 | Acc: 50.054% (23161/46272)\n",
            "Loss: 1.376 | Acc: 50.037% (23185/46336)\n",
            "Loss: 1.376 | Acc: 50.041% (23219/46400)\n",
            "Loss: 1.376 | Acc: 50.039% (23250/46464)\n",
            "Loss: 1.376 | Acc: 50.034% (23280/46528)\n",
            "Loss: 1.376 | Acc: 50.030% (23310/46592)\n",
            "Loss: 1.376 | Acc: 50.017% (23336/46656)\n",
            "Loss: 1.376 | Acc: 50.009% (23364/46720)\n",
            "Loss: 1.376 | Acc: 50.013% (23398/46784)\n",
            "Loss: 1.376 | Acc: 50.019% (23433/46848)\n",
            "Loss: 1.376 | Acc: 50.013% (23462/46912)\n",
            "Loss: 1.376 | Acc: 50.019% (23497/46976)\n",
            "Loss: 1.376 | Acc: 50.036% (23537/47040)\n",
            "Loss: 1.376 | Acc: 50.047% (23574/47104)\n",
            "Loss: 1.376 | Acc: 50.047% (23606/47168)\n",
            "Loss: 1.376 | Acc: 50.042% (23636/47232)\n",
            "Loss: 1.376 | Acc: 50.034% (23664/47296)\n",
            "Loss: 1.376 | Acc: 50.044% (23701/47360)\n",
            "Loss: 1.376 | Acc: 50.046% (23734/47424)\n",
            "Loss: 1.376 | Acc: 50.055% (23770/47488)\n",
            "Loss: 1.376 | Acc: 50.053% (23801/47552)\n",
            "Loss: 1.376 | Acc: 50.046% (23830/47616)\n",
            "Loss: 1.376 | Acc: 50.057% (23867/47680)\n",
            "Loss: 1.376 | Acc: 50.057% (23899/47744)\n",
            "Loss: 1.376 | Acc: 50.067% (23936/47808)\n",
            "Loss: 1.376 | Acc: 50.075% (23972/47872)\n",
            "Loss: 1.375 | Acc: 50.077% (24005/47936)\n",
            "Loss: 1.375 | Acc: 50.069% (24033/48000)\n",
            "Loss: 1.375 | Acc: 50.077% (24069/48064)\n",
            "Loss: 1.375 | Acc: 50.077% (24101/48128)\n",
            "Loss: 1.375 | Acc: 50.081% (24135/48192)\n",
            "Loss: 1.375 | Acc: 50.087% (24170/48256)\n",
            "Loss: 1.375 | Acc: 50.106% (24211/48320)\n",
            "Loss: 1.375 | Acc: 50.085% (24233/48384)\n",
            "Loss: 1.375 | Acc: 50.091% (24268/48448)\n",
            "Loss: 1.375 | Acc: 50.082% (24296/48512)\n",
            "Loss: 1.375 | Acc: 50.080% (24327/48576)\n",
            "Loss: 1.375 | Acc: 50.090% (24364/48640)\n",
            "Loss: 1.375 | Acc: 50.094% (24398/48704)\n",
            "Loss: 1.375 | Acc: 50.105% (24435/48768)\n",
            "Loss: 1.375 | Acc: 50.094% (24462/48832)\n",
            "Loss: 1.375 | Acc: 50.092% (24493/48896)\n",
            "Loss: 1.375 | Acc: 50.094% (24526/48960)\n",
            "Loss: 1.375 | Acc: 50.088% (24543/49000)\n",
            "Epoch 2 of training is completed, Training accuracy for this epoch is 50.087755102040816\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 1.329 | Acc: 46.875% (30/64)\n",
            "Loss: 1.292 | Acc: 46.875% (60/128)\n",
            "Loss: 1.361 | Acc: 45.833% (88/192)\n",
            "Loss: 1.376 | Acc: 46.484% (119/256)\n",
            "Loss: 1.342 | Acc: 48.438% (155/320)\n",
            "Loss: 1.377 | Acc: 47.656% (183/384)\n",
            "Loss: 1.376 | Acc: 48.214% (216/448)\n",
            "Loss: 1.357 | Acc: 48.242% (247/512)\n",
            "Loss: 1.342 | Acc: 49.132% (283/576)\n",
            "Loss: 1.341 | Acc: 49.062% (314/640)\n",
            "Loss: 1.368 | Acc: 48.011% (338/704)\n",
            "Loss: 1.377 | Acc: 48.047% (369/768)\n",
            "Loss: 1.375 | Acc: 48.077% (400/832)\n",
            "Loss: 1.360 | Acc: 48.438% (434/896)\n",
            "Loss: 1.350 | Acc: 49.271% (473/960)\n",
            "Loss: 1.331 | Acc: 50.000% (512/1024)\n",
            "Loss: 1.334 | Acc: 50.000% (544/1088)\n",
            "Loss: 1.334 | Acc: 50.260% (579/1152)\n",
            "Loss: 1.330 | Acc: 50.411% (613/1216)\n",
            "Loss: 1.340 | Acc: 49.922% (639/1280)\n",
            "Loss: 1.334 | Acc: 50.000% (672/1344)\n",
            "Loss: 1.334 | Acc: 50.000% (704/1408)\n",
            "Loss: 1.331 | Acc: 50.136% (738/1472)\n",
            "Loss: 1.335 | Acc: 50.260% (772/1536)\n",
            "Loss: 1.335 | Acc: 50.500% (808/1600)\n",
            "Loss: 1.337 | Acc: 50.721% (844/1664)\n",
            "Loss: 1.339 | Acc: 50.694% (876/1728)\n",
            "Loss: 1.339 | Acc: 50.614% (907/1792)\n",
            "Loss: 1.338 | Acc: 50.862% (944/1856)\n",
            "Loss: 1.340 | Acc: 51.094% (981/1920)\n",
            "Loss: 1.341 | Acc: 51.210% (1016/1984)\n",
            "Loss: 1.345 | Acc: 51.270% (1050/2048)\n",
            "Loss: 1.342 | Acc: 51.373% (1085/2112)\n",
            "Loss: 1.343 | Acc: 51.149% (1113/2176)\n",
            "Loss: 1.342 | Acc: 51.295% (1149/2240)\n",
            "Loss: 1.344 | Acc: 51.042% (1176/2304)\n",
            "Loss: 1.345 | Acc: 51.056% (1209/2368)\n",
            "Loss: 1.346 | Acc: 51.110% (1243/2432)\n",
            "Loss: 1.341 | Acc: 51.322% (1281/2496)\n",
            "Loss: 1.349 | Acc: 51.211% (1311/2560)\n",
            "Loss: 1.350 | Acc: 51.181% (1343/2624)\n",
            "Loss: 1.349 | Acc: 51.228% (1377/2688)\n",
            "Loss: 1.345 | Acc: 51.417% (1415/2752)\n",
            "Loss: 1.346 | Acc: 51.278% (1444/2816)\n",
            "Loss: 1.349 | Acc: 51.181% (1474/2880)\n",
            "Loss: 1.345 | Acc: 51.325% (1511/2944)\n",
            "Loss: 1.346 | Acc: 51.263% (1542/3008)\n",
            "Loss: 1.344 | Acc: 51.335% (1577/3072)\n",
            "Loss: 1.343 | Acc: 51.244% (1607/3136)\n",
            "Loss: 1.342 | Acc: 51.219% (1639/3200)\n",
            "Loss: 1.342 | Acc: 51.103% (1668/3264)\n",
            "Loss: 1.340 | Acc: 51.322% (1708/3328)\n",
            "Loss: 1.340 | Acc: 51.415% (1744/3392)\n",
            "Loss: 1.341 | Acc: 51.447% (1778/3456)\n",
            "Loss: 1.340 | Acc: 51.307% (1806/3520)\n",
            "Loss: 1.337 | Acc: 51.562% (1848/3584)\n",
            "Loss: 1.338 | Acc: 51.562% (1881/3648)\n",
            "Loss: 1.337 | Acc: 51.616% (1916/3712)\n",
            "Loss: 1.340 | Acc: 51.510% (1945/3776)\n",
            "Loss: 1.337 | Acc: 51.510% (1978/3840)\n",
            "Loss: 1.337 | Acc: 51.434% (2008/3904)\n",
            "Loss: 1.335 | Acc: 51.487% (2043/3968)\n",
            "Loss: 1.337 | Acc: 51.438% (2074/4032)\n",
            "Loss: 1.339 | Acc: 51.392% (2105/4096)\n",
            "Loss: 1.340 | Acc: 51.442% (2140/4160)\n",
            "Loss: 1.340 | Acc: 51.515% (2176/4224)\n",
            "Loss: 1.337 | Acc: 51.493% (2208/4288)\n",
            "Loss: 1.337 | Acc: 51.677% (2249/4352)\n",
            "Loss: 1.332 | Acc: 51.925% (2293/4416)\n",
            "Loss: 1.332 | Acc: 51.942% (2327/4480)\n",
            "Loss: 1.332 | Acc: 52.003% (2363/4544)\n",
            "Loss: 1.334 | Acc: 51.910% (2392/4608)\n",
            "Loss: 1.332 | Acc: 51.991% (2429/4672)\n",
            "Loss: 1.329 | Acc: 52.111% (2468/4736)\n",
            "Loss: 1.333 | Acc: 51.979% (2495/4800)\n",
            "Loss: 1.331 | Acc: 52.015% (2530/4864)\n",
            "Loss: 1.331 | Acc: 51.928% (2559/4928)\n",
            "Loss: 1.332 | Acc: 51.863% (2589/4992)\n",
            "Loss: 1.332 | Acc: 51.820% (2620/5056)\n",
            "Loss: 1.335 | Acc: 51.719% (2648/5120)\n",
            "Loss: 1.335 | Acc: 51.659% (2678/5184)\n",
            "Loss: 1.336 | Acc: 51.582% (2707/5248)\n",
            "Loss: 1.335 | Acc: 51.544% (2738/5312)\n",
            "Loss: 1.336 | Acc: 51.544% (2771/5376)\n",
            "Loss: 1.336 | Acc: 51.544% (2804/5440)\n",
            "Loss: 1.336 | Acc: 51.508% (2835/5504)\n",
            "Loss: 1.340 | Acc: 51.311% (2857/5568)\n",
            "Loss: 1.341 | Acc: 51.207% (2884/5632)\n",
            "Loss: 1.341 | Acc: 51.229% (2918/5696)\n",
            "Loss: 1.341 | Acc: 51.267% (2953/5760)\n",
            "Loss: 1.339 | Acc: 51.339% (2990/5824)\n",
            "Loss: 1.341 | Acc: 51.189% (3014/5888)\n",
            "Loss: 1.340 | Acc: 51.294% (3053/5952)\n",
            "Loss: 1.340 | Acc: 51.313% (3087/6016)\n",
            "Loss: 1.340 | Acc: 51.234% (3115/6080)\n",
            "Loss: 1.341 | Acc: 51.221% (3147/6144)\n",
            "Loss: 1.342 | Acc: 51.176% (3177/6208)\n",
            "Loss: 1.345 | Acc: 51.052% (3202/6272)\n",
            "Loss: 1.345 | Acc: 51.136% (3240/6336)\n",
            "Loss: 1.347 | Acc: 51.031% (3266/6400)\n",
            "Loss: 1.347 | Acc: 51.067% (3301/6464)\n",
            "Loss: 1.346 | Acc: 51.026% (3331/6528)\n",
            "Loss: 1.349 | Acc: 50.971% (3360/6592)\n",
            "Loss: 1.347 | Acc: 51.007% (3395/6656)\n",
            "Loss: 1.348 | Acc: 50.938% (3423/6720)\n",
            "Loss: 1.347 | Acc: 51.032% (3462/6784)\n",
            "Loss: 1.345 | Acc: 51.081% (3498/6848)\n",
            "Loss: 1.347 | Acc: 50.969% (3523/6912)\n",
            "Loss: 1.347 | Acc: 51.003% (3558/6976)\n",
            "Loss: 1.347 | Acc: 50.980% (3589/7040)\n",
            "Loss: 1.348 | Acc: 51.014% (3624/7104)\n",
            "Loss: 1.347 | Acc: 51.046% (3659/7168)\n",
            "Loss: 1.349 | Acc: 50.982% (3687/7232)\n",
            "Loss: 1.346 | Acc: 51.042% (3724/7296)\n",
            "Loss: 1.344 | Acc: 51.087% (3760/7360)\n",
            "Loss: 1.344 | Acc: 51.051% (3790/7424)\n",
            "Loss: 1.343 | Acc: 51.095% (3826/7488)\n",
            "Loss: 1.342 | Acc: 51.165% (3864/7552)\n",
            "Loss: 1.342 | Acc: 51.129% (3894/7616)\n",
            "Loss: 1.343 | Acc: 51.094% (3924/7680)\n",
            "Loss: 1.341 | Acc: 51.175% (3963/7744)\n",
            "Loss: 1.340 | Acc: 51.242% (4001/7808)\n",
            "Loss: 1.342 | Acc: 51.156% (4027/7872)\n",
            "Loss: 1.341 | Acc: 51.222% (4065/7936)\n",
            "Loss: 1.341 | Acc: 51.188% (4095/8000)\n",
            "Loss: 1.342 | Acc: 51.153% (4125/8064)\n",
            "Loss: 1.342 | Acc: 51.206% (4162/8128)\n",
            "Loss: 1.340 | Acc: 51.160% (4191/8192)\n",
            "Loss: 1.341 | Acc: 51.139% (4222/8256)\n",
            "Loss: 1.344 | Acc: 51.010% (4244/8320)\n",
            "Loss: 1.345 | Acc: 50.930% (4270/8384)\n",
            "Loss: 1.344 | Acc: 50.900% (4300/8448)\n",
            "Loss: 1.345 | Acc: 50.940% (4336/8512)\n",
            "Loss: 1.344 | Acc: 50.979% (4372/8576)\n",
            "Loss: 1.345 | Acc: 51.007% (4407/8640)\n",
            "Loss: 1.346 | Acc: 50.954% (4435/8704)\n",
            "Loss: 1.346 | Acc: 50.935% (4466/8768)\n",
            "Loss: 1.347 | Acc: 50.894% (4495/8832)\n",
            "Loss: 1.346 | Acc: 50.922% (4530/8896)\n",
            "Loss: 1.346 | Acc: 50.859% (4557/8960)\n",
            "Loss: 1.346 | Acc: 50.853% (4589/9024)\n",
            "Loss: 1.345 | Acc: 50.858% (4622/9088)\n",
            "Loss: 1.346 | Acc: 50.907% (4659/9152)\n",
            "Loss: 1.345 | Acc: 50.955% (4696/9216)\n",
            "Loss: 1.343 | Acc: 50.991% (4732/9280)\n",
            "Loss: 1.344 | Acc: 51.006% (4766/9344)\n",
            "Loss: 1.344 | Acc: 51.020% (4800/9408)\n",
            "Loss: 1.345 | Acc: 51.003% (4831/9472)\n",
            "Loss: 1.345 | Acc: 51.049% (4868/9536)\n",
            "Loss: 1.344 | Acc: 51.042% (4900/9600)\n",
            "Loss: 1.344 | Acc: 51.045% (4933/9664)\n",
            "Loss: 1.344 | Acc: 51.090% (4970/9728)\n",
            "Loss: 1.346 | Acc: 51.042% (4998/9792)\n",
            "Loss: 1.346 | Acc: 51.015% (5028/9856)\n",
            "Loss: 1.346 | Acc: 50.988% (5058/9920)\n",
            "Loss: 1.347 | Acc: 50.942% (5086/9984)\n",
            "Loss: 1.347 | Acc: 50.920% (5092/10000)\n",
            "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 50.92\n",
            "\n",
            "Final train set accuracy is 50.087755102040816\n",
            "Final test set accuracy is 50.92\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.002\n",
        "input_dims = 3\n",
        "hidden_dims = 128\n",
        "output_dims=10\n",
        "num_trans_layers = 4\n",
        "num_heads=4\n",
        "image_k=32\n",
        "patch_k=4\n",
        "\n",
        "network = None\n",
        "optimizer = None\n",
        "\n",
        "################################################################################\n",
        "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "network = ViT(hidden_dims, input_dims, output_dims, num_trans_layers,num_heads,image_k,patch_k)\n",
        "\n",
        "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             \n",
        "################################################################################\n",
        "\n",
        "tr_accs=[]\n",
        "test_accs=[]\n",
        "for epoch in range(3):\n",
        "    tr_acc = train(network, optimizer, loader_train)\n",
        "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
        "              .format(epoch, tr_acc))  \n",
        "    \n",
        "    test_acc = evaluate(network, loader_test)\n",
        "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
        "              .format(epoch, test_acc))  \n",
        "    \n",
        "    tr_accs.append(tr_acc)\n",
        "    test_accs.append(test_acc)\n",
        "    \n",
        "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
        "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec757796",
      "metadata": {
        "id": "ec757796"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d342e26f331b45cd9c390a7bdd1c1e05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_238cf8e173a548c0ab6cf47f75735fe3",
              "IPY_MODEL_d86e82a2c0e349faa81b2d4db15b77b7",
              "IPY_MODEL_4ea1a7c745c14811af373576f212cd8a"
            ],
            "layout": "IPY_MODEL_885bd009bd5f49f18cd6c3917eafe8d7"
          }
        },
        "238cf8e173a548c0ab6cf47f75735fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece3db6e8fb843e6b3d8055fcb50b4d1",
            "placeholder": "​",
            "style": "IPY_MODEL_afa737f1c7a54d36bcf2feed2b6f1b95",
            "value": "100%"
          }
        },
        "d86e82a2c0e349faa81b2d4db15b77b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd8104dc914c400989fb1b9b7897b479",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3caba48189a48cca56663445598b014",
            "value": 170498071
          }
        },
        "4ea1a7c745c14811af373576f212cd8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_618c338ee0804002a4e5c2c0dd0158b7",
            "placeholder": "​",
            "style": "IPY_MODEL_62807e8b2fae424aa17bb71c90d971fb",
            "value": " 170498071/170498071 [00:06&lt;00:00, 29189395.79it/s]"
          }
        },
        "885bd009bd5f49f18cd6c3917eafe8d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ece3db6e8fb843e6b3d8055fcb50b4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afa737f1c7a54d36bcf2feed2b6f1b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd8104dc914c400989fb1b9b7897b479": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3caba48189a48cca56663445598b014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "618c338ee0804002a4e5c2c0dd0158b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62807e8b2fae424aa17bb71c90d971fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}